{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "out = 'bert-large-bahasa-standard-cased'\n",
    "os.makedirs(out, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel, BertConfig, AutoTokenizer, AutoModelWithLMHead, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.0.2'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('bert-large-bahasa-standard-cased/vocab.txt',\n",
       " 'bert-large-bahasa-standard-cased/special_tokens_map.json',\n",
       " 'bert-large-bahasa-standard-cased/added_tokens.json')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = BertTokenizer('BERT.wordpiece', do_lower_case = False)\n",
    "tokenizer.save_pretrained('bert-large-bahasa-standard-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('./bert-large-bahasa-standard-cased', do_lower_case = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !transformers-cli convert --model_type bert \\\n",
    "#   --tf_checkpoint bert-base/model.ckpt-500000 \\\n",
    "#   --config BASE_config.json \\\n",
    "#   --pytorch_dump_output bert-base-bahasa-standard-cased/pytorch_model.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = BertConfig(f'{directory}/config.json')\n",
    "config.vocab_size = 32000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/husein/.local/lib/python3.6/site-packages/transformers/modeling_auto.py:798: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  FutureWarning,\n",
      "Some weights of the model checkpoint at ./bert-base-bahasa-standard-cased/pytorch_model.bin were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelWithLMHead.from_pretrained('./bert-base-bahasa-standard-cased/pytorch_model.bin', config = config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_mask = pipeline('fill-mask', model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sequence': '[CLS] makan ayam dengan ayam [SEP]',\n",
       "  'score': 0.16134978830814362,\n",
       "  'token': 5838,\n",
       "  'token_str': 'ayam'},\n",
       " {'sequence': '[CLS] makan ayam dengan itik [SEP]',\n",
       "  'score': 0.054859865456819534,\n",
       "  'token': 27174,\n",
       "  'token_str': 'itik'},\n",
       " {'sequence': '[CLS] makan ayam dengan telur [SEP]',\n",
       "  'score': 0.028192562982439995,\n",
       "  'token': 8900,\n",
       "  'token_str': 'telur'},\n",
       " {'sequence': '[CLS] makan ayam dengan sos [SEP]',\n",
       "  'score': 0.025005826726555824,\n",
       "  'token': 18840,\n",
       "  'token_str': 'sos'},\n",
       " {'sequence': '[CLS] makan ayam dengan sambal [SEP]',\n",
       "  'score': 0.02045358717441559,\n",
       "  'token': 24098,\n",
       "  'token_str': 'sambal'}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_mask('makan ayam dengan [MASK]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sequence': '[CLS] mahathir sebenarnya sangat mencintai tanah airnya [SEP]',\n",
       "  'score': 0.09542842209339142,\n",
       "  'token': 14727,\n",
       "  'token_str': 'mencintai'},\n",
       " {'sequence': '[CLS] mahathir sebenarnya sangat cintakan tanah airnya [SEP]',\n",
       "  'score': 0.06102743372321129,\n",
       "  'token': 24174,\n",
       "  'token_str': 'cintakan'},\n",
       " {'sequence': '[CLS] mahathir sebenarnya sangat suka tanah airnya [SEP]',\n",
       "  'score': 0.05743848904967308,\n",
       "  'token': 3085,\n",
       "  'token_str': 'suka'},\n",
       " {'sequence': '[CLS] mahathir sebenarnya sangat sayangkan tanah airnya [SEP]',\n",
       "  'score': 0.04153375327587128,\n",
       "  'token': 22562,\n",
       "  'token_str': 'sayangkan'},\n",
       " {'sequence': '[CLS] mahathir sebenarnya sangat luas tanah airnya [SEP]',\n",
       "  'score': 0.033663298934698105,\n",
       "  'token': 6425,\n",
       "  'token_str': 'luas'}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_mask('mahathir sebenarnya sangat [MASK] tanah airnya')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('bert-base-bahasa-standard-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "!transformers-cli upload ./bert-base-bahasa-standard-cased"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
