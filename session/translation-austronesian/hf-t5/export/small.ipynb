{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['finetune-t5-small-standard-bahasa-cased-austronesian/checkpoint-910000',\n",
       " 'finetune-t5-small-standard-bahasa-cased-austronesian/checkpoint-920000',\n",
       " 'finetune-t5-small-standard-bahasa-cased-austronesian/checkpoint-930000']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from glob import glob\n",
    "\n",
    "checkpoints = sorted(glob('finetune-t5-small-standard-bahasa-cased-austronesian/checkpoint-*'))\n",
    "checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained('mesolitica/t5-small-standard-bahasa-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = T5ForConditionalGeneration.from_pretrained(checkpoints[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> Hai orang! Saya perhatikan kemarin & hari ini banyak yang mendapatkan cookie ini, bukan. Jadi hari ini saya ingin membagikan beberapa post mortem dari kelompok pertama kami:</s>\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer.encode('terjemah Melayu ke Indonesia: Hi guys! I noticed semalam & harini dah ramai yang dapat cookies ni kan. So harini i nak share some post mortem of our first batch:', return_tensors = 'pt')\n",
    "outputs = model.generate(input_ids, max_length = 100, temperature = 0.5)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> Aku ora seneng lele lan pitik goreng</s>\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer.encode('terjemah Melayu ke Jawa: saya tak suka ikan keli dan ayam goreng', return_tensors = 'pt')\n",
    "outputs = model.generate(input_ids, max_length = 100)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> Saya tidak suka lele dan ayam goreng</s>\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer.encode('terjemah Melayu ke Indonesia: saya tak suka ikan keli dan ayam goreng', return_tensors = 'pt')\n",
    "outputs = model.generate(input_ids, max_length = 100)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> Wahai orang! Saya perhatikan semalam & hari ini ramai yang dapat kuih ini. Jadi hari ini saya ingin berkongsi beberapa post mortem dari kumpulan pertama kami:</s>\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer.encode('terjemah Indonesia ke Melayu: Hai orang! Saya perhatikan kemarin & hari ini banyak yang mendapatkan cookie ini. Jadi hari ini saya ingin berbagi beberapa post mortem dari kelompok pertama kami:', return_tensors = 'pt')\n",
    "outputs = model.generate(input_ids, max_length = 100)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/husein/.local/lib/python3.8/site-packages/transformers/utils/hub.py:651: UserWarning: The `organization` argument is deprecated and will be removed in v5 of Transformers. Set your organization directly in the `repo_id` passed instead (`repo_id={organization}/{model_id}`).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/mesolitica/finetune-translation-austronesian-t5-small-standard-bahasa-cased/commit/11d47b0fe8634ba967e9c82230b52fec72617eeb', commit_message='Upload T5ForConditionalGeneration', commit_description='', oid='11d47b0fe8634ba967e9c82230b52fec72617eeb', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.push_to_hub('finetune-translation-austronesian-t5-small-standard-bahasa-cased', organization='mesolitica')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/mesolitica/finetune-translation-austronesian-t5-small-standard-bahasa-cased/commit/0f7c951b017dbca0d0d5429c4f9fd2b94cfd3423', commit_message='Upload tokenizer', commit_description='', oid='0f7c951b017dbca0d0d5429c4f9fd2b94cfd3423', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.push_to_hub('finetune-translation-austronesian-t5-small-standard-bahasa-cased', organization='mesolitica')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sacrebleu.metrics import BLEU, CHRF, TER\n",
    "\n",
    "bleu = BLEU()\n",
    "chrf = CHRF(word_order = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unidecode import unidecode\n",
    "\n",
    "with open('ind_Latn.dev') as fopen:\n",
    "    ind = fopen.read().split('\\n')[:-1]\n",
    "\n",
    "with open('jav_Latn.dev') as fopen:\n",
    "    jav = fopen.read().split('\\n')[:-1]\n",
    "    \n",
    "with open('zsm_Latn.dev') as fopen:\n",
    "    ms = fopen.read().split('\\n')[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = [unidecode(s) for s in ind]\n",
    "jav = [unidecode(s) for s in jav]\n",
    "ms = [unidecode(s) for s in ms]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 997/997 [04:13<00:00,  3.93it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "results = []\n",
    "for i in tqdm(range(0, len(ind), batch_size)):\n",
    "    input_ids = [{'input_ids': tokenizer.encode(f'terjemah Indonesia ke Melayu: {s}', return_tensors = 'pt')[0]} for s in ind[i:i + batch_size]]\n",
    "    padded = tokenizer.pad(input_ids, padding = 'longest')\n",
    "    outputs = model.generate(**padded, max_length = 512)\n",
    "    for o in outputs:\n",
    "        results.append(tokenizer.decode(o, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'name': 'BLEU',\n",
       "  'score': 30.24358980824753,\n",
       "  '_mean': -1.0,\n",
       "  '_ci': -1.0,\n",
       "  '_verbose': '61.1/36.9/23.8/15.6 (BP = 1.000 ratio = 1.052 hyp_len = 23174 ref_len = 22027)',\n",
       "  'bp': 1.0,\n",
       "  'counts': [14159, 8189, 5042, 3144],\n",
       "  'totals': [23174, 22177, 21180, 20183],\n",
       "  'sys_len': 23174,\n",
       "  'ref_len': 22027,\n",
       "  'precisions': [61.098645033226894,\n",
       "   36.925643684898766,\n",
       "   23.80547686496695,\n",
       "   15.577466184412625],\n",
       "  'prec_str': '61.1/36.9/23.8/15.6',\n",
       "  'ratio': 1.0520724565306214},\n",
       " chrF2++ = 58.43)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_left, filtered_right = [], []\n",
    "for no, r in enumerate(results):\n",
    "    if len(r):\n",
    "        filtered_left.append(r)\n",
    "        filtered_right.append(ms[no])\n",
    "        \n",
    "refs = [filtered_right]\n",
    "sys = filtered_left\n",
    "r = bleu.corpus_score(sys, refs)\n",
    "r.__dict__, chrf.corpus_score(sys, refs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 997/997 [04:19<00:00,  3.85it/s]\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for i in tqdm(range(0, len(ms), batch_size)):\n",
    "    input_ids = [{'input_ids': tokenizer.encode(f'terjemah Melayu ke Indonesia: {s}', return_tensors = 'pt')[0]} for s in ms[i:i + batch_size]]\n",
    "    padded = tokenizer.pad(input_ids, padding = 'longest')\n",
    "    outputs = model.generate(**padded, max_length = 512)\n",
    "    for o in outputs:\n",
    "        results.append(tokenizer.decode(o, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'name': 'BLEU',\n",
       "  'score': 35.95448072225675,\n",
       "  '_mean': -1.0,\n",
       "  '_ci': -1.0,\n",
       "  '_verbose': '66.3/42.6/29.1/20.3 (BP = 1.000 ratio = 1.014 hyp_len = 22164 ref_len = 21856)',\n",
       "  'bp': 1.0,\n",
       "  'counts': [14691, 9018, 5871, 3898],\n",
       "  'totals': [22164, 21167, 20170, 19173],\n",
       "  'sys_len': 22164,\n",
       "  'ref_len': 21856,\n",
       "  'precisions': [66.28316188413643,\n",
       "   42.60405347947277,\n",
       "   29.10758552305404,\n",
       "   20.330673342721536],\n",
       "  'prec_str': '66.3/42.6/29.1/20.3',\n",
       "  'ratio': 1.0140922401171304},\n",
       " chrF2++ = 61.02)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_left, filtered_right = [], []\n",
    "for no, r in enumerate(results):\n",
    "    if len(r):\n",
    "        filtered_left.append(r)\n",
    "        filtered_right.append(ind[no])\n",
    "        \n",
    "refs = [filtered_right]\n",
    "sys = filtered_left\n",
    "r = bleu.corpus_score(sys, refs)\n",
    "r.__dict__, chrf.corpus_score(sys, refs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 997/997 [06:41<00:00,  2.48it/s]\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for i in tqdm(range(0, len(ms), batch_size)):\n",
    "    input_ids = [{'input_ids': tokenizer.encode(f'terjemah Melayu ke Jawa: {s}', return_tensors = 'pt')[0]} for s in ms[i:i + batch_size]]\n",
    "    padded = tokenizer.pad(input_ids, padding = 'longest')\n",
    "    outputs = model.generate(**padded, max_length = 512)\n",
    "    for o in outputs:\n",
    "        results.append(tokenizer.decode(o, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'name': 'BLEU',\n",
       "  'score': 24.599989427145964,\n",
       "  '_mean': -1.0,\n",
       "  '_ci': -1.0,\n",
       "  '_verbose': '58.3/31.6/18.5/11.2 (BP = 0.990 ratio = 0.990 hyp_len = 21391 ref_len = 21609)',\n",
       "  'bp': 0.9898605524286408,\n",
       "  'counts': [12475, 6440, 3580, 2065],\n",
       "  'totals': [21391, 20394, 19397, 18400],\n",
       "  'sys_len': 21391,\n",
       "  'ref_len': 21609,\n",
       "  'precisions': [58.31891917161423,\n",
       "   31.577915073060705,\n",
       "   18.45646233953704,\n",
       "   11.222826086956522],\n",
       "  'prec_str': '58.3/31.6/18.5/11.2',\n",
       "  'ratio': 0.9899116109028645},\n",
       " chrF2++ = 51.65)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_left, filtered_right = [], []\n",
    "for no, r in enumerate(results):\n",
    "    if len(r):\n",
    "        filtered_left.append(r)\n",
    "        filtered_right.append(jav[no])\n",
    "        \n",
    "refs = [filtered_right]\n",
    "sys = filtered_left\n",
    "r = bleu.corpus_score(sys, refs)\n",
    "r.__dict__, chrf.corpus_score(sys, refs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 997/997 [04:14<00:00,  3.92it/s]\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for i in tqdm(range(0, len(ms), batch_size)):\n",
    "    input_ids = [{'input_ids': tokenizer.encode(f'terjemah Jawa ke Melayu: {s}', return_tensors = 'pt')[0]} for s in jav[i:i + batch_size]]\n",
    "    padded = tokenizer.pad(input_ids, padding = 'longest')\n",
    "    outputs = model.generate(**padded, max_length = 512)\n",
    "    for o in outputs:\n",
    "        results.append(tokenizer.decode(o, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'name': 'BLEU',\n",
       "  'score': 25.24437731940083,\n",
       "  '_mean': -1.0,\n",
       "  '_ci': -1.0,\n",
       "  '_verbose': '57.7/31.9/19.0/11.6 (BP = 1.000 ratio = 1.022 hyp_len = 22516 ref_len = 22027)',\n",
       "  'bp': 1.0,\n",
       "  'counts': [12999, 6872, 3909, 2258],\n",
       "  'totals': [22516, 21519, 20522, 19525],\n",
       "  'sys_len': 22516,\n",
       "  'ref_len': 22027,\n",
       "  'precisions': [57.732279268076034,\n",
       "   31.934569450253264,\n",
       "   19.04785108663873,\n",
       "   11.564660691421254],\n",
       "  'prec_str': '57.7/31.9/19.0/11.6',\n",
       "  'ratio': 1.0222000272392973},\n",
       " chrF2++ = 52.58)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_left, filtered_right = [], []\n",
    "for no, r in enumerate(results):\n",
    "    if len(r):\n",
    "        filtered_left.append(r)\n",
    "        filtered_right.append(ms[no])\n",
    "        \n",
    "refs = [filtered_right]\n",
    "sys = filtered_left\n",
    "r = bleu.corpus_score(sys, refs)\n",
    "r.__dict__, chrf.corpus_score(sys, refs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
