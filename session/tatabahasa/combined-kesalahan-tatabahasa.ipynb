{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import json\n",
    "import copy\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Load quantized model will cause accuracy drop.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/husein/.local/lib/python3.6/site-packages/malaya/function/__init__.py:74: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/husein/.local/lib/python3.6/site-packages/malaya/function/__init__.py:74: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/husein/.local/lib/python3.6/site-packages/malaya/function/__init__.py:76: The name tf.GraphDef is deprecated. Please use tf.compat.v1.GraphDef instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/husein/.local/lib/python3.6/site-packages/malaya/function/__init__.py:76: The name tf.GraphDef is deprecated. Please use tf.compat.v1.GraphDef instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/husein/.local/lib/python3.6/site-packages/malaya/function/__init__.py:51: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/husein/.local/lib/python3.6/site-packages/malaya/function/__init__.py:51: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/husein/.local/lib/python3.6/site-packages/malaya/function/__init__.py:66: The name tf.InteractiveSession is deprecated. Please use tf.compat.v1.InteractiveSession instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/husein/.local/lib/python3.6/site-packages/malaya/function/__init__.py:66: The name tf.InteractiveSession is deprecated. Please use tf.compat.v1.InteractiveSession instead.\n",
      "\n",
      "WARNING:root:Load quantized model will cause accuracy drop.\n"
     ]
    }
   ],
   "source": [
    "import malaya\n",
    "from malaya.text.regex import _expressions\n",
    "\n",
    "model = malaya.dependency.transformer(model = 'xlnet', quantized = True)\n",
    "pos = malaya.pos.transformer(model = 'xlnet', quantized = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = malaya.preprocessing.TOKENIZER(date = False, time = False).tokenize\n",
    "sastrawi = malaya.stem.sastrawi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://raw.githubusercontent.com/huseinzol05/Malay-Dataset/master/dictionary/synonym/synonym0.json\n",
    "# !wget https://raw.githubusercontent.com/huseinzol05/Malay-Dataset/master/dictionary/synonym/synonym1.json\n",
    "files = ['synonym0.json', 'synonym1.json']\n",
    "synonyms = defaultdict(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in files:\n",
    "    with open(file) as fopen:\n",
    "        data = json.load(fopen)\n",
    "    for i in data:\n",
    "        if not len(i[1]):\n",
    "            continue\n",
    "        synonyms[i[0]].extend(i[1])\n",
    "        for r in i[1]:\n",
    "            synonyms[r].append(i[0])\n",
    "            \n",
    "for k, v in synonyms.items():\n",
    "    synonyms[k] = list(set(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_t(tokens):\n",
    "    t = []\n",
    "    for i in range(len(tokens)):\n",
    "        t.append([tokens[i], 2])\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_3_0(t, row, selected = ['compound', 'flat']):\n",
    "    text, tokens, tokens_lower, graph = row\n",
    "    l = list(graph.nodes.items())\n",
    "    for no, n in enumerate(l[1:]):\n",
    "        n = n[1]\n",
    "        if n['rel'] in selected and n['address'] - 1 == n['head']:\n",
    "            if n['word'] == t[n['head'] - 1][0]:\n",
    "                print('repeated word, continue')\n",
    "                continue\n",
    "            if n['word'][0].isupper() or t[n['head'] - 1][0][0].isupper():\n",
    "                continue\n",
    "            if n['word'].lower() in set_combined_penjodoh_bilangan or \\\n",
    "            t[n['head'] - 1][0].lower() in set_combined_penjodoh_bilangan:\n",
    "                continue\n",
    "                \n",
    "            c = t[n['head'] - 1].copy()\n",
    "            c[1] = 3\n",
    "            t[n['head'] - 1] = [t[n['address'] - 1][0], 3]\n",
    "            t[n['address'] - 1] = c\n",
    "            tokens[n['head'] - 1] = t[n['address'] - 1][0]\n",
    "            tokens[n['address'] - 1] = c[0]\n",
    "            tokens_lower[n['head'] - 1] = t[n['address'] - 1][0].lower()\n",
    "            tokens_lower[n['address'] - 1] = c[0].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://ms.wikipedia.org/wiki/Penjodoh_bilangan_bahasa_Melayu\n",
    "penjodoh_bilangan = [\n",
    "    'angkatan',\n",
    "    'baris',\n",
    "    'batang',\n",
    "    'bentuk',\n",
    "    'bidang',\n",
    "    'biji',\n",
    "    'bilah',\n",
    "    'buah',\n",
    "    'buku',\n",
    "    'bungkus',\n",
    "    'butir',\n",
    "    'carik',\n",
    "    'cebis',\n",
    "    'cekak',\n",
    "    'cubit',\n",
    "    'cucuk',\n",
    "    'das',\n",
    "    'deret',\n",
    "    'ekor',\n",
    "    'gugus',\n",
    "    'gelung',\n",
    "    'gemal',\n",
    "    'genggam',\n",
    "    'gulung',\n",
    "    'gumpal',\n",
    "    'helai',\n",
    "    'hidangan',\n",
    "    'hiris',\n",
    "    'ikat',\n",
    "    'jambak',\n",
    "    'jambangan',\n",
    "    'jemput',\n",
    "    'kaki',\n",
    "    'kalung',\n",
    "    'kandang',\n",
    "    'kapur',\n",
    "    'kawan',\n",
    "    'kelompok',\n",
    "    'kepal',\n",
    "    'keping',\n",
    "    'kepul',\n",
    "    'kerat',\n",
    "    'ketul',\n",
    "    'kotak',\n",
    "    'kuntum',\n",
    "    'laras',\n",
    "    'lembar',\n",
    "    'lingkar',\n",
    "    'longgok',\n",
    "    'naskhah',\n",
    "    'orang',\n",
    "    'papan',\n",
    "    'pasang',\n",
    "    'pasukan',\n",
    "    'patah',\n",
    "    'pintu',\n",
    "    'potong',\n",
    "    'pucuk',\n",
    "    'puntung',\n",
    "    'rangkap',\n",
    "    'rawan',\n",
    "    'ruas',\n",
    "    'rumpun',\n",
    "    'sikat',\n",
    "    'sisir',\n",
    "    'suap',\n",
    "    'tandan',\n",
    "    'tangkai',\n",
    "    'teguk',\n",
    "    'timbun',\n",
    "    'titik',\n",
    "    'tongkol',\n",
    "    'ulas',\n",
    "    'untai',\n",
    "    'urat',\n",
    "    'utas',\n",
    "]\n",
    "hubung_list = [\n",
    "    'agar',\n",
    "    'apabila',\n",
    "    'atau',\n",
    "    'bahawa',\n",
    "    'dan',\n",
    "    'hingga',\n",
    "    'jika',\n",
    "    'jikalau',\n",
    "    'kecuali',\n",
    "    'kerana',\n",
    "    'lalu',\n",
    "    'manakala',\n",
    "    'sambil',\n",
    "    'serta',\n",
    "    'semenjak',\n",
    "    'sementara',\n",
    "    'sungguhpun',\n",
    "    'supaya',\n",
    "    'walaupun',\n",
    "    'tetapi',\n",
    "    'berkenan',\n",
    "    'berkenaan',\n",
    "    'yang',\n",
    "    'juga',\n",
    "    'tersebut'\n",
    "]\n",
    "end_4 = ['nya']\n",
    "reserved_4 = ['mereka', 'pelajar', 'rakyat', 'penduduk', 'umat', 'kami', 'semua', 'kumpulan', 'para']\n",
    "start_4 = ['be', 'ber', 'ter', 'se']\n",
    "sepenjodoh_bilangan = [f'se{w}' for w in penjodoh_bilangan]\n",
    "set_sepenjodoh_bilangan = set(sepenjodoh_bilangan)\n",
    "set_penjodoh_bilangan = set(penjodoh_bilangan)\n",
    "set_reserved_4 = set(reserved_4)\n",
    "set_combined_penjodoh_bilangan = set_sepenjodoh_bilangan | set_penjodoh_bilangan\n",
    "\n",
    "# [penjodoh bilangan] [kata nama] -> [penjodoh bilangan] [kata nama - kata nama]\n",
    "# dua buah kereta -> dua buah kereta-kereta\n",
    "def augment_4_0(t, row):\n",
    "    text, tokens, tokens_lower, penjodoh = row\n",
    "    for word in penjodoh:\n",
    "        try:\n",
    "            i = tokens_lower.index(word) + 1\n",
    "            if tokens_lower[i] in hubung_list:\n",
    "                continue\n",
    "            if tokens[i][0].isupper():\n",
    "                continue\n",
    "            if tokens[i].endswith('nya'):\n",
    "                tokens[i] = tokens[i][:-3]\n",
    "                ends = 'nya'\n",
    "            else:\n",
    "                ends = ''\n",
    "            word = f'{tokens[i]}-{tokens[i]}{ends}'\n",
    "            t[i][0] = word\n",
    "            t[i][1] = 4\n",
    "            tokens[i] = word\n",
    "            tokens_lower[i] = word.lower()\n",
    "        except Exception as e:\n",
    "            print('augment_4_0', e)\n",
    "            pass\n",
    "\n",
    "# [kata nama - kata nama] -> [kata nama]\n",
    "# ayam-ayam itu -> ayam itu\n",
    "def augment_4_1(t, row):\n",
    "    text, tokens, tokens_lower, penjodoh = row\n",
    "    for no, word in enumerate(tokens):\n",
    "        if re.findall(_expressions['hypen'], word.lower()):\n",
    "            stemmed = sastrawi.stem(word)\n",
    "            if stemmed != word.split('-')[0]:\n",
    "                continue\n",
    "            if word[0].isupper():\n",
    "                continue\n",
    "            word = word.split('-')[0]\n",
    "            t[no][0] = word\n",
    "            t[no][1] = 4\n",
    "            tokens[no] = word\n",
    "            tokens_lower[no] = word.lower()\n",
    "            \n",
    "            \n",
    "def augment_4_2(t, row):\n",
    "    text, tokens, tokens_lower, penjodoh = row\n",
    "    for word in penjodoh:\n",
    "        try:\n",
    "            i = tokens_lower.index(word)\n",
    "            if tokens[i].endswith('nya'):\n",
    "                tokens[i] = tokens[i][:-3]\n",
    "                ends = 'nya'\n",
    "            else:\n",
    "                ends = ''\n",
    "            t[i][0] = f'{tokens[i]}-{tokens[i]}{ends}'\n",
    "            t[i][1] = 4\n",
    "            tokens[i] = word\n",
    "            tokens_lower[i] = word.lower()\n",
    "        except Exception as e:\n",
    "            print('augment_4_2', e)\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "penguat_list = [\n",
    "    'paling',\n",
    "    'agak',\n",
    "    'sungguh',\n",
    "    'amat',\n",
    "    'terlalu',\n",
    "    'nian',\n",
    "    'benar',\n",
    "    'paling',\n",
    "    'sangat'\n",
    "]\n",
    "end_penguat_list = ['sekali', 'sungguh', 'sangat']\n",
    "set_penguat_list = set(penguat_list)\n",
    "\n",
    "def augment_5_0(t, row):\n",
    "    text, tokens, tokens_lower, penguat = row\n",
    "    for word in penguat:\n",
    "        try:\n",
    "            i = tokens_lower.index(word) + 1\n",
    "            if tokens[i][0].isupper():\n",
    "                continue\n",
    "            ends = random.choice(end_penguat_list)\n",
    "            word = f'{tokens[i]} {ends}'\n",
    "            t[i][0] = word\n",
    "            t[i][1] = 5\n",
    "            tokens[i] = word\n",
    "            tokens_lower[i] = word.lower()\n",
    "        except Exception as e:\n",
    "            print('augmentation_5_0', e)\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_start_ter(word):\n",
    "    stemmed = sastrawi.stem(word)\n",
    "    if word.startswith('ter') and not stemmed.startswith('ter') and stemmed in word:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def augment_6_0(t, row):\n",
    "    text, tokens, tokens_lower, penguat = row\n",
    "    for i in range(len(tokens)):\n",
    "        if check_start_ter(tokens[i]):\n",
    "            ends = random.choice(end_penguat_list)\n",
    "            word = f'{tokens[i]} {ends}'\n",
    "            t[i][0] = word\n",
    "            t[i][1] = 6\n",
    "            tokens[i] = word\n",
    "            tokens_lower[i] = word.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "hubung_list = [\n",
    "    'agar',\n",
    "    'apabila',\n",
    "    'atau',\n",
    "    'bahawa',\n",
    "    'dan',\n",
    "    'hingga',\n",
    "    'jika',\n",
    "    'jikalau',\n",
    "    'kecuali',\n",
    "    'kerana',\n",
    "    'lalu',\n",
    "    'manakala',\n",
    "    'sambil',\n",
    "    'serta',\n",
    "    'semenjak',\n",
    "    'sementara',\n",
    "    'sungguhpun',\n",
    "    'supaya',\n",
    "    'walaupun',\n",
    "    'tetapi',\n",
    "    'berkenan',\n",
    "    'berkenaan',\n",
    "]\n",
    "set_hubung_list = set(hubung_list)\n",
    "\n",
    "def augment_7_0(t, row):\n",
    "    text, tokens, tokens_lower, hubung = row\n",
    "    for word in hubung:\n",
    "        i = tokens_lower.index(word)\n",
    "        negate = list(set_hubung_list - {word})\n",
    "        choice = random.choice(negate)\n",
    "        t[i][0] = choice\n",
    "        t[i][1] = 7\n",
    "        tokens[i] = choice\n",
    "        tokens_lower[i] = choice.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_8 = ['be', 'ber', 'ter', 'se']\n",
    "\n",
    "def check_bilangan(word):\n",
    "    if re.findall(_expressions['hypen'], word.lower()):\n",
    "        stemmed = sastrawi.stem(word)\n",
    "        splitted = word.split('-')\n",
    "        for s in start_8:\n",
    "            if word.startswith(s) and f'{s}{stemmed}' == splitted[0] and stemmed == splitted[1]:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def augment_8_0(t, row):\n",
    "    text, tokens, tokens_lower = row\n",
    "    for i in range(len(tokens)):\n",
    "        if check_bilangan(tokens[i]):\n",
    "            word = tokens[i].split('-')[0]\n",
    "            t[i][0] = word\n",
    "            t[i][1] = 8\n",
    "            tokens[i] = word\n",
    "            tokens_lower[i] = word.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sendi_list = [\n",
    "    'akan',\n",
    "    'kepada',\n",
    "    'terhadap',\n",
    "    'bagi',\n",
    "    'untuk',\n",
    "    'dari',\n",
    "    'daripada',\n",
    "    'di',\n",
    "    'dengan',\n",
    "    'hingga',\n",
    "    'sampai',\n",
    "    'ke',\n",
    "    'kepada',\n",
    "    'oleh',\n",
    "    'pada',\n",
    "    'sejak',\n",
    "    'seperti',\n",
    "    'umpama',\n",
    "    'bak',\n",
    "    'tentang',\n",
    "    'laksanabagai',\n",
    "    'semenjak',\n",
    "    'dalam',\n",
    "    'antara',\n",
    "]\n",
    "set_sendi_list = set(sendi_list)\n",
    "\n",
    "def augment_9_0(t, row):\n",
    "    text, tokens, tokens_lower, sendi = row\n",
    "    for word in sendi:\n",
    "        i = tokens_lower.index(word)\n",
    "        negate = list(set_sendi_list - {word})\n",
    "        choice = random.choice(negate)\n",
    "        t[i][0] = choice\n",
    "        t[i][1] = 9\n",
    "        tokens[i] = choice\n",
    "        tokens_lower[i] = choice.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_10_0(t, row):\n",
    "    text, tokens, tokens_lower, penjodoh = row\n",
    "    for word in penjodoh:\n",
    "        try:\n",
    "            i = tokens_lower.index(word)\n",
    "            negate = list(set_penjodoh_bilangan - {word})\n",
    "            choice = random.choice(negate)\n",
    "            t[i][0] = choice\n",
    "            t[i][1] = 10\n",
    "            tokens[i] = choice\n",
    "            tokens_lower[i] = choice.lower()\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            pass\n",
    "        \n",
    "def augment_10_1(t, row):\n",
    "    text, tokens, tokens_lower, penjodoh = row\n",
    "    for word in penjodoh:\n",
    "        try:\n",
    "            i = tokens_lower.index(word)\n",
    "            negate = list(set_sepenjodoh_bilangan - {word})\n",
    "            choice = random.choice(negate)\n",
    "            t[i][0] = choice\n",
    "            t[i][1] = 10\n",
    "            tokens[i] = choice\n",
    "            tokens_lower[i] = choice.lower()\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "gantinama_list = [\n",
    "    'aku',\n",
    "    'saya',\n",
    "    'hamba',\n",
    "    'patik',\n",
    "    'beta',\n",
    "    'kami',\n",
    "    'kita',\n",
    "    'anda',\n",
    "    'awak',\n",
    "    'engkau',\n",
    "    'tuanku',\n",
    "    'kalian',\n",
    "    'kamu',\n",
    "    'baginda',\n",
    "    'beliau',\n",
    "    'mereka',\n",
    "    'ini',\n",
    "    'itu',\n",
    "    'sini',\n",
    "    'situ',\n",
    "    'sana',\n",
    "    'kini',\n",
    "    'dia',\n",
    "    'kau',\n",
    "]\n",
    "set_gantinama_list = set(gantinama_list)\n",
    "\n",
    "def augment_11_0(t, row):\n",
    "    text, tokens, tokens_lower, nama = row\n",
    "    for word in nama:\n",
    "        i = tokens_lower.index(word)\n",
    "        negate = list(set_gantinama_list - {word})\n",
    "        choice = random.choice(negate)\n",
    "        t[i][0] = choice\n",
    "        t[i][1] = 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_12_0(t, row):\n",
    "    text, tokens, tokens_lower, tagging = row\n",
    "    for i in range(len(tokens) - 2):\n",
    "        if tagging[i] == 'ADV' \\\n",
    "        and tagging[i + 1] in ['PRON', 'NOUN'] \\\n",
    "        and tagging[i + 2] in ['VERB', 'NOUN'] \\\n",
    "        and tokens_lower[i] in ['telah', 'mesti']:\n",
    "            v = f'di{tokens[i + 2]}'\n",
    "            n = f'oleh {tokens[i + 1]}'\n",
    "            t[i][1] = 12\n",
    "            t[i + 1][0] = v\n",
    "            t[i + 1][1] = 12\n",
    "            t[i + 2][0] = n\n",
    "            t[i + 2][1] = 12\n",
    "            \n",
    "def augment_12_1(t, row):\n",
    "    text, tokens, tokens_lower, tagging = row\n",
    "    for i in range(len(tokens) - 1):\n",
    "        if tagging[i] == 'PRON' and tagging[i + 1] == 'VERB' and sastrawi.stem(tokens[i + 1]) == tokens[i + 1]:\n",
    "            v = f'men{tokens[i + 1]}'\n",
    "            if sastrawi.stem(v) == v:\n",
    "                v = f'mem{tokens[i + 1]}'\n",
    "            t[i][1] = 12\n",
    "            t[i + 1][0] = v\n",
    "            t[i + 1][1] = 12\n",
    "            \n",
    "def augment_12_2(t, row):\n",
    "    text, tokens, tokens_lower, tagging = row\n",
    "    for i in range(len(tokens) - 2):\n",
    "        if tagging[i] == 'VERB' \\\n",
    "        and tagging[i + 1] in ['ADP'] \\\n",
    "        and tagging[i + 2] in ['PRON', 'NOUN'] \\\n",
    "        and tokens_lower[i + 1] in ['oleh']:\n",
    "            v = sastrawi.stem(tokens[i])\n",
    "            t[i][0] = tokens[i + 2]\n",
    "            t[i][1] = 12\n",
    "            t[i + 1][0] = v\n",
    "            t[i + 1][1] = 12\n",
    "            t[i + 2][0] = ''\n",
    "            t[i + 2][1] = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tanya_list = [\n",
    "    'kenapa',\n",
    "    'bila',\n",
    "    'siapa',\n",
    "    'mengapa',\n",
    "    'apa',\n",
    "    'bagaimana',\n",
    "    'berapa',\n",
    "    'mana',\n",
    "]\n",
    "kah_tanya_list = [f'{w}kah' for w in tanya_list]\n",
    "combined = tanya_list + kah_tanya_list\n",
    "set_combined = set(combined)\n",
    "\n",
    "def augment_13_0(t, row):\n",
    "    text, tokens, tokens_lower, tanya = row\n",
    "    for word in tanya:\n",
    "        i = tokens_lower.index(word)\n",
    "        negate = list(set_combined - {word})\n",
    "        choice = random.choice(negate)\n",
    "        t[i][0] = choice\n",
    "        t[i][1] = 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "punc = '.?!,;:'\n",
    "set_punc = set(punc)\n",
    "\n",
    "def augment_14_0(t, row):\n",
    "    text, tokens, tokens_lower, p = row\n",
    "    for word in p:\n",
    "        i = tokens_lower.index(word)\n",
    "        negate = list(set_punc - {word})\n",
    "        choice = random.choice(negate)\n",
    "        t[i][0] = choice\n",
    "        t[i][1] = 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_15 = ['ber', 'ter', 'me', 'men']\n",
    "\n",
    "def check_tak_transitif(word):\n",
    "    stemmed = sastrawi.stem(word)\n",
    "    for s in start_15:\n",
    "        if word.startswith(s) and f'{s}{stemmed}' == word:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def augment_15_0(t, row):\n",
    "    text, tokens, tokens_lower, tagging = row\n",
    "    for i in range(len(tokens) - 1):\n",
    "        if tagging[i] == 'VERB' \\\n",
    "        and tagging[i + 1] not in ['PRON', 'NOUN'] \\\n",
    "        and check_tak_transitif(tokens[i]):\n",
    "            t[i][0] = sastrawi.stem(tokens[i])\n",
    "            t[i][1] = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_end = {'me': 'kan', 'mem': 'kan', 'men': 'kan', 'mem': '', 'me': '', 'men': ''}\n",
    "\n",
    "def check_transitif(word):\n",
    "    stemmed = sastrawi.stem(word)\n",
    "    for k, v in start_end.items():\n",
    "        if word.startswith(k) and word.endswith(v) and f'{k}{stemmed}{v}' == word:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def augment_16_0(t, row):\n",
    "    text, tokens, tokens_lower, tagging = row\n",
    "    for i in range(len(tokens) - 2):\n",
    "        if tagging[i] in ['PRON', 'NOUN'] \\\n",
    "        and tagging[i + 1] == 'VERB' \\\n",
    "        and tagging[i + 2] in ['PRON', 'NOUN'] \\\n",
    "        and check_transitif(tokens[i + 1]):\n",
    "            t[i][1] = 16\n",
    "            t[i + 1][0] = sastrawi.stem(tokens[i + 1])\n",
    "            t[i + 1][1] = 16\n",
    "            t[i + 2][1] = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_17_0(t, row):\n",
    "    text, tokens, tokens_lower = row\n",
    "    for i in range(len(tokens)):\n",
    "        if tokens_lower[i] in synonyms:\n",
    "            w = random.choice(synonyms[tokens_lower[i]])\n",
    "            t[i][0] = w\n",
    "            t[i][1] = 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2037249"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('filtered-dumping-wiki.txt') as fopen:\n",
    "    data = list(filter(None, fopen.read().split('\\n')))\n",
    "    \n",
    "data = [i for i in data if len(i) >= 2]\n",
    "\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra = [\n",
    "    'gunung itu sangat tinggi',\n",
    "    'dapat markah yang tertinggi',\n",
    "    'jom mengaji agar kita pandai',\n",
    "    'berjuta-juta rakyat malaysia',\n",
    "    'aku sayang akan engkau',\n",
    "    '2 buah kereta',\n",
    "    'jom mengaji agar kita pandai',\n",
    "    'Cerpen itu telah saya karang.', \n",
    "    'Latihan itu mesti kau buat.',\n",
    "    'Kereta itu saya beli daripada Ali.',\n",
    "    'Surat itu dihantar oleh abang semalam.',\n",
    "    'Kamu berasal dari mana?',\n",
    "    'jom mengaji agar kita pandai.',\n",
    "    'Cerpen itu telah saya karang.', \n",
    "    'Latihan itu mesti kau buat.',\n",
    "    'Kereta itu saya beli daripada Ali.',\n",
    "    'Surat itu dihantar oleh abang semalam.',\n",
    "    'Jangan melompat.'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 29/2037249 [00:22<438:19:20,  1.29it/s]"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "threshold = 0.5\n",
    "for text in tqdm(data):\n",
    "    try:\n",
    "        tokens = tokenizer(text)\n",
    "        t = reset_t(tokens)\n",
    "        t_ = copy.deepcopy(t)\n",
    "        tokens_lower = tokenizer(text.lower())\n",
    "        tagging, indexing = malaya.stack.voting_stack([model] * 3, ' '.join(tokens))\n",
    "        graph = malaya.dependency.dependency_graph(tagging, indexing)\n",
    "\n",
    "        pos_tagging = malaya.stack.voting_stack([pos] * 3, ' '.join(tokens))\n",
    "        pos_tagging = list(zip(*pos_tagging))[1]\n",
    "\n",
    "        r = (t, tokens, tokens_lower, graph)\n",
    "        if random.random() > threshold:\n",
    "            augment_3_0(t_, r)\n",
    "\n",
    "        set_tokens = set(tokens_lower)\n",
    "        r_penjodoh_bilangan = set_tokens & set_penjodoh_bilangan\n",
    "        r_sepenjodoh_bilangan = set_tokens & set_sepenjodoh_bilangan\n",
    "        r_reserved = set_tokens & set_reserved_4\n",
    "        r = (t, tokens, tokens_lower, r_penjodoh_bilangan | r_sepenjodoh_bilangan)\n",
    "        \n",
    "        if random.random() > threshold:\n",
    "            augment_4_1(t_, r)\n",
    "        if random.random() > threshold:\n",
    "            augment_4_0(t_, r)\n",
    "\n",
    "        r = (t, tokens, tokens_lower, r_reserved)\n",
    "        if random.random() > threshold:\n",
    "            augment_4_2(t_, r)\n",
    "\n",
    "        set_tokens = set(tokens_lower)\n",
    "        r_penguat_list = set_tokens & set_penguat_list\n",
    "        r = (t, tokens, tokens_lower, r_penguat_list)\n",
    "        if random.random() > threshold:\n",
    "            augment_5_0(t_, r)\n",
    "\n",
    "        set_tokens = set(tokens_lower)\n",
    "        r_penguat_list = set_tokens & set_penguat_list\n",
    "        r = (t, tokens, tokens_lower, r_penguat_list)\n",
    "        if random.random() > threshold:\n",
    "            augment_6_0(t_, r)\n",
    "\n",
    "        set_tokens = set(tokens_lower)\n",
    "        r_hubung_list = set_tokens & set_hubung_list\n",
    "        r = (t, tokens, tokens_lower, r_hubung_list)\n",
    "        if random.random() > threshold:\n",
    "            augment_7_0(t_, r)\n",
    "\n",
    "        r = (t, tokens, tokens_lower)\n",
    "        if random.random() > threshold:\n",
    "            augment_8_0(t_, r)\n",
    "\n",
    "        set_tokens = set(tokens_lower)\n",
    "        r_sendi_list = set_tokens & set_sendi_list\n",
    "        r = (t, tokens, tokens_lower, r_sendi_list)\n",
    "        if random.random() > threshold:\n",
    "            augment_9_0(t_, r)\n",
    "\n",
    "        set_tokens = set(tokens_lower)\n",
    "        r_penjodoh_bilangan = set_tokens & set_penjodoh_bilangan\n",
    "        r_sepenjodoh_bilangan = set_tokens & set_sepenjodoh_bilangan\n",
    "        r = (t, tokens, tokens_lower, r_penjodoh_bilangan)\n",
    "        if random.random() > threshold:\n",
    "            augment_10_0(t_, r)\n",
    "        r = (t, tokens, tokens_lower, r_sepenjodoh_bilangan)\n",
    "        if random.random() > threshold:\n",
    "            augment_10_1(t_, r)\n",
    "\n",
    "        set_tokens = set(tokens_lower)\n",
    "        r_gantinama_list = set_tokens & set_gantinama_list\n",
    "        r = (t, tokens, tokens_lower, r_gantinama_list)\n",
    "        if random.random() > threshold:\n",
    "            augment_11_0(t_, r)\n",
    "\n",
    "        set_tokens = set(tokens_lower)\n",
    "        r = (t, tokens, tokens_lower, pos_tagging)\n",
    "        if random.random() > threshold:\n",
    "            augment_12_0(t_, r)\n",
    "        a = list(zip(*t_))[1]\n",
    "        if 12 not in a:\n",
    "            if random.random() > threshold:\n",
    "                augment_12_1(t_, r)\n",
    "        a = list(zip(*t_))[1]\n",
    "        if 12 not in a:\n",
    "            if random.random() > threshold:\n",
    "                augment_12_2(t_, r)\n",
    "\n",
    "        set_tokens = set(tokens_lower)\n",
    "        r_set_combined = set_tokens & set_combined\n",
    "        r = (t, tokens, tokens_lower, r_set_combined)\n",
    "        if random.random() > threshold:\n",
    "            augment_13_0(t_, r)\n",
    "\n",
    "        set_tokens = set(tokens_lower)\n",
    "        r_set_punc = set_tokens & set_punc\n",
    "        r = (t, tokens, tokens_lower, r_set_punc)\n",
    "        if random.random() > threshold:\n",
    "            augment_14_0(t_, r)\n",
    "\n",
    "        set_tokens = set(tokens_lower)\n",
    "        r = (t, tokens, tokens_lower, pos_tagging)\n",
    "        if random.random() > threshold:\n",
    "            augment_15_0(t_, r)\n",
    "\n",
    "        r = (t, tokens, tokens_lower, pos_tagging)\n",
    "        if random.random() > threshold:\n",
    "            augment_16_0(t_, r)\n",
    "\n",
    "        r = (t, tokens, tokens_lower)\n",
    "        if random.random() > 0.8:\n",
    "            augment_17_0(t_, r)\n",
    "\n",
    "        results.append((t, t_))\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([['Dirk', 2],\n",
       "   ['Jan', 2],\n",
       "   ['Klaas', 2],\n",
       "   ['\"', 2],\n",
       "   ['Klaas-Jan', 2],\n",
       "   ['\"', 2],\n",
       "   ['Huntelaar', 2],\n",
       "   ['(', 2],\n",
       "   ['lahir', 2],\n",
       "   ['12', 2],\n",
       "   ['Ogos', 2],\n",
       "   ['1983', 2],\n",
       "   [')', 2],\n",
       "   ['merupakan', 2],\n",
       "   ['pemain', 2],\n",
       "   ['bola', 2],\n",
       "   ['sepak', 2],\n",
       "   ['Belanda', 2],\n",
       "   ['yang', 2],\n",
       "   ['bermain', 2],\n",
       "   ['di', 2],\n",
       "   ['posisi', 2],\n",
       "   ['penyerang', 2],\n",
       "   ['.', 2]],\n",
       "  [['Dirk', 2],\n",
       "   ['Jan', 2],\n",
       "   ['Klaas', 2],\n",
       "   ['\"', 2],\n",
       "   ['Klaas-Jan', 2],\n",
       "   ['\"', 2],\n",
       "   ['Huntelaar', 2],\n",
       "   ['(', 2],\n",
       "   ['lahir', 2],\n",
       "   ['12', 2],\n",
       "   ['Ogos', 2],\n",
       "   ['1983', 2],\n",
       "   [')', 2],\n",
       "   ['merupakan', 2],\n",
       "   ['pemain', 2],\n",
       "   ['bola', 2],\n",
       "   ['sepak', 2],\n",
       "   ['Belanda', 2],\n",
       "   ['yang', 2],\n",
       "   ['bermain', 2],\n",
       "   ['seperti', 9],\n",
       "   ['posisi', 2],\n",
       "   ['penyerang', 2],\n",
       "   ['!', 14]]),\n",
       " ([['Beliau', 2],\n",
       "   ['kini', 2],\n",
       "   ['bermain', 2],\n",
       "   ['untuk', 2],\n",
       "   ['kelab', 2],\n",
       "   ['Ajax', 2],\n",
       "   ['.', 2]],\n",
       "  [['Beliau', 2],\n",
       "   ['kini', 2],\n",
       "   ['bermain', 2],\n",
       "   ['untuk', 2],\n",
       "   ['kelab', 2],\n",
       "   ['Ajax', 2],\n",
       "   [',', 14]]),\n",
       " ([['Hypo-Arena', 2], ['.', 2]], [['Hypo-Arena', 2], ['.', 2]]),\n",
       " ([['Hypo-Arena', 2],\n",
       "   ['(', 2],\n",
       "   ['dahulu', 2],\n",
       "   ['dikenali', 2],\n",
       "   ['sebagai', 2],\n",
       "   [')', 2],\n",
       "   ['ialah', 2],\n",
       "   ['sebuah', 2],\n",
       "   ['stadium', 2],\n",
       "   ['serba', 2],\n",
       "   ['guna', 2],\n",
       "   ['di', 2],\n",
       "   ['Klagenfurt', 2],\n",
       "   [',', 2],\n",
       "   ['Austria', 2],\n",
       "   ['.', 2]],\n",
       "  [['Hypo-Arena', 2],\n",
       "   ['(', 2],\n",
       "   ['dahulu', 2],\n",
       "   ['dikenali', 2],\n",
       "   ['sebagai', 2],\n",
       "   [')', 2],\n",
       "   ['ialah', 2],\n",
       "   ['secubit', 10],\n",
       "   ['stadium-stadium', 4],\n",
       "   ['serba', 2],\n",
       "   ['guna', 2],\n",
       "   ['di', 2],\n",
       "   ['Klagenfurt', 2],\n",
       "   [':', 14],\n",
       "   ['Austria', 2],\n",
       "   [':', 14]]),\n",
       " ([['Ia', 2],\n",
       "   ['merupakan', 2],\n",
       "   ['stadium', 2],\n",
       "   ['pasukan', 2],\n",
       "   ['Austria', 2],\n",
       "   ['Karnten', 2],\n",
       "   ['.', 2]],\n",
       "  [['Ia', 2],\n",
       "   ['merupakan', 2],\n",
       "   ['stadium', 2],\n",
       "   ['tandan', 10],\n",
       "   ['Austria', 2],\n",
       "   ['Karnten', 2],\n",
       "   ['.', 2]]),\n",
       " ([['Stadium', 2],\n",
       "   ['lama', 2],\n",
       "   ['dikenali', 2],\n",
       "   ['sebagai', 2],\n",
       "   ['Wortherseestadion', 2],\n",
       "   [',', 2],\n",
       "   ['dibina', 2],\n",
       "   ['pada', 2],\n",
       "   ['1960', 2],\n",
       "   ['dan', 2],\n",
       "   ['mempunyai', 2],\n",
       "   ['kapasiti', 2],\n",
       "   ['sebanyak', 2],\n",
       "   ['10,900', 2],\n",
       "   ['.', 2]],\n",
       "  [['Stadium', 2],\n",
       "   ['lama', 2],\n",
       "   ['dikenali', 2],\n",
       "   ['sebagai', 2],\n",
       "   ['Wortherseestadion', 2],\n",
       "   ['.', 14],\n",
       "   ['dibina', 2],\n",
       "   ['pada', 2],\n",
       "   ['1960', 2],\n",
       "   ['dan', 2],\n",
       "   ['mempunyai', 2],\n",
       "   ['kapasiti', 2],\n",
       "   ['sebanyak', 2],\n",
       "   ['10,900', 2],\n",
       "   ['?', 14]]),\n",
       " ([['Ia', 2],\n",
       "   ['dirobohkan', 2],\n",
       "   ['pada', 2],\n",
       "   ['2005', 2],\n",
       "   ['dan', 2],\n",
       "   ['digantikan', 2],\n",
       "   ['dengan', 2],\n",
       "   ['Hypo-Arena', 2],\n",
       "   ['yang', 2],\n",
       "   ['baru', 2],\n",
       "   [',', 2],\n",
       "   ['juga', 2],\n",
       "   ['dikenali', 2],\n",
       "   ['sehingga', 2],\n",
       "   ['30', 2],\n",
       "   ['Jun', 2],\n",
       "   ['2007', 2],\n",
       "   ['dengan', 2],\n",
       "   ['nama', 2],\n",
       "   ['\"', 2],\n",
       "   ['Wortherseestadion', 2],\n",
       "   ['\"', 2],\n",
       "   ['.', 2]],\n",
       "  [['Ia', 2],\n",
       "   ['dirobohkan', 2],\n",
       "   ['kaki', 17],\n",
       "   ['2005', 2],\n",
       "   ['atau', 7],\n",
       "   ['digantikan', 2],\n",
       "   ['dengan', 2],\n",
       "   ['Hypo-Arena', 2],\n",
       "   ['yang', 2],\n",
       "   ['moden', 17],\n",
       "   ['.', 14],\n",
       "   ['pun', 17],\n",
       "   ['dikenali', 2],\n",
       "   ['sehingga', 2],\n",
       "   ['30', 2],\n",
       "   ['Jun', 2],\n",
       "   ['2007', 2],\n",
       "   ['dengan', 2],\n",
       "   ['sebutan', 17],\n",
       "   ['\"', 2],\n",
       "   ['Wortherseestadion', 2],\n",
       "   ['\"', 2],\n",
       "   ['!', 14]]),\n",
       " ([['Ia', 2],\n",
       "   ['adalah', 2],\n",
       "   ['salah', 2],\n",
       "   ['satu', 2],\n",
       "   ['daripada', 2],\n",
       "   ['8', 2],\n",
       "   ['stadium', 2],\n",
       "   ['untuk', 2],\n",
       "   ['UEFA', 2],\n",
       "   ['Euro', 2],\n",
       "   ['2008', 2],\n",
       "   [',', 2],\n",
       "   ['dan', 2],\n",
       "   ['dibina', 2],\n",
       "   ['untuk', 2],\n",
       "   ['menampung', 2],\n",
       "   ['32,000', 2],\n",
       "   ['penonton', 2],\n",
       "   ['.', 2]],\n",
       "  [['Ia', 2],\n",
       "   ['mewakili', 17],\n",
       "   ['bohong', 17],\n",
       "   ['kepingan', 17],\n",
       "   ['daripada', 2],\n",
       "   ['8', 2],\n",
       "   ['stadium', 2],\n",
       "   ['untuk', 2],\n",
       "   ['UEFA', 2],\n",
       "   ['Euro', 2],\n",
       "   ['2008', 2],\n",
       "   ['.', 14],\n",
       "   ['bahawa', 7],\n",
       "   ['dibina', 2],\n",
       "   ['untuk', 2],\n",
       "   ['dilindungi', 17],\n",
       "   ['32,000', 2],\n",
       "   ['saksi', 17],\n",
       "   ['!', 14]]),\n",
       " ([['Selepas', 2],\n",
       "   ['acara', 2],\n",
       "   ['tersebut', 2],\n",
       "   [',', 2],\n",
       "   ['kapasiti', 2],\n",
       "   ['stadium', 2],\n",
       "   ['ini', 2],\n",
       "   ['sedang', 2],\n",
       "   ['dipertimbangkan', 2],\n",
       "   ['untuk', 2],\n",
       "   ['dikurangkan', 2],\n",
       "   ['kepada', 2],\n",
       "   ['12,500', 2],\n",
       "   ['.', 2]],\n",
       "  [['Selepas', 2],\n",
       "   ['acara', 2],\n",
       "   ['tersebut sangat', 6],\n",
       "   [',', 2],\n",
       "   ['kapasiti', 2],\n",
       "   ['stadium', 2],\n",
       "   ['ini', 2],\n",
       "   ['sedang', 2],\n",
       "   ['dipertimbangkan', 2],\n",
       "   ['di', 9],\n",
       "   ['dikurangkan', 2],\n",
       "   ['umpama', 9],\n",
       "   ['12,500', 2],\n",
       "   ['.', 2]]),\n",
       " ([['Stadium', 2],\n",
       "   ['ini', 2],\n",
       "   ['dibuka', 2],\n",
       "   ['secara', 2],\n",
       "   ['rasmi', 2],\n",
       "   ['pada', 2],\n",
       "   ['7', 2],\n",
       "   ['September', 2],\n",
       "   ['2007', 2],\n",
       "   ['dengan', 2],\n",
       "   ['menjadi', 2],\n",
       "   ['tuan', 2],\n",
       "   ['rumah', 2],\n",
       "   ['untuk', 2],\n",
       "   ['perlawanan', 2],\n",
       "   ['persahabatan', 2],\n",
       "   ['di', 2],\n",
       "   ['antara', 2],\n",
       "   ['Austria', 2],\n",
       "   ['dan', 2],\n",
       "   ['Jepun', 2],\n",
       "   ['di', 2],\n",
       "   ['hadapan', 2],\n",
       "   ['26,500', 2],\n",
       "   ['penonton', 2],\n",
       "   ['.', 2]],\n",
       "  [['Stadium', 2],\n",
       "   ['ini', 2],\n",
       "   ['dibuka', 2],\n",
       "   ['secara', 2],\n",
       "   ['rasmi', 2],\n",
       "   ['dalam', 9],\n",
       "   ['7', 2],\n",
       "   ['September', 2],\n",
       "   ['2007', 2],\n",
       "   ['antara', 9],\n",
       "   ['menjadi', 2],\n",
       "   ['rumah', 3],\n",
       "   ['tuan', 3],\n",
       "   ['untuk', 2],\n",
       "   ['persahabatan', 3],\n",
       "   ['perlawanan', 3],\n",
       "   ['tentang', 9],\n",
       "   ['sejak', 9],\n",
       "   ['Austria', 2],\n",
       "   ['apabila', 7],\n",
       "   ['Jepun', 2],\n",
       "   ['di', 2],\n",
       "   ['hadapan', 2],\n",
       "   ['26,500', 2],\n",
       "   ['penonton', 2],\n",
       "   ['.', 2]])]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('dataset-tatabahasa.pkl', 'wb') as fopen:\n",
    "    pickle.dump(results, fopen)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
