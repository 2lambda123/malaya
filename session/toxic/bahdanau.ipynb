{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import collections\n",
    "from sklearn import metrics\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from unidecode import unidecode\n",
    "from nltk.util import ngrams\n",
    "from tqdm import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "permulaan = [\n",
    "    'bel',\n",
    "    'se',\n",
    "    'ter',\n",
    "    'men',\n",
    "    'meng',\n",
    "    'mem',\n",
    "    'memper',\n",
    "    'di',\n",
    "    'pe',\n",
    "    'me',\n",
    "    'ke',\n",
    "    'ber',\n",
    "    'pen',\n",
    "    'per',\n",
    "]\n",
    "\n",
    "hujung = ['kan', 'kah', 'lah', 'tah', 'nya', 'an', 'wan', 'wati', 'ita']\n",
    "\n",
    "def naive_stemmer(word):\n",
    "    assert isinstance(word, str), 'input must be a string'\n",
    "    hujung_result = re.findall(r'^(.*?)(%s)$' % ('|'.join(hujung)), word)\n",
    "    word = hujung_result[0][0] if len(hujung_result) else word\n",
    "    permulaan_result = re.findall(r'^(.*?)(%s)' % ('|'.join(permulaan[::-1])), word)\n",
    "    permulaan_result.extend(re.findall(r'^(.*?)(%s)' % ('|'.join(permulaan)), word))\n",
    "    mula = permulaan_result if len(permulaan_result) else ''\n",
    "    if len(mula):\n",
    "        mula = mula[1][1] if len(mula[1][1]) > len(mula[0][1]) else mula[0][1]\n",
    "    return word.replace(mula, '')\n",
    "\n",
    "def build_dataset(words, n_words):\n",
    "    count = [['GO', 0], ['PAD', 1], ['EOS', 2], ['UNK', 3]]\n",
    "    counter = collections.Counter(words).most_common(n_words)\n",
    "    count.extend(counter)\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    data = list()\n",
    "    unk_count = 0\n",
    "    for word in words:\n",
    "        index = dictionary.get(word, 3)\n",
    "        if index == 0:\n",
    "            unk_count += 1\n",
    "        data.append(index)\n",
    "    count[0][1] = unk_count\n",
    "    reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return data, count, dictionary, reversed_dictionary\n",
    "\n",
    "\n",
    "def classification_textcleaning(string):\n",
    "    string = re.sub(\n",
    "        'http\\S+|www.\\S+',\n",
    "        '',\n",
    "        ' '.join(\n",
    "            [i for i in string.split() if i.find('#') < 0 and i.find('@') < 0]\n",
    "        ),\n",
    "    )\n",
    "    string = unidecode(string).replace('.', ' . ').replace(',', ' , ')\n",
    "    string = re.sub('[^A-Za-z ]+', ' ', string)\n",
    "    string = re.sub(r'[ ]+', ' ', string).strip()\n",
    "    string = ' '.join(\n",
    "        [i for i in re.findall('[\\\\w\\']+|[;:\\-\\(\\)&.,!?\"]', string) if len(i)]\n",
    "    )\n",
    "    string = string.lower().split()\n",
    "    string = [(naive_stemmer(word), word) for word in string]\n",
    "    return (\n",
    "        ' '.join([word[0] for word in string if len(word[0]) > 1]),\n",
    "        ' '.join([word[1] for word in string if len(word[0]) > 1]),\n",
    "    )\n",
    "\n",
    "\n",
    "def str_idx(corpus, dic, maxlen, UNK = 3):\n",
    "    X = np.zeros((len(corpus), maxlen))\n",
    "    for i in range(len(corpus)):\n",
    "        for no, k in enumerate(corpus[i].split()[:maxlen][::-1]):\n",
    "            val = dic[k] if k in dic else UNK\n",
    "            X[i, -1 - no] = val\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40911, 7)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('toxic-bm.csv')\n",
    "df = df.dropna()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(df.shape[0]):\n",
    "    df.iloc[i,0] = classification_textcleaning(df.iloc[i,0])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab from size: 55906\n",
      "Most common words [('yang', 103249), ('anda', 68711), ('tidak', 54325), ('untuk', 50517), ('ada', 39335), ('saya', 32581)]\n",
      "Sample data [68, 96, 78, 4, 41, 126, 276, 2599, 6314, 73] ['jelas', 'gapa', 'gedit', 'yang', 'buat', 'bawah', 'minat', 'tegar', 'tallica', 'nama']\n"
     ]
    }
   ],
   "source": [
    "texts = df.iloc[:,0].tolist()\n",
    "concat = ' '.join(texts).split()\n",
    "vocabulary_size = len(list(set(concat)))\n",
    "data, count, dictionary, rev_dictionary = build_dataset(concat, vocabulary_size)\n",
    "print('vocab from size: %d'%(vocabulary_size))\n",
    "print('Most common words', count[4:10])\n",
    "print('Sample data', data[:10], [rev_dictionary[i] for i in data[:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40911, 6)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "Y = df[list_classes].values\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(\n",
    "        self,\n",
    "        size_layer,\n",
    "        num_layers,\n",
    "        dimension_output,\n",
    "        learning_rate,\n",
    "        dropout,\n",
    "        dict_size,\n",
    "    ):\n",
    "        def cells(size, reuse = False):\n",
    "            return tf.contrib.rnn.DropoutWrapper(\n",
    "                tf.nn.rnn_cell.LSTMCell(\n",
    "                    size,\n",
    "                    initializer = tf.orthogonal_initializer(),\n",
    "                    reuse = reuse,\n",
    "                ),\n",
    "                state_keep_prob = dropout,\n",
    "                output_keep_prob = dropout,\n",
    "            )\n",
    "\n",
    "        self.X = tf.placeholder(tf.int32, [None, None])\n",
    "        self.Y = tf.placeholder(tf.float32, [None, dimension_output])\n",
    "        encoder_embeddings = tf.Variable(\n",
    "            tf.random_uniform([dict_size, size_layer], -1, 1)\n",
    "        )\n",
    "        encoder_embedded = tf.nn.embedding_lookup(encoder_embeddings, self.X)\n",
    "        attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(\n",
    "            num_units = size_layer, memory = encoder_embedded\n",
    "        )\n",
    "        rnn_cells = tf.contrib.seq2seq.AttentionWrapper(\n",
    "            cell = tf.nn.rnn_cell.MultiRNNCell(\n",
    "                [cells(size_layer) for _ in range(num_layers)]\n",
    "            ),\n",
    "            attention_mechanism = attention_mechanism,\n",
    "            attention_layer_size = size_layer,\n",
    "            alignment_history = True,\n",
    "        )\n",
    "        outputs, last_state = tf.nn.dynamic_rnn(\n",
    "            rnn_cells, encoder_embedded, dtype = tf.float32\n",
    "        )\n",
    "        self.alignments = tf.transpose(\n",
    "            last_state.alignment_history.stack(), [1, 2, 0]\n",
    "        )\n",
    "        W = tf.get_variable(\n",
    "            'w',\n",
    "            shape = (size_layer, dimension_output),\n",
    "            initializer = tf.glorot_uniform_initializer(),\n",
    "        )\n",
    "        b = tf.get_variable(\n",
    "            'b',\n",
    "            shape = (dimension_output),\n",
    "            initializer = tf.zeros_initializer(),\n",
    "        )\n",
    "        self.logits = tf.add(tf.matmul(outputs[:, -1], W), b, name = 'logits')\n",
    "        self.cost = tf.reduce_mean(\n",
    "            tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                logits = self.logits, labels = self.Y\n",
    "            )\n",
    "        )\n",
    "        self.optimizer = tf.train.AdamOptimizer(\n",
    "            learning_rate = learning_rate\n",
    "        ).minimize(self.cost)\n",
    "        correct_prediction = tf.equal(tf.round(tf.nn.sigmoid(self.logits)), tf.round(self.Y))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        self.attention = tf.nn.softmax(\n",
    "            tf.reduce_sum(self.alignments[0], 1), name = 'alphas'\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bahdanau/model.ckpt'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size_layer = 256\n",
    "num_layers = 2\n",
    "dimension_output = Y.shape[1]\n",
    "learning_rate = 1e-4\n",
    "batch_size = 32\n",
    "dropout = 0.8\n",
    "maxlen = 80\n",
    "\n",
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "model = Model(\n",
    "    size_layer,\n",
    "    num_layers,\n",
    "    dimension_output,\n",
    "    learning_rate,\n",
    "    dropout,\n",
    "    len(dictionary),\n",
    ")\n",
    "sess.run(tf.global_variables_initializer())\n",
    "saver = tf.train.Saver(tf.trainable_variables())\n",
    "saver.save(sess, 'bahdanau/model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = str_idx(df.iloc[:,0].tolist(), dictionary, maxlen)\n",
    "train_X, test_X, train_Y, test_Y = train_test_split(\n",
    "    vectors, Y, test_size = 0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 1023/1023 [17:39<00:00,  1.61it/s, accuracy=0.965, cost=0.124] \n",
      "test minibatch loop: 100%|██████████| 256/256 [01:53<00:00,  2.12it/s, accuracy=0.993, cost=0.0291]\n",
      "train minibatch loop:   0%|          | 0/1023 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, pass acc: 0.000000, current acc: 0.973983\n",
      "time taken: 1172.8256270885468\n",
      "epoch: 0, training loss: 0.130629, training acc: 0.963412, valid loss: 0.089508, valid acc: 0.973983\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 1023/1023 [04:34<00:00,  4.51it/s, accuracy=0.965, cost=0.104] \n",
      "test minibatch loop: 100%|██████████| 256/256 [00:18<00:00, 14.11it/s, accuracy=0.993, cost=0.0373]\n",
      "train minibatch loop:   0%|          | 0/1023 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, pass acc: 0.973983, current acc: 0.976081\n",
      "time taken: 293.0026590824127\n",
      "epoch: 1, training loss: 0.082537, training acc: 0.974315, valid loss: 0.078792, valid acc: 0.976081\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 1023/1023 [03:53<00:00,  4.52it/s, accuracy=0.958, cost=0.0941]\n",
      "test minibatch loop: 100%|██████████| 256/256 [00:18<00:00, 14.43it/s, accuracy=1, cost=0.0314]    \n",
      "train minibatch loop:   0%|          | 0/1023 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2, pass acc: 0.976081, current acc: 0.976964\n",
      "time taken: 251.9325828552246\n",
      "epoch: 2, training loss: 0.071557, training acc: 0.976610, valid loss: 0.075631, valid acc: 0.976964\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 1023/1023 [03:52<00:00,  4.49it/s, accuracy=0.958, cost=0.0949]\n",
      "test minibatch loop: 100%|██████████| 256/256 [00:18<00:00, 14.34it/s, accuracy=1, cost=0.0298]    \n",
      "train minibatch loop:   0%|          | 0/1023 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken: 250.3645830154419\n",
      "epoch: 3, training loss: 0.065284, training acc: 0.977995, valid loss: 0.074502, valid acc: 0.976781\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 1023/1023 [03:52<00:00,  4.58it/s, accuracy=0.972, cost=0.0882]\n",
      "test minibatch loop: 100%|██████████| 256/256 [00:18<00:00, 13.78it/s, accuracy=1, cost=0.0202]    \n",
      "train minibatch loop:   0%|          | 0/1023 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4, pass acc: 0.976964, current acc: 0.977107\n",
      "time taken: 250.42156171798706\n",
      "epoch: 4, training loss: 0.059939, training acc: 0.979338, valid loss: 0.073523, valid acc: 0.977107\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 1023/1023 [04:01<00:00,  4.53it/s, accuracy=0.972, cost=0.0826]\n",
      "test minibatch loop: 100%|██████████| 256/256 [00:18<00:00, 14.10it/s, accuracy=1, cost=0.0155]    \n",
      "train minibatch loop:   0%|          | 0/1023 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5, pass acc: 0.977107, current acc: 0.977290\n",
      "time taken: 259.5673520565033\n",
      "epoch: 5, training loss: 0.055244, training acc: 0.980611, valid loss: 0.075217, valid acc: 0.977290\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 1023/1023 [04:10<00:00,  4.16it/s, accuracy=0.972, cost=0.0808] \n",
      "test minibatch loop: 100%|██████████| 256/256 [00:20<00:00, 12.65it/s, accuracy=1, cost=0.0142]     \n",
      "train minibatch loop:   0%|          | 0/1023 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken: 270.63135743141174\n",
      "epoch: 6, training loss: 0.050431, training acc: 0.982159, valid loss: 0.077296, valid acc: 0.977026\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 1023/1023 [03:56<00:00,  4.66it/s, accuracy=0.979, cost=0.0873] \n",
      "test minibatch loop: 100%|██████████| 256/256 [00:18<00:00, 13.94it/s, accuracy=1, cost=0.0132]    \n",
      "train minibatch loop:   0%|          | 0/1023 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken: 254.7599949836731\n",
      "epoch: 7, training loss: 0.046363, training acc: 0.983582, valid loss: 0.080549, valid acc: 0.976883\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 1023/1023 [03:51<00:00,  4.62it/s, accuracy=0.979, cost=0.0923] \n",
      "test minibatch loop: 100%|██████████| 256/256 [00:18<00:00, 14.18it/s, accuracy=1, cost=0.0139]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken: 249.45675706863403\n",
      "epoch: 8, training loss: 0.042123, training acc: 0.985135, valid loss: 0.086130, valid acc: 0.976272\n",
      "\n",
      "break epoch:9\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "EARLY_STOPPING, CURRENT_CHECKPOINT, CURRENT_ACC, EPOCH = 3, 0, 0, 0\n",
    "\n",
    "while True:\n",
    "    lasttime = time.time()\n",
    "    if CURRENT_CHECKPOINT == EARLY_STOPPING:\n",
    "        print('break epoch:%d\\n' % (EPOCH))\n",
    "        break\n",
    "\n",
    "    train_acc, train_loss, test_acc, test_loss = 0, 0, 0, 0\n",
    "    pbar = tqdm(\n",
    "        range(0, len(train_X), batch_size), desc = 'train minibatch loop'\n",
    "    )\n",
    "    for i in pbar:\n",
    "        batch_x = train_X[i : min(i + batch_size, train_X.shape[0])]\n",
    "        batch_y = train_Y[i : min(i + batch_size, train_X.shape[0])]\n",
    "        batch_x_expand = np.expand_dims(batch_x,axis = 1)\n",
    "        acc, cost, _ = sess.run(\n",
    "            [model.accuracy, model.cost, model.optimizer],\n",
    "            feed_dict = {\n",
    "                model.Y: batch_y,\n",
    "                model.X: batch_x\n",
    "            },\n",
    "        )\n",
    "        assert not np.isnan(cost)\n",
    "        train_loss += cost\n",
    "        train_acc += acc\n",
    "        pbar.set_postfix(cost = cost, accuracy = acc)\n",
    "\n",
    "    pbar = tqdm(range(0, len(test_X), batch_size), desc = 'test minibatch loop')\n",
    "    for i in pbar:\n",
    "        batch_x = test_X[i : min(i + batch_size, test_X.shape[0])]\n",
    "        batch_y = test_Y[i : min(i + batch_size, test_X.shape[0])]\n",
    "        batch_x_expand = np.expand_dims(batch_x,axis = 1)\n",
    "        acc, cost = sess.run(\n",
    "            [model.accuracy, model.cost],\n",
    "            feed_dict = {\n",
    "                model.Y: batch_y,\n",
    "                model.X: batch_x\n",
    "            },\n",
    "        )\n",
    "        test_loss += cost\n",
    "        test_acc += acc\n",
    "        pbar.set_postfix(cost = cost, accuracy = acc)\n",
    "\n",
    "    train_loss /= len(train_X) / batch_size\n",
    "    train_acc /= len(train_X) / batch_size\n",
    "    test_loss /= len(test_X) / batch_size\n",
    "    test_acc /= len(test_X) / batch_size\n",
    "\n",
    "    if test_acc > CURRENT_ACC:\n",
    "        print(\n",
    "            'epoch: %d, pass acc: %f, current acc: %f'\n",
    "            % (EPOCH, CURRENT_ACC, test_acc)\n",
    "        )\n",
    "        CURRENT_ACC = test_acc\n",
    "        CURRENT_CHECKPOINT = 0\n",
    "    else:\n",
    "        CURRENT_CHECKPOINT += 1\n",
    "\n",
    "    print('time taken:', time.time() - lasttime)\n",
    "    print(\n",
    "        'epoch: %d, training loss: %f, training acc: %f, valid loss: %f, valid acc: %f\\n'\n",
    "        % (EPOCH, train_loss, train_acc, test_loss, test_acc)\n",
    "    )\n",
    "    EPOCH += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test minibatch loop: 100%|██████████| 256/256 [00:37<00:00,  6.37it/s]\n"
     ]
    }
   ],
   "source": [
    "stack = []\n",
    "pbar = tqdm(range(0, len(test_X), batch_size), desc = 'test minibatch loop')\n",
    "for i in pbar:\n",
    "    batch_x = test_X[i : min(i + batch_size, test_X.shape[0])]\n",
    "    batch_y = test_Y[i : min(i + batch_size, test_X.shape[0])]\n",
    "    stack.append(sess.run(tf.nn.sigmoid(model.logits),\n",
    "                         feed_dict = {model.X: batch_x}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.75      0.58      0.65       797\n",
      "          1       0.46      0.36      0.41        77\n",
      "          2       0.71      0.59      0.64       423\n",
      "          3       0.00      0.00      0.00        30\n",
      "          4       0.68      0.56      0.61       384\n",
      "          5       0.40      0.23      0.29        74\n",
      "\n",
      "avg / total       0.69      0.54      0.60      1785\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(test_Y,np.around(np.concatenate(stack,axis=0))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.9814238 , 0.07019142, 0.78172404, 0.02179804, 0.9483101 ,\n",
       "        0.30378604]], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'bodoh lah anti sosial'\n",
    "new_vector = str_idx([classification_textcleaning(text)[0]], dictionary, maxlen)\n",
    "sess.run(tf.nn.sigmoid(model.logits), feed_dict={model.X:new_vector})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('bahdanau-toxic.json','w') as fopen:\n",
    "    fopen.write(json.dumps({'dictionary':dictionary,'reverse_dictionary':rev_dictionary}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bahdanau/model.ckpt'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saver.save(sess, 'bahdanau/model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_graph(model_dir, output_node_names):\n",
    "\n",
    "    if not tf.gfile.Exists(model_dir):\n",
    "        raise AssertionError(\n",
    "            \"Export directory doesn't exists. Please specify an export \"\n",
    "            'directory: %s' % model_dir\n",
    "        )\n",
    "\n",
    "    checkpoint = tf.train.get_checkpoint_state(model_dir)\n",
    "    input_checkpoint = checkpoint.model_checkpoint_path\n",
    "\n",
    "    absolute_model_dir = '/'.join(input_checkpoint.split('/')[:-1])\n",
    "    output_graph = absolute_model_dir + '/frozen_model.pb'\n",
    "    clear_devices = True\n",
    "    with tf.Session(graph = tf.Graph()) as sess:\n",
    "        saver = tf.train.import_meta_graph(\n",
    "            input_checkpoint + '.meta', clear_devices = clear_devices\n",
    "        )\n",
    "        saver.restore(sess, input_checkpoint)\n",
    "        output_graph_def = tf.graph_util.convert_variables_to_constants(\n",
    "            sess,\n",
    "            tf.get_default_graph().as_graph_def(),\n",
    "            output_node_names.split(','),\n",
    "        )\n",
    "        with tf.gfile.GFile(output_graph, 'wb') as f:\n",
    "            f.write(output_graph_def.SerializeToString())\n",
    "        print('%d ops in the final graph.' % len(output_graph_def.node))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from bahdanau/model.ckpt\n",
      "INFO:tensorflow:Froze 11 variables.\n",
      "INFO:tensorflow:Converted 11 variables to const ops.\n",
      "458 ops in the final graph.\n"
     ]
    }
   ],
   "source": [
    "strings = ','.join(\n",
    "    [\n",
    "        n.name\n",
    "        for n in tf.get_default_graph().as_graph_def().node\n",
    "        if ('Variable' in n.op\n",
    "        or 'Placeholder' in n.name\n",
    "        or 'logits' in n.name\n",
    "        or 'alphas' in n.name)\n",
    "        and 'Adam' not in n.name\n",
    "        and 'beta' not in n.name\n",
    "    ]\n",
    ")\n",
    "\n",
    "freeze_graph('bahdanau', strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_graph(frozen_graph_filename):\n",
    "    with tf.gfile.GFile(frozen_graph_filename, 'rb') as f:\n",
    "        graph_def = tf.GraphDef()\n",
    "        graph_def.ParseFromString(f.read())\n",
    "    with tf.Graph().as_default() as graph:\n",
    "        tf.import_graph_def(graph_def)\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([[ 3.9710352, -2.6655428,  1.439822 , -4.383592 ,  2.9508197,\n",
       "         -1.1368153]], dtype=float32),\n",
       " array([3.1028743e-17, 3.1028743e-17, 3.1028743e-17, 3.1028743e-17,\n",
       "        3.1028743e-17, 3.1028743e-17, 3.1028743e-17, 3.1028743e-17,\n",
       "        3.1028743e-17, 3.1028743e-17, 3.1028743e-17, 3.1028743e-17,\n",
       "        3.1028743e-17, 3.1028743e-17, 3.1028743e-17, 3.1028743e-17,\n",
       "        3.1028743e-17, 3.1028743e-17, 3.1028743e-17, 3.1028743e-17,\n",
       "        3.1028743e-17, 3.1028743e-17, 3.1028743e-17, 3.1028743e-17,\n",
       "        3.1028743e-17, 3.1028743e-17, 3.1028743e-17, 3.1028743e-17,\n",
       "        3.1028743e-17, 3.1028743e-17, 3.1028743e-17, 3.1028743e-17,\n",
       "        3.1028743e-17, 3.1028743e-17, 3.1028743e-17, 3.1028743e-17,\n",
       "        3.1028743e-17, 3.1028743e-17, 3.1028743e-17, 3.1028743e-17,\n",
       "        3.1028743e-17, 3.1028743e-17, 3.1028743e-17, 3.1028743e-17,\n",
       "        3.1028743e-17, 3.1028743e-17, 3.1028743e-17, 3.1028743e-17,\n",
       "        3.1028743e-17, 3.1028743e-17, 3.1028743e-17, 3.1028743e-17,\n",
       "        3.1028743e-17, 3.1028743e-17, 3.1028743e-17, 3.1028743e-17,\n",
       "        3.1028743e-17, 3.1028743e-17, 3.1028743e-17, 3.1028743e-17,\n",
       "        3.1028743e-17, 3.1028743e-17, 3.1028743e-17, 3.1028743e-17,\n",
       "        3.1028743e-17, 3.1028743e-17, 3.1028743e-17, 3.1028743e-17,\n",
       "        3.1028743e-17, 3.1028743e-17, 3.1028743e-17, 3.1028743e-17,\n",
       "        3.1028743e-17, 3.1028743e-17, 3.1028743e-17, 3.1028743e-17,\n",
       "        3.1028743e-17, 8.4309846e-01, 3.1896447e-16, 1.5690155e-01],\n",
       "       dtype=float32)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = load_graph('bahdanau/frozen_model.pb')\n",
    "x = g.get_tensor_by_name('import/Placeholder:0')\n",
    "logits = g.get_tensor_by_name('import/logits:0')\n",
    "alphas = g.get_tensor_by_name('import/alphas:0')\n",
    "test_sess = tf.InteractiveSession(graph = g)\n",
    "test_sess.run([logits, alphas], feed_dict = {x: new_vector})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = classification_textcleaning(text)\n",
    "new_vector = str_idx([text[0]], dictionary, len(text[0].split()))\n",
    "result = test_sess.run([tf.nn.softmax(logits), alphas], feed_dict = {x: new_vector})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f526de24240>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize = (15, 7))\n",
    "labels = [word for word in text[1].split()]\n",
    "val = [val for val in result[1]]\n",
    "plt.bar(np.arange(len(labels)), val)\n",
    "plt.xticks(np.arange(len(labels)), labels, rotation = 'vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
