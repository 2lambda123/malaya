{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['finetune-t5-small-noisy-bahasa-cased/checkpoint-150000',\n",
       " 'finetune-t5-small-noisy-bahasa-cased/checkpoint-160000',\n",
       " 'finetune-t5-small-noisy-bahasa-cased/checkpoint-170000',\n",
       " 'finetune-t5-small-noisy-bahasa-cased/checkpoint-180000',\n",
       " 'finetune-t5-small-noisy-bahasa-cased/checkpoint-190000']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from glob import glob\n",
    "\n",
    "checkpoints = sorted(glob('finetune-t5-small-noisy-bahasa-cased/checkpoint-*'))\n",
    "checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained('mesolitica/t5-small-standard-bahasa-cased')\n",
    "model = T5ForConditionalGeneration.from_pretrained(checkpoints[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> Hi guys! I noticed yesterday and today many have got these cookies, right? So today I want to share some of our first batch of mortem posts:</s>\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer.encode('terjemah Melayu ke Inggeris: Hi guys! I noticed semalam & harini dah ramai yang dapat cookies ni kan. So harini i nak share some post mortem of our first batch:', return_tensors = 'pt')\n",
    "outputs = model.generate(input_ids, max_length = 100)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> Hai kawan-kawan! Saya perhatikan semalam & harini dah ramai yang dapat cookies ni kan. Jadi harini saya nak kongsi beberapa post mortem kumpulan pertama kami:</s>\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer.encode('terjemah Inggeris ke Melayu: Hi guys! I noticed semalam & harini dah ramai yang dapat cookies ni kan. So harini i nak share some post mortem of our first batch:', return_tensors = 'pt')\n",
    "outputs = model.generate(input_ids, max_length = 100)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> I don't understand la</s>\n",
      "<pad> At 8 o'clock in the market, it's a great place to choose.</s>\n",
      "<pad> So it's a fucking shit.</s>\n",
      "<pad> Where do you want to go?</s>\n",
      "<pad> It's like taking half a day</s>\n",
      "<pad> I've been making this gengs and sold haha salad only k and haha drinks only k.</s>\n",
      "<pad> I'll see what tickets from Kuala Lumpur are at.</s>\n",
      "<pad> Imagine PH and win pru-14. There are many back doorways. Last-last Ismail Sabri goes up. That's why I don't give a fk about politics anymore. I swear I'm fk up.</s>\n"
     ]
    }
   ],
   "source": [
    "strings = [\n",
    "    'ak tak paham la',\n",
    "    'jam 8 di pasar KK memang org ramai üòÇ, pandai dia pilih tmpt.',\n",
    "    'Jadi haram jadahüòÄüòÉü§≠',\n",
    "    'nak gi mana tuu',\n",
    "    'Macam nak ambil half day',\n",
    "    'jadi aku tadi bikin ini gengs dan dijual haha salad only k dan haha drinks only k',\n",
    "    'nanti aku tengok dulu tiket dari Kuala Lumpur pukul berapa ada ya',\n",
    "    \"Bayangkan PH dan menang pru-14. Pastu macam-macam pintu belakang ada. Last-last Ismail Sabri naik. That's why I don't give a fk about politics anymore. Sumpah dah fk up dah.\",\n",
    "]\n",
    "for s in strings:\n",
    "    input_ids = tokenizer.encode(f'terjemah Melayu ke Inggeris: {s}', return_tensors = 'pt')\n",
    "    outputs = model.generate(input_ids, max_length = 100)\n",
    "    print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> u ni, bercakap lah</s>\n",
      "<pad> baru sahaja menghadiri majlis perkahwinan sepupu saya. pelik jugak dia buat biasa2 je sebab gaya hidupnya kelihatan mewah. kemudian saya dapati mereka akan berbulan madu 3 minggu. keputusan pintar <unk> </s>\n",
      "<pad> Saya selepas melihat video ini: saya dapnya burger benjo extra mayo</s>\n",
      "<pad> power lah even shopback datang edmw riao</s>\n",
      "<pad> Hai kawan-kawan! Saya perhatikan semalam & harini dah ramai yang dapat cookies ni kan. Jadi harini saya nak kongsi beberapa post mortem kumpulan pertama kami:</s>\n"
     ]
    }
   ],
   "source": [
    "strings = [\n",
    "    'u ni, talk properly lah',\n",
    "    \"just attended my cousin's wedding. pelik jugak dia buat majlis biasa2 je sebab her lifestyle looks lavish. then i found out they're going on a 3 weeks honeymoon. smart decision üëç\",\n",
    "    'Me after seeing this video: mm dapnya burger benjo extra mayo',\n",
    "    'power lah even shopback come to edmw riao',\n",
    "    'Hi guys! I noticed semalam & harini dah ramai yang dapat cookies ni kan. So harini i nak share some post mortem of our first batch:',\n",
    "]\n",
    "for s in strings:\n",
    "    input_ids = tokenizer.encode(f'terjemah Inggeris ke Melayu: {s}', return_tensors = 'pt')\n",
    "    outputs = model.generate(input_ids, max_length = 100)\n",
    "    print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n"
     ]
    }
   ],
   "source": [
    "model.push_to_hub('finetune-noisy-translation-t5-small-bahasa-cased', organization='mesolitica')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.push_to_hub('finetune-noisy-translation-t5-small-bahasa-cased', organization='mesolitica')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp -r finetune-t5-tiny-noisy-bahasa-cased/runs finetune-noisy-translation-t5-tiny-bahasa-cased\n",
    "!cd finetune-noisy-translation-t5-tiny-bahasa-cased && git add . && git commit -m 'add tensorboard' && git push"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sacrebleu.metrics import BLEU, CHRF, TER\n",
    "\n",
    "bleu = BLEU()\n",
    "chrf = CHRF(word_order = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6854"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from unidecode import unidecode\n",
    "import json\n",
    "\n",
    "with open('test-noisy-shuffled.json') as fopen:\n",
    "    test = fopen.read().split('\\n')\n",
    "    test = [json.loads(t) for t in test if len(t)]\n",
    "    \n",
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6854/6854 [25:43<00:00,  4.44it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "results_en_ms, filtered_right_en_ms = [], []\n",
    "results_ms_en, filtered_right_ms_en = [], []\n",
    "for i in tqdm(range(len(test))):\n",
    "    t = test[i]['translation']\n",
    "    p = t['prefix']\n",
    "    s = t['src']\n",
    "    tgt = t['tgt']\n",
    "    \n",
    "    input_ids = [{'input_ids': tokenizer.encode(f'{p}{s}', return_tensors = 'pt')[0]}]\n",
    "    padded = tokenizer.pad(input_ids, padding = 'longest')\n",
    "    outputs = model.generate(**padded, max_length = 1000)[0]\n",
    "    o = tokenizer.decode(outputs, skip_special_tokens=True)\n",
    "    if len(o):\n",
    "        if 'Inggeris ke Melayu' in p:\n",
    "            results_en_ms.append(o)\n",
    "            filtered_right_en_ms.append(tgt)\n",
    "        else:\n",
    "            results_ms_en.append(o)\n",
    "            filtered_right_ms_en.append(tgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2937, 3917)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(results_en_ms), len(results_ms_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'name': 'BLEU',\n",
       "  'score': 41.15794003172596,\n",
       "  '_mean': -1.0,\n",
       "  '_ci': -1.0,\n",
       "  '_verbose': '72.2/48.8/34.5/24.8 (BP = 0.988 ratio = 0.988 hyp_len = 63689 ref_len = 64473)',\n",
       "  'bp': 0.9877656378545313,\n",
       "  'counts': [45968, 29622, 19968, 13610],\n",
       "  'totals': [63689, 60752, 57815, 54878],\n",
       "  'sys_len': 63689,\n",
       "  'ref_len': 64473,\n",
       "  'precisions': [72.17572893278275,\n",
       "   48.758888596260206,\n",
       "   34.537749718931074,\n",
       "   24.800466489303545],\n",
       "  'prec_str': '72.2/48.8/34.5/24.8',\n",
       "  'ratio': 0.9878398709537326},\n",
       " chrF2++ = 65.51)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "refs = [filtered_right_en_ms]\n",
    "sys = results_en_ms\n",
    "r = bleu.corpus_score(sys, refs)\n",
    "r.__dict__, chrf.corpus_score(sys, refs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'name': 'BLEU',\n",
       "  'score': 41.83407099646298,\n",
       "  '_mean': -1.0,\n",
       "  '_ci': -1.0,\n",
       "  '_verbose': '71.7/48.7/35.4/26.0 (BP = 0.989 ratio = 0.989 hyp_len = 91952 ref_len = 92985)',\n",
       "  'bp': 0.9888287449603974,\n",
       "  'counts': [65929, 42830, 29766, 20815],\n",
       "  'totals': [91952, 88035, 84118, 80201],\n",
       "  'sys_len': 91952,\n",
       "  'ref_len': 92985,\n",
       "  'precisions': [71.6993648860275,\n",
       "   48.65110467427728,\n",
       "   35.386005373404025,\n",
       "   25.95354172641239],\n",
       "  'prec_str': '71.7/48.7/35.4/26.0',\n",
       "  'ratio': 0.9888906812926817},\n",
       " chrF2++ = 64.52)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "refs = [filtered_right_ms_en]\n",
    "sys = results_ms_en\n",
    "r = bleu.corpus_score(sys, refs)\n",
    "r.__dict__, chrf.corpus_score(sys, refs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
