# FastFormer-Bahasa

Thanks to Google for opensourcing most of the source code to develop FastFormer, https://github.com/Rishit-dagli/Fast-Transformer. Malaya just create custom pretraining on TPU.

**This directory is very lack of comments, understand Tensorflow, Tensorflow estimator, Tensorflow Dataset really helpful**.

## Table of contents
  * [Objective](#objective)
  * [Acknowledgement](#acknowledgement)

## Objective

1. Provide **SMALL** and **BASE** FastFormer for Bahasa.

## Acknowledgement

Thanks to [Im Big](https://www.facebook.com/imbigofficial/), [LigBlou](https://www.facebook.com/ligblou), [Mesolitica](https://mesolitica.com/), [KeyReply](https://www.keyreply.com/) and [TensorFlow Research Cloud](https://www.tensorflow.org/tfrc) for sponsoring AWS, Google and GPU clouds to train FastFormer for Bahasa.

## How-to

1. Generate dataset using BERT repository, follow https://github.com/google-research/bert

