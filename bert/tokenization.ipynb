{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from unidecode import unidecode\n",
    "import malaya\n",
    "import json\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import collections\n",
    "import glob\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = malaya.preprocessing._SocialTokenizer().tokenize\n",
    "rules_normalizer = malaya.texts._tatabahasa.rules_normalizer\n",
    "rejected = ['wkwk', 'http', 'https', 'lolol', 'hahaha']\n",
    "\n",
    "def is_number_regex(s):\n",
    "    if re.match(\"^\\d+?\\.\\d+?$\", s) is None:\n",
    "        return s.isdigit()\n",
    "    return True\n",
    "\n",
    "def detect_money(word):\n",
    "    if word[:2] == 'rm' and is_number_regex(word[2:]):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def preprocessing(string):\n",
    "    string = ''.join(''.join(s)[:2] for _, s in itertools.groupby(unidecode(string)))\n",
    "    tokenized = tokenizer(string)\n",
    "    tokenized = [malaya.stem.naive(w) for w in tokenized]\n",
    "    tokenized = [w.lower() for w in tokenized if len(w) > 1]\n",
    "    tokenized = [w for w in tokenized if all([r not in w for r in rejected])]\n",
    "    tokenized = [rules_normalizer.get(w, w) for w in tokenized]\n",
    "    tokenized = ['<NUM>' if is_number_regex(w) else w for w in tokenized]\n",
    "    tokenized = ['<MONEY>' if detect_money(w) else w for w in tokenized]\n",
    "    return tokenized\n",
    "\n",
    "def build_dataset(words, n_words, atleast = 2):\n",
    "    count = [['[PAD]', 0], ['[UNK]', 1], ['[CLS]', 2], ['[SEP]', 3], ['[MASK]', 4],\n",
    "            ['<S>', 5], ['<T>', 6]]\n",
    "    counter = collections.Counter(words).most_common(n_words)\n",
    "    counter = [i for i in counter if i[1] >= atleast]\n",
    "    count.extend(counter)\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    data = list()\n",
    "    unk_count = 0\n",
    "    for word in words:\n",
    "        index = dictionary.get(word, 3)\n",
    "        if index == 0:\n",
    "            unk_count += 1\n",
    "        data.append(index)\n",
    "    count[0][1] = unk_count\n",
    "    reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return data, count, dictionary, reversed_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment = []\n",
    "for file in glob.glob('Malaya-Dataset/twitter-sentiment/*/*.json'):\n",
    "    with open(file) as fopen:\n",
    "        sentiment.extend(json.load(fopen))\n",
    "        \n",
    "for file in glob.glob('Malaya-Dataset/multidomain-sentiment/*.json'):\n",
    "    with open(file) as fopen:\n",
    "        x = json.load(fopen)\n",
    "        sentiment.extend(x['negative'] + x['positive'])\n",
    "        \n",
    "sentiment.extend(pd.read_csv('Malaya-Dataset/news-sentiment/sentiment-data-v2.csv')['text'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "664429"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "420516"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion = []\n",
    "for file in glob.glob('Malaya-Dataset/emotion/*.json'):\n",
    "    with open(file) as fopen:\n",
    "        emotion.extend(json.load(fopen))\n",
    "        \n",
    "for file in glob.glob('Malaya-Dataset/emotion/translated*'):\n",
    "    with open(file) as fopen:\n",
    "        x = list(filter(None, fopen.read().split('\\n')))\n",
    "        emotion.extend(x)\n",
    "        \n",
    "len(emotion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42023"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_news = []\n",
    "for file in glob.glob('negative/*.json'):\n",
    "    with open(file) as fopen:\n",
    "        fake_news.extend(json.load(fopen))\n",
    "        \n",
    "for file in glob.glob('positive/*.json'):\n",
    "    with open(file) as fopen:\n",
    "        fake_news.extend(json.load(fopen))\n",
    "        \n",
    "len(fake_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9673"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news = []\n",
    "files = glob.glob('news/*.json')\n",
    "for file in files:\n",
    "    with open(file) as fopen:\n",
    "        x = json.load(fopen)\n",
    "    news.extend([i['text'] for i in x if i['language'] != 'ENGLISH' and len(i['text']) > 20])\n",
    "len(news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1663373"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('clean-wiki.txt') as fopen:\n",
    "    wiki = fopen.readlines()\n",
    "wiki = [i for i in wiki if i.count(' ') > 1]\n",
    "len(wiki)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2800014"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = news + fake_news + emotion + sentiment + wiki\n",
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2800014/2800014 [27:12<00:00, 1715.28it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "pbar = tqdm(range(len(texts)))\n",
    "for i in pbar:\n",
    "    texts[i] = preprocessing(texts[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab from size: 932618\n",
      "Most common words [('yang', 1762593), ('saya', 902073), ('pada', 899529), ('untuk', 777441), ('deng', 624771), ('ini', 601432)]\n",
      "Sample data [17578, 708, 1782, 50, 593, 15, 11, 72, 337, 811] ['pergera', 'tenaga', 'akademik', 'malaysia', 'gerak', 'te', 'ada', 'beberapa', 'siri', 'bincang']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.43447263509818596"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "concat = list(itertools.chain(*texts))\n",
    "vocabulary_size = len(list(set(concat)))\n",
    "data, count, dictionary, rev_dictionary = build_dataset(concat, vocabulary_size)\n",
    "print('vocab from size: %d'%(vocabulary_size))\n",
    "print('Most common words', count[4:10])\n",
    "print('Sample data', data[:10], [rev_dictionary[i] for i in data[:10]])\n",
    "len(dictionary) / vocabulary_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "405197"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dictionary.json', 'w') as fopen:\n",
    "    fopen.write(json.dumps({'dictionary':dictionary,'reverse_dictionary':rev_dictionary}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('texts.json', 'w') as fopen:\n",
    "    json.dump(texts, fopen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('texts.txt', 'w') as fopen:\n",
    "    fopen.write('\\n'.join(texts))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
