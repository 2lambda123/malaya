{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1,2,3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensor2tensor.data_generators import problem\n",
    "from tensor2tensor.data_generators import text_problems\n",
    "from tensor2tensor.data_generators import translate\n",
    "from tensor2tensor.utils import registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32512"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system('!rm -rf t2t-tiny')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATASETS = [\n",
    "    [\n",
    "        \"https://f000.backblazeb2.com/file/malay-dataset/paraphrase/train-paraphrase.tar.gz\", \n",
    "        (\"train/before.txt\",\n",
    "         \"train/after.txt\")\n",
    "    ]\n",
    "]\n",
    "\n",
    "TEST_DATASETS = [\n",
    "    [\n",
    "        \"https://f000.backblazeb2.com/file/malay-dataset/paraphrase/test-paraphrase.tar.gz\", \n",
    "        (\"test/before.txt\",\n",
    "         \"test/after.txt\")\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@registry.register_problem\n",
    "class Paraphrase32k(translate.TranslateProblem):\n",
    "    \"\"\"En-de translation trained on WMT corpus.\"\"\"\n",
    "\n",
    "    @property\n",
    "    def additional_training_datasets(self):\n",
    "        \"\"\"Allow subclasses to add training datasets.\"\"\"\n",
    "        return []\n",
    "\n",
    "    def source_data_files(self, dataset_split):\n",
    "        train = dataset_split == problem.DatasetSplit.TRAIN\n",
    "        train_datasets = TRAIN_DATASETS + self.additional_training_datasets\n",
    "        return train_datasets if train else TEST_DATASETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from tensor2tensor.utils import registry\n",
    "from tensor2tensor import problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "DATA_DIR = os.path.expanduser(\"t2t-tiny/data\")\n",
    "TMP_DIR = os.path.expanduser(\"t2t-tiny/tmp\")\n",
    "TRAIN_DIR = os.path.expanduser(\"t2t-tiny/train\")\n",
    "EXPORT_DIR = os.path.expanduser(\"t2t-tiny/export\")\n",
    "TRANSLATIONS_DIR = os.path.expanduser(\"t2t-tiny/translation\")\n",
    "EVENT_DIR = os.path.expanduser(\"t2t-tiny/event\")\n",
    "USR_DIR = os.path.expanduser(\"t2t-tiny/user\")\n",
    " \n",
    "tf.gfile.MakeDirs(DATA_DIR)\n",
    "tf.gfile.MakeDirs(TMP_DIR)\n",
    "tf.gfile.MakeDirs(TRAIN_DIR)\n",
    "tf.gfile.MakeDirs(EXPORT_DIR)\n",
    "tf.gfile.MakeDirs(TRANSLATIONS_DIR)\n",
    "tf.gfile.MakeDirs(EVENT_DIR)\n",
    "tf.gfile.MakeDirs(USR_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/husein/.local/lib/python3.6/site-packages/tensor2tensor/data_generators/translate.py:169: The name tf.gfile.Exists is deprecated. Please use tf.io.gfile.exists instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/husein/.local/lib/python3.6/site-packages/tensor2tensor/data_generators/translate.py:169: The name tf.gfile.Exists is deprecated. Please use tf.io.gfile.exists instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/husein/.local/lib/python3.6/site-packages/tensor2tensor/data_generators/translate.py:170: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/husein/.local/lib/python3.6/site-packages/tensor2tensor/data_generators/translate.py:170: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Skipping compile data, found files:\n",
      "t2t-tiny/tmp/paraphrase32k-compiled-train.lang1\n",
      "t2t-tiny/tmp/paraphrase32k-compiled-train.lang2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Skipping compile data, found files:\n",
      "t2t-tiny/tmp/paraphrase32k-compiled-train.lang1\n",
      "t2t-tiny/tmp/paraphrase32k-compiled-train.lang2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Found vocab file: t2t-tiny/data/vocab.paraphrase32k.32768.subwords\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Found vocab file: t2t-tiny/data/vocab.paraphrase32k.32768.subwords\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Skipping generator because outputs files exists at ['t2t-tiny/data/paraphrase32k-unshuffled-train-00000-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00001-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00002-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00003-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00004-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00005-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00006-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00007-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00008-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00009-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00010-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00011-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00012-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00013-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00014-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00015-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00016-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00017-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00018-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00019-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00020-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00021-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00022-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00023-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00024-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00025-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00026-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00027-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00028-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00029-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00030-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00031-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00032-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00033-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00034-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00035-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00036-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00037-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00038-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00039-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00040-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00041-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00042-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00043-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00044-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00045-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00046-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00047-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00048-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00049-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00050-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00051-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00052-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00053-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00054-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00055-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00056-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00057-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00058-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00059-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00060-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00061-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00062-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00063-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00064-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00065-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00066-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00067-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00068-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00069-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00070-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00071-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00072-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00073-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00074-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00075-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00076-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00077-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00078-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00079-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00080-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00081-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00082-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00083-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00084-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00085-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00086-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00087-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00088-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00089-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00090-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00091-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00092-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00093-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00094-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00095-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00096-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00097-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00098-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00099-of-00100']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Skipping generator because outputs files exists at ['t2t-tiny/data/paraphrase32k-unshuffled-train-00000-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00001-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00002-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00003-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00004-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00005-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00006-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00007-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00008-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00009-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00010-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00011-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00012-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00013-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00014-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00015-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00016-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00017-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00018-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00019-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00020-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00021-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00022-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00023-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00024-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00025-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00026-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00027-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00028-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00029-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00030-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00031-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00032-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00033-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00034-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00035-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00036-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00037-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00038-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00039-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00040-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00041-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00042-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00043-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00044-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00045-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00046-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00047-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00048-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00049-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00050-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00051-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00052-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00053-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00054-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00055-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00056-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00057-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00058-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00059-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00060-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00061-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00062-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00063-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00064-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00065-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00066-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00067-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00068-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00069-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00070-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00071-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00072-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00073-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00074-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00075-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00076-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00077-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00078-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00079-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00080-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00081-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00082-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00083-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00084-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00085-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00086-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00087-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00088-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00089-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00090-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00091-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00092-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00093-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00094-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00095-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00096-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00097-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00098-of-00100', 't2t-tiny/data/paraphrase32k-unshuffled-train-00099-of-00100']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Skipping compile data, found files:\n",
      "t2t-tiny/tmp/paraphrase32k-compiled-dev.lang1\n",
      "t2t-tiny/tmp/paraphrase32k-compiled-dev.lang2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Skipping compile data, found files:\n",
      "t2t-tiny/tmp/paraphrase32k-compiled-dev.lang1\n",
      "t2t-tiny/tmp/paraphrase32k-compiled-dev.lang2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Found vocab file: t2t-tiny/data/vocab.paraphrase32k.32768.subwords\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Found vocab file: t2t-tiny/data/vocab.paraphrase32k.32768.subwords\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Skipping generator because outputs files exists at ['t2t-tiny/data/paraphrase32k-unshuffled-dev-00000-of-00001']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Skipping generator because outputs files exists at ['t2t-tiny/data/paraphrase32k-unshuffled-dev-00000-of-00001']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Skipping shuffle because output files exist\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Skipping shuffle because output files exist\n"
     ]
    }
   ],
   "source": [
    "PROBLEM = 'paraphrase32k'\n",
    "t2t_problem = problems.problem(PROBLEM)\n",
    "t2t_problem.generate_data(DATA_DIR, TMP_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/husein/.local/lib/python3.6/site-packages/tensor2tensor/utils/optimize.py:187: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/husein/.local/lib/python3.6/site-packages/tensor2tensor/utils/optimize.py:187: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/husein/.local/lib/python3.6/site-packages/tensor2tensor/utils/trainer_lib.py:111: The name tf.OptimizerOptions is deprecated. Please use tf.compat.v1.OptimizerOptions instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/husein/.local/lib/python3.6/site-packages/tensor2tensor/utils/trainer_lib.py:111: The name tf.OptimizerOptions is deprecated. Please use tf.compat.v1.OptimizerOptions instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/husein/.local/lib/python3.6/site-packages/tensor2tensor/models/research/neural_stack.py:52: The name tf.nn.rnn_cell.RNNCell is deprecated. Please use tf.compat.v1.nn.rnn_cell.RNNCell instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/husein/.local/lib/python3.6/site-packages/tensor2tensor/models/research/neural_stack.py:52: The name tf.nn.rnn_cell.RNNCell is deprecated. Please use tf.compat.v1.nn.rnn_cell.RNNCell instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/husein/.local/lib/python3.6/site-packages/tensorflow_gan/python/estimator/tpu_gan_estimator.py:42: The name tf.estimator.tpu.TPUEstimator is deprecated. Please use tf.compat.v1.estimator.tpu.TPUEstimator instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/husein/.local/lib/python3.6/site-packages/tensorflow_gan/python/estimator/tpu_gan_estimator.py:42: The name tf.estimator.tpu.TPUEstimator is deprecated. Please use tf.compat.v1.estimator.tpu.TPUEstimator instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensor2tensor.utils.trainer_lib import create_run_config, create_experiment\n",
    "from tensor2tensor.utils.trainer_lib import create_hparams\n",
    "from tensor2tensor.utils import registry\n",
    "from tensor2tensor import models\n",
    "from tensor2tensor import problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_steps = 100000\n",
    "eval_steps = 10\n",
    "batch_size = 1536\n",
    "save_checkpoints_steps = 1000\n",
    "ALPHA = 0.1\n",
    "schedule = \"continuous_train_and_eval\"\n",
    "MODEL = \"transformer\"\n",
    "HPARAMS = \"transformer_tiny\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from tensor2tensor.utils.trainer_lib import create_run_config, create_experiment\n",
    "from tensor2tensor.utils.trainer_lib import create_hparams\n",
    "from tensor2tensor.utils import registry\n",
    "from tensor2tensor import models\n",
    "from tensor2tensor import problems\n",
    "\n",
    "hparams = create_hparams(HPARAMS)\n",
    "hparams.batch_size = batch_size\n",
    "hparams.learning_rate = ALPHA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_CONFIG = create_run_config(\n",
    "      model_dir=TRAIN_DIR,\n",
    "      model_name=MODEL,\n",
    "      save_checkpoints_steps= save_checkpoints_steps,\n",
    "      num_gpus=3\n",
    ")\n",
    "\n",
    "tensorflow_exp_fn = create_experiment(\n",
    "        run_config=RUN_CONFIG,\n",
    "        hparams=hparams,\n",
    "        model_name=MODEL,\n",
    "        problem_name=PROBLEM,\n",
    "        data_dir=DATA_DIR, \n",
    "        train_steps=train_steps, \n",
    "        eval_steps=eval_steps, \n",
    "        #use_xla=True # For acceleration\n",
    "    ) \n",
    "\n",
    "tensorflow_exp_fn.train_and_evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensor2tensor import models\n",
    "from tensor2tensor import problems\n",
    "from tensor2tensor.layers import common_layers\n",
    "from tensor2tensor.utils import trainer_lib\n",
    "from tensor2tensor.utils import t2t_model\n",
    "from tensor2tensor.utils import registry\n",
    "from tensor2tensor.utils import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROBLEM = 'paraphrase32k'\n",
    "problem = problems.problem(PROBLEM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'t2t-tiny/data/vocab.paraphrase32k.32768.subwords'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_name = \"vocab.paraphrase32k.32768.subwords\"\n",
    "vocab_file = os.path.join(DATA_DIR, vocab_name)\n",
    "vocab_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t2t-tiny/train/model.ckpt-100000\n"
     ]
    }
   ],
   "source": [
    "encoders = problem.feature_encoders(DATA_DIR)\n",
    "\n",
    "ckpt_path = tf.train.latest_checkpoint(os.path.join(TRAIN_DIR))\n",
    "print(ckpt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def encode(input_str, output_str=None):\n",
    "  \"\"\"Input str to features dict, ready for inference\"\"\"\n",
    "  inputs = encoders[\"inputs\"].encode(input_str) + [1]  # add EOS id\n",
    "  batch_inputs = tf.reshape(inputs, [1, -1, 1])  # Make it 3D.\n",
    "  return {\"inputs\": batch_inputs}\n",
    "\n",
    "def decode(integers):\n",
    "  \"\"\"List of ints to str\"\"\"\n",
    "  integers = list(np.squeeze(integers))\n",
    "  if 1 in integers:\n",
    "    integers = integers[:integers.index(1)]\n",
    "  return encoders[\"inputs\"].decode(np.squeeze(integers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Setting T2TModel mode to 'infer'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Setting T2TModel mode to 'infer'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Setting hparams.dropout to 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Setting hparams.dropout to 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Setting hparams.label_smoothing to 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Setting hparams.label_smoothing to 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Setting hparams.layer_prepostprocess_dropout to 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Setting hparams.layer_prepostprocess_dropout to 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Setting hparams.symbol_dropout to 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Setting hparams.symbol_dropout to 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Setting hparams.attention_dropout to 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Setting hparams.attention_dropout to 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Setting hparams.relu_dropout to 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Setting hparams.relu_dropout to 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Greedy Decoding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Greedy Decoding\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/husein/.local/lib/python3.6/site-packages/tensor2tensor/models/transformer.py:1226: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/husein/.local/lib/python3.6/site-packages/tensor2tensor/models/transformer.py:1226: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "Modes = tf.estimator.ModeKeys\n",
    "string = \"Beliau yang juga saksi pendakwaan kesembilan berkata, ia bagi mengelak daripada wujud isu digunakan terhadap Najib.\"\n",
    "encoded_inputs = encode(string)\n",
    "hparams = trainer_lib.create_hparams(HPARAMS, data_dir=DATA_DIR, problem_name=PROBLEM)\n",
    "translate_model = registry.model(MODEL)(hparams, Modes.PREDICT)\n",
    "model_output = translate_model.infer(encoded_inputs)[\"outputs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from t2t-tiny/train/model.ckpt-100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from t2t-tiny/train/model.ckpt-100000\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "var_lists = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n",
    "saver = tf.train.Saver(var_list = var_lists)\n",
    "saver.restore(sess, ckpt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dia juga seorang saksi pendakwaan kesembilan berkata, ia bagi mengelak daripada wujud isu digunakan terhadap Najib.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode(sess.run(model_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
