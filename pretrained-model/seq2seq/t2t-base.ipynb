{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1,2,3'\n",
    "\n",
    "from tensor2tensor.data_generators import problem\n",
    "from tensor2tensor.data_generators import text_problems\n",
    "from tensor2tensor.data_generators import translate\n",
    "from tensor2tensor.utils import registry\n",
    "from tensor2tensor import problems\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger()\n",
    "tf.logging.set_verbosity(tf.logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "vocab = 'sp10m.cased.t5.model'\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.Load(vocab)\n",
    "\n",
    "class Encoder:\n",
    "    def __init__(self, sp):\n",
    "        self.sp = sp\n",
    "        self.vocab_size = sp.GetPieceSize()\n",
    "    \n",
    "    def encode(self, s):\n",
    "        return self.sp.EncodeAsIds(s)\n",
    "    \n",
    "    def decode(self, ids, strip_extraneous=False):\n",
    "        return self.sp.DecodeIds(list(ids))\n",
    "    \n",
    "encoder = Encoder(sp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "\n",
    "@registry.register_problem\n",
    "class Seq2Seq(text_problems.Text2TextProblem):\n",
    "\n",
    "    @property\n",
    "    def approx_vocab_size(self):\n",
    "        return 32000\n",
    "    \n",
    "    @property\n",
    "    def is_generate_per_split(self):\n",
    "        return False\n",
    "    \n",
    "    @property\n",
    "    def dataset_splits(self):\n",
    "        return [{\n",
    "            \"split\": problem.DatasetSplit.TRAIN,\n",
    "            \"shards\": 9,\n",
    "        }, {\n",
    "            \"split\": problem.DatasetSplit.EVAL,\n",
    "            \"shards\": 1,\n",
    "        }]\n",
    "            \n",
    "    def feature_encoders(self, data_dir):\n",
    "        encoder = Encoder(sp)\n",
    "        return {\n",
    "            \"inputs\": encoder,\n",
    "            \"targets\": encoder\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system('mkdir t2t2/train-base')\n",
    "DATA_DIR = os.path.expanduser('t2t2/data')\n",
    "TMP_DIR = os.path.expanduser('t2t2/tmp')\n",
    "TRAIN_DIR = os.path.expanduser('t2t2/train-base')\n",
    "EXPORT_DIR = os.path.expanduser('t2t2/export')\n",
    "TRANSLATIONS_DIR = os.path.expanduser('t2t2/translation')\n",
    "EVENT_DIR = os.path.expanduser('t2t2/event')\n",
    "USR_DIR = os.path.expanduser('t2t2/user')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROBLEM = 'seq2_seq'\n",
    "t2t_problem = problems.problem(PROBLEM)\n",
    "\n",
    "train_steps = 500000\n",
    "eval_steps = 10\n",
    "batch_size = 1024 * 20\n",
    "save_checkpoints_steps = 50000\n",
    "ALPHA = 0.1\n",
    "schedule = 'continuous_train_and_eval'\n",
    "MODEL = 'transformer'\n",
    "HPARAMS = 'transformer_base'\n",
    "optimizer = 'adafactor'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensor2tensor.utils.trainer_lib import create_run_config, create_experiment\n",
    "from tensor2tensor.utils.trainer_lib import create_hparams\n",
    "from tensor2tensor.utils import registry\n",
    "from tensor2tensor import models\n",
    "from tensor2tensor import problems\n",
    "\n",
    "hparams = create_hparams(HPARAMS)\n",
    "hparams.batch_size = batch_size\n",
    "hparams.learning_rate = ALPHA\n",
    "hparams.optimizer = optimizer\n",
    "hparams.max_length = 768\n",
    "hparams.filter_size = 3072\n",
    "hparams.hidden_size = 768\n",
    "hparams.num_hidden_layers = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_CONFIG = create_run_config(\n",
    "    model_dir = TRAIN_DIR,\n",
    "    model_name = MODEL,\n",
    "    save_checkpoints_steps = save_checkpoints_steps,\n",
    "    num_gpus = 3,\n",
    ")\n",
    "\n",
    "tensorflow_exp_fn = create_experiment(\n",
    "    run_config = RUN_CONFIG,\n",
    "    hparams = hparams,\n",
    "    model_name = MODEL,\n",
    "    problem_name = PROBLEM,\n",
    "    data_dir = DATA_DIR,\n",
    "    train_steps = train_steps,\n",
    "    eval_steps = eval_steps,\n",
    "    # use_xla=True # For acceleration\n",
    ")\n",
    "\n",
    "tensorflow_exp_fn.train_and_evaluate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
