{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import malaya\n",
    "tf.compat.v1.set_random_seed(1234)\n",
    "import numpy as np\n",
    "import logging\n",
    "logging.basicConfig(level = logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_shape_list(tensor, expected_rank = None, name = None):\n",
    "    \"\"\"Returns a list of the shape of tensor, preferring static dimensions.\n",
    "  Args:\n",
    "    tensor: A tf.Tensor object to find the shape of.\n",
    "    expected_rank: (optional) int. The expected rank of `tensor`. If this is\n",
    "      specified and the `tensor` has a different rank, and exception will be\n",
    "      thrown.\n",
    "    name: Optional name of the tensor for the error message.\n",
    "  Returns:\n",
    "    A list of dimensions of the shape of tensor. All static dimensions will\n",
    "    be returned as python integers, and dynamic dimensions will be returned\n",
    "    as tf.Tensor scalars.\n",
    "  \"\"\"\n",
    "    if name is None:\n",
    "        name = tensor.name\n",
    "\n",
    "    shape = tensor.shape.as_list()\n",
    "\n",
    "    non_static_indexes = []\n",
    "    for (index, dim) in enumerate(shape):\n",
    "        if dim is None:\n",
    "            non_static_indexes.append(index)\n",
    "\n",
    "    if not non_static_indexes:\n",
    "        return shape\n",
    "\n",
    "    dyn_shape = tf.shape(tensor)\n",
    "    for index in non_static_indexes:\n",
    "        shape[index] = dyn_shape[index]\n",
    "    return shape\n",
    "\n",
    "\n",
    "def create_initializer(initializer_range = 0.02):\n",
    "    \"\"\"Creates a `truncated_normal_initializer` with the given range.\"\"\"\n",
    "    return tf.truncated_normal_initializer(stddev = initializer_range)\n",
    "\n",
    "\n",
    "def layer_norm(input_tensor, name = None):\n",
    "    return tf.contrib.layers.layer_norm(\n",
    "        inputs = input_tensor,\n",
    "        begin_norm_axis = -1,\n",
    "        begin_params_axis = -1,\n",
    "        scope = name,\n",
    "    )\n",
    "\n",
    "\n",
    "def embedding_lookup(\n",
    "    input_ids,\n",
    "    vocab_size,\n",
    "    embedding_size = 128,\n",
    "    initializer_range = 0.02,\n",
    "    word_embedding_name = 'word_embeddings',\n",
    "    use_one_hot_embeddings = False,\n",
    "):\n",
    "    \"\"\"Looks up words embeddings for id tensor.\n",
    "  Args:\n",
    "    input_ids: int32 Tensor of shape [batch_size, seq_length] containing word\n",
    "      ids.\n",
    "    vocab_size: int. Size of the embedding vocabulary.\n",
    "    embedding_size: int. Width of the word embeddings.\n",
    "    initializer_range: float. Embedding initialization range.\n",
    "    word_embedding_name: string. Name of the embedding table.\n",
    "    use_one_hot_embeddings: bool. If True, use one-hot method for word\n",
    "      embeddings. If False, use `tf.gather()`.\n",
    "  Returns:\n",
    "    float Tensor of shape [batch_size, seq_length, embedding_size].\n",
    "  \"\"\"\n",
    "    # This function assumes that the input is of shape [batch_size, seq_length,\n",
    "    # num_inputs].\n",
    "    #\n",
    "    # If the input is a 2D tensor of shape [batch_size, seq_length], we\n",
    "    # reshape to [batch_size, seq_length, 1].\n",
    "    if input_ids.shape.ndims == 2:\n",
    "        input_ids = tf.expand_dims(input_ids, axis = [-1])\n",
    "\n",
    "    embedding_table = tf.get_variable(\n",
    "        name = word_embedding_name,\n",
    "        shape = [vocab_size, embedding_size],\n",
    "        initializer = create_initializer(initializer_range),\n",
    "    )\n",
    "\n",
    "    flat_input_ids = tf.reshape(input_ids, [-1])\n",
    "    if use_one_hot_embeddings:\n",
    "        one_hot_input_ids = tf.one_hot(flat_input_ids, depth = vocab_size)\n",
    "        output = tf.matmul(one_hot_input_ids, embedding_table)\n",
    "    else:\n",
    "        output = tf.gather(embedding_table, flat_input_ids)\n",
    "\n",
    "    input_shape = get_shape_list(input_ids)\n",
    "\n",
    "    output = tf.reshape(\n",
    "        output, input_shape[0:-1] + [input_shape[-1] * embedding_size]\n",
    "    )\n",
    "    return (output, embedding_table)\n",
    "\n",
    "\n",
    "def embedding_postprocessor(\n",
    "    input_tensor,\n",
    "    use_token_type = False,\n",
    "    token_type_ids = None,\n",
    "    token_type_vocab_size = 2,\n",
    "    token_type_embedding_name = 'token_type_embeddings',\n",
    "    use_position_embeddings = True,\n",
    "    position_embedding_name = 'position_embeddings',\n",
    "    initializer_range = 0.02,\n",
    "    max_position_embeddings = 512,\n",
    "):\n",
    "    \"\"\"Performs various post-processing on a word embedding tensor.\n",
    "  Args:\n",
    "    input_tensor: float Tensor of shape [batch_size, seq_length,\n",
    "      embedding_size].\n",
    "    use_token_type: bool. Whether to add embeddings for `token_type_ids`.\n",
    "    token_type_ids: (optional) int32 Tensor of shape [batch_size, seq_length].\n",
    "      Must be specified if `use_token_type` is True.\n",
    "    token_type_vocab_size: int. The vocabulary size of `token_type_ids`.\n",
    "    token_type_embedding_name: string. The name of the embedding table variable\n",
    "      for token type ids.\n",
    "    use_position_embeddings: bool. Whether to add position embeddings for the\n",
    "      position of each token in the sequence.\n",
    "    position_embedding_name: string. The name of the embedding table variable\n",
    "      for positional embeddings.\n",
    "    initializer_range: float. Range of the weight initialization.\n",
    "    max_position_embeddings: int. Maximum sequence length that might ever be\n",
    "      used with this model. This can be longer than the sequence length of\n",
    "      input_tensor, but cannot be shorter.\n",
    "    dropout_prob: float. Dropout probability applied to the final output tensor.\n",
    "  Returns:\n",
    "    float tensor with same shape as `input_tensor`.\n",
    "  Raises:\n",
    "    ValueError: One of the tensor shapes or input values is invalid.\n",
    "  \"\"\"\n",
    "    input_shape = get_shape_list(input_tensor, expected_rank = 3)\n",
    "    batch_size = input_shape[0]\n",
    "    seq_length = input_shape[1]\n",
    "    width = input_shape[2]\n",
    "\n",
    "    output = input_tensor\n",
    "\n",
    "    if use_token_type:\n",
    "        if token_type_ids is None:\n",
    "            raise ValueError(\n",
    "                '`token_type_ids` must be specified if'\n",
    "                '`use_token_type` is True.'\n",
    "            )\n",
    "        token_type_table = tf.get_variable(\n",
    "            name = token_type_embedding_name,\n",
    "            shape = [token_type_vocab_size, width],\n",
    "            initializer = create_initializer(initializer_range),\n",
    "        )\n",
    "        flat_token_type_ids = tf.reshape(token_type_ids, [-1])\n",
    "        one_hot_ids = tf.one_hot(\n",
    "            flat_token_type_ids, depth = token_type_vocab_size\n",
    "        )\n",
    "        token_type_embeddings = tf.matmul(one_hot_ids, token_type_table)\n",
    "        token_type_embeddings = tf.reshape(\n",
    "            token_type_embeddings, [batch_size, seq_length, width]\n",
    "        )\n",
    "        output += token_type_embeddings\n",
    "\n",
    "    if use_position_embeddings:\n",
    "        assert_op = tf.assert_less_equal(seq_length, max_position_embeddings)\n",
    "        with tf.control_dependencies([assert_op]):\n",
    "            full_position_embeddings = tf.get_variable(\n",
    "                name = position_embedding_name,\n",
    "                shape = [max_position_embeddings, width],\n",
    "                initializer = create_initializer(initializer_range),\n",
    "            )\n",
    "            position_embeddings = tf.slice(\n",
    "                full_position_embeddings, [0, 0], [seq_length, -1]\n",
    "            )\n",
    "            num_dims = len(output.shape.as_list())\n",
    "            position_broadcast_shape = []\n",
    "            for _ in range(num_dims - 2):\n",
    "                position_broadcast_shape.append(1)\n",
    "            position_broadcast_shape.extend([seq_length, width])\n",
    "            position_embeddings = tf.reshape(\n",
    "                position_embeddings, position_broadcast_shape\n",
    "            )\n",
    "            output += position_embeddings\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "def gelu(x):\n",
    "    cdf = 0.5 * (\n",
    "        1.0 + tf.tanh((np.sqrt(2 / np.pi) * (x + 0.044_715 * tf.pow(x, 3))))\n",
    "    )\n",
    "    return x * cdf\n",
    "\n",
    "\n",
    "class Forward(tf.keras.layers.Layer):\n",
    "    def __init__(self, dim, mlp_dim, dropout, **kwargs):\n",
    "        super(Forward, self).__init__(**kwargs)\n",
    "        self.rate = dropout\n",
    "        self.dense1 = tf.keras.layers.Dense(mlp_dim, activation = gelu)\n",
    "        self.dense2 = tf.keras.layers.Dense(dim)\n",
    "        self.dropout = tf.keras.layers.Dropout(self.rate)\n",
    "\n",
    "    def call(self, inputs, training = True):\n",
    "        X = self.dense1(inputs)\n",
    "        X = self.dropout(X, training = training)\n",
    "        X = self.dense2(X)\n",
    "        X = self.dropout(X, training = training)\n",
    "        return X\n",
    "\n",
    "\n",
    "class FNetBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, dim, mlp_dim, dropout = 0.1, **kwargs):\n",
    "        super(FNetBlock, self).__init__(name = 'FNetBlock', **kwargs)\n",
    "        self.norm_fourier = tf.keras.layers.LayerNormalization()\n",
    "        self.norm_ffn = tf.keras.layers.LayerNormalization()\n",
    "        self.ffn = Forward(dim, mlp_dim, dropout = dropout)\n",
    "\n",
    "    def call(self, inputs, training = True):\n",
    "        X_complex = tf.cast(inputs, tf.complex64)\n",
    "        X_fft = tf.math.real(tf.signal.fft2d(X_complex))\n",
    "        X_norm1 = self.norm_fourier(X_fft + inputs, training = training)\n",
    "        X_dense = self.ffn(X_norm1, training = training)\n",
    "        X_norm2 = self.norm_ffn(X_dense + X_norm1, training = training)\n",
    "        return X_norm2\n",
    "\n",
    "\n",
    "class Model(tf.keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        vocab_size,\n",
    "        depth,\n",
    "        mlp_dim,\n",
    "        dropout = 0.1,\n",
    "        dropout_embedding = 0.1,\n",
    "        max_position_embeddings = 1024,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super(Model, self).__init__(name = 'Model', **kwargs)\n",
    "        self.dim = dim\n",
    "        self.hidden_size = dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.dropout_embedding = dropout_embedding\n",
    "        self.max_position_embeddings = max_position_embeddings\n",
    "        self.attn = []\n",
    "        for _ in range(depth):\n",
    "            self.attn.append(\n",
    "                FNetBlock(dim = dim, mlp_dim = mlp_dim, dropout = dropout)\n",
    "            )\n",
    "        self.layernorm_dropout = tf.keras.Sequential()\n",
    "        self.layernorm_dropout.add(tf.keras.layers.LayerNormalization())\n",
    "        self.layernorm_dropout.add(tf.keras.layers.Dropout(dropout_embedding))\n",
    "\n",
    "    def call(\n",
    "        self, x, input_mask = None, token_type_ids = None, training = True\n",
    "    ):\n",
    "\n",
    "        if input_mask is None:\n",
    "            input_mask = tf.ones(\n",
    "                shape = [tf.shape(x)[0], tf.shape(x)[1]], dtype = tf.int32\n",
    "            )\n",
    "\n",
    "        input_mask = tf.expand_dims(tf.cast(input_mask, tf.float32), -1)\n",
    "\n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = tf.zeros(\n",
    "                shape = [tf.shape(x)[0], tf.shape(x)[1]], dtype = tf.int32\n",
    "            )\n",
    "        (self.embedding_output, self.embedding_table) = embedding_lookup(\n",
    "            input_ids = x,\n",
    "            vocab_size = self.vocab_size,\n",
    "            embedding_size = self.dim,\n",
    "            initializer_range = 0.02,\n",
    "            word_embedding_name = 'word_embeddings',\n",
    "            use_one_hot_embeddings = False,\n",
    "        )\n",
    "        self.embedding_output = embedding_postprocessor(\n",
    "            input_tensor = self.embedding_output,\n",
    "            use_token_type = True,\n",
    "            token_type_ids = token_type_ids,\n",
    "            token_type_vocab_size = 2,\n",
    "            token_type_embedding_name = 'token_type_embeddings',\n",
    "            use_position_embeddings = True,\n",
    "            position_embedding_name = 'position_embeddings',\n",
    "            initializer_range = 0.02,\n",
    "            max_position_embeddings = self.max_position_embeddings,\n",
    "        )\n",
    "        x = self.layernorm_dropout(self.embedding_output, training = training)\n",
    "        for no, attn in enumerate(self.attn):\n",
    "            x = attn(x, training = training)\n",
    "            x = x * input_mask\n",
    "\n",
    "        with tf.variable_scope('pooler'):\n",
    "            first_token_tensor = tf.squeeze(x[:, 0:1, :], axis = 1)\n",
    "            self.pooled_output = tf.layers.dense(\n",
    "                first_token_tensor,\n",
    "                self.hidden_size,\n",
    "                activation = tf.tanh,\n",
    "                kernel_initializer = create_initializer(0.02),\n",
    "            )\n",
    "        return x\n",
    "\n",
    "\n",
    "# x = tf.placeholder(tf.int32, (None, None))\n",
    "# model = Model(768, 32000, 12, 768)\n",
    "# o = model(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.int32, (None, None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(768, 32000, 12, 768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/huseinzolkepli/Documents/tf-1.15/env/lib/python3.7/site-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/huseinzolkepli/Documents/tf-1.15/env/lib/python3.7/site-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/huseinzolkepli/Documents/tf-1.15/env/lib/python3.7/site-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/huseinzolkepli/Documents/tf-1.15/env/lib/python3.7/site-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/huseinzolkepli/Documents/tf-1.15/env/lib/python3.7/site-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.assert_less_equal is deprecated. Please use tf.compat.v1.assert_less_equal instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/huseinzolkepli/Documents/tf-1.15/env/lib/python3.7/site-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.assert_less_equal is deprecated. Please use tf.compat.v1.assert_less_equal instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/huseinzolkepli/Documents/tf-1.15/env/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/huseinzolkepli/Documents/tf-1.15/env/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-3-2cce56a796df>:295: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Dense instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-3-2cce56a796df>:295: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Dense instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/huseinzolkepli/Documents/tf-1.15/env/lib/python3.7/site-packages/tensorflow_core/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/huseinzolkepli/Documents/tf-1.15/env/lib/python3.7/site-packages/tensorflow_core/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Model/mul_11:0' shape=(?, ?, 768) dtype=float32>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o = model(x)\n",
    "o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 433 ms, sys: 23.6 ms, total: 456 ms\n",
      "Wall time: 492 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[-0.14676172,  0.12887064, -0.6372858 , ...,  0.37827602,\n",
       "          0.5740778 , -0.84948176],\n",
       "        [ 0.4179115 , -0.3492962 ,  0.99186915, ..., -0.82482225,\n",
       "          0.7381474 , -0.86820143],\n",
       "        [ 0.7566453 , -0.1262211 ,  0.04796262, ...,  1.421154  ,\n",
       "          0.7783395 , -0.01940406],\n",
       "        [ 0.12364741,  0.39592683,  2.253871  , ..., -0.11123513,\n",
       "          0.922476  , -0.94825745]]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "sess.run(o, feed_dict = {x: [[1,2,3,4]]})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
