{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import collections\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from unidecode import unidecode\n",
    "from tqdm import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules_normalizer = {\n",
    "    'experience': 'pengalaman',\n",
    "    'bagasi': 'bagasi',\n",
    "    'kg': 'kampung',\n",
    "    'kilo': 'kilogram',\n",
    "    'g': 'gram',\n",
    "    'grm': 'gram',\n",
    "    'k': 'okay',\n",
    "    'abgkat': 'abang dekat',\n",
    "    'abis': 'habis',\n",
    "    'ade': 'ada',\n",
    "    'adoi': 'aduh',\n",
    "    'adoii': 'aduhh',\n",
    "    'aerodarat': 'kapal darat',\n",
    "    'agkt': 'angkat',\n",
    "    'ahh': 'ah',\n",
    "    'ailior': 'air liur',\n",
    "    'airasia': 'air asia x',\n",
    "    'airasiax': 'penerbangan',\n",
    "    'airline': 'penerbangan',\n",
    "    'airlines': 'penerbangan',\n",
    "    'airport': 'lapangan terbang',\n",
    "    'airpot': 'lapangan terbang',\n",
    "    'aje': 'sahaja',\n",
    "    'ajelah': 'sahajalah',\n",
    "    'ajer': 'sahaja',\n",
    "    'ak': 'aku',\n",
    "    'aq': 'aku',\n",
    "    'all': 'semua',\n",
    "    'ambik': 'ambil',\n",
    "    'amek': 'ambil',\n",
    "    'amer': 'amir',\n",
    "    'amik': 'ambil',\n",
    "    'ana': 'saya',\n",
    "    'angkt': 'angkat',\n",
    "    'anual': 'tahunan',\n",
    "    'apapun': 'apa pun',\n",
    "    'ape': 'apa',\n",
    "    'arab': 'arab',\n",
    "    'area': 'kawasan',\n",
    "    'aritu': 'hari itu',\n",
    "    'ask': 'tanya',\n",
    "    'astro': 'astro',\n",
    "    'at': 'pada',\n",
    "    'attitude': 'sikap',\n",
    "    'babi': 'khinzir',\n",
    "    'back': 'belakang',\n",
    "    'bag': 'beg',\n",
    "    'bang': 'abang',\n",
    "    'bangla': 'bangladesh',\n",
    "    'banyk': 'banyak',\n",
    "    'bard': 'pujangga',\n",
    "    'bargasi': 'bagasi',\n",
    "    'bawak': 'bawa',\n",
    "    'bawanges': 'bawang',\n",
    "    'be': 'jadi',\n",
    "    'behave': 'berkelakuan baik',\n",
    "    'belagak': 'berlagak',\n",
    "    'berdisiplin': 'berdisplin',\n",
    "    'berenti': 'berhenti',\n",
    "    'beskal': 'basikal',\n",
    "    'bff': 'rakan karib',\n",
    "    'bg': 'bagi',\n",
    "    'bgi': 'bagi',\n",
    "    'biase': 'biasa',\n",
    "    'big': 'besar',\n",
    "    'bike': 'basikal',\n",
    "    'bile': 'bila',\n",
    "    'binawe': 'binatang',\n",
    "    'bini': 'isteri',\n",
    "    'bkn': 'bukan',\n",
    "    'bla': 'bila',\n",
    "    'blom': 'belum',\n",
    "    'bnyak': 'banyak',\n",
    "    'body': 'tubuh',\n",
    "    'bole': 'boleh',\n",
    "    'boss': 'bos',\n",
    "    'bowling': 'boling',\n",
    "    'bpe': 'berapa',\n",
    "    'brand': 'jenama',\n",
    "    'brg': 'barang',\n",
    "    'briefing': 'taklimat',\n",
    "    'brng': 'barang',\n",
    "    'bro': 'abang',\n",
    "    'bru': 'baru',\n",
    "    'bruntung': 'beruntung',\n",
    "    'bsikal': 'basikal',\n",
    "    'btnggjwb': 'bertanggungjawab',\n",
    "    'btul': 'betul',\n",
    "    'buatlh': 'buatlah',\n",
    "    'buh': 'letak',\n",
    "    'buka': 'buka',\n",
    "    'but': 'tetapi',\n",
    "    'bwk': 'bawa',\n",
    "    'by': 'dengan',\n",
    "    'byr': 'bayar',\n",
    "    'bz': 'sibuk',\n",
    "    'camera': 'kamera',\n",
    "    'camni': 'macam ini',\n",
    "    'cane': 'macam mana',\n",
    "    'cant': 'tak boleh',\n",
    "    'carakerja': 'cara kerja',\n",
    "    'care': 'jaga',\n",
    "    'cargo': 'kargo',\n",
    "    'cctv': 'kamera litar tertutup',\n",
    "    'celako': 'celaka',\n",
    "    'cer': 'cerita',\n",
    "    'cheap': 'murah',\n",
    "    'check': 'semak',\n",
    "    'ciput': 'sedikit',\n",
    "    'cite': 'cerita',\n",
    "    'citer': 'cerita',\n",
    "    'ckit': 'sikit',\n",
    "    'ckp': 'cakap',\n",
    "    'class': 'kelas',\n",
    "    'cm': 'macam',\n",
    "    'cmni': 'macam ini',\n",
    "    'cmpak': 'campak',\n",
    "    'committed': 'komited',\n",
    "    'company': 'syarikat',\n",
    "    'complain': 'aduan',\n",
    "    'corn': 'jagung',\n",
    "    'couldnt': 'tak boleh',\n",
    "    'cr': 'cari',\n",
    "    'crew': 'krew',\n",
    "    'cube': 'cuba',\n",
    "    'cuma': 'cuma',\n",
    "    'curinyaa': 'curinya',\n",
    "    'cust': 'pelanggan',\n",
    "    'customer': 'pelanggan',\n",
    "    'd': 'di',\n",
    "    'da': 'dah',\n",
    "    'dn': 'dan',\n",
    "    'dahh': 'dah',\n",
    "    'damaged': 'rosak',\n",
    "    'dapek': 'dapat',\n",
    "    'day': 'hari',\n",
    "    'dazrin': 'dazrin',\n",
    "    'dbalingnya': 'dibalingnya',\n",
    "    'de': 'ada',\n",
    "    'deep': 'dalam',\n",
    "    'deliberately': 'sengaja',\n",
    "    'depa': 'mereka',\n",
    "    'dessa': 'desa',\n",
    "    'dgn': 'dengan',\n",
    "    'dh': 'dah',\n",
    "    'didunia': 'di dunia',\n",
    "    'diorang': 'mereka',\n",
    "    'diorng': 'mereka',\n",
    "    'direct': 'secara terus',\n",
    "    'diving': 'junam',\n",
    "    'dkt': 'dekat',\n",
    "    'dlempar': 'dilempar',\n",
    "    'dlm': 'dalam',\n",
    "    'dlt': 'padam',\n",
    "    'dlu': 'dulu',\n",
    "    'done': 'siap',\n",
    "    'dont': 'jangan',\n",
    "    'dorg': 'mereka',\n",
    "    'dpermudhkn': 'dipermudahkan',\n",
    "    'dpt': 'dapat',\n",
    "    'dr': 'dari',\n",
    "    'dri': 'dari',\n",
    "    'dsb': 'dan sebagainya',\n",
    "    'dy': 'dia',\n",
    "    'educate': 'mendidik',\n",
    "    'ensure': 'memastikan',\n",
    "    'everything': 'semua',\n",
    "    'ewahh': 'wah',\n",
    "    'expect': 'sangka',\n",
    "    'fb': 'facebook',\n",
    "    'fired': 'pecat',\n",
    "    'first': 'pertama',\n",
    "    'fkr': 'fikir',\n",
    "    'flight': 'kapal terbang',\n",
    "    'for': 'untuk',\n",
    "    'free': 'percuma',\n",
    "    'friend': 'kawan',\n",
    "    'fyi': 'untuk pengetahuan anda',\n",
    "    'gantila': 'gantilah',\n",
    "    'gantirugi': 'ganti rugi',\n",
    "    'gentlemen': 'lelaki budiman',\n",
    "    'gerenti': 'jaminan',\n",
    "    'gile': 'gila',\n",
    "    'gk': 'juga',\n",
    "    'gnti': 'ganti',\n",
    "    'go': 'pergi',\n",
    "    'gomen': 'kerajaan',\n",
    "    'goment': 'kerajaan',\n",
    "    'good': 'baik',\n",
    "    'ground': 'tanah',\n",
    "    'guarno': 'macam mana',\n",
    "    'hampa': 'mereka',\n",
    "    'hampeh': 'teruk',\n",
    "    'hanat': 'jahanam',\n",
    "    'handle': 'kawal',\n",
    "    'handling': 'kawalan',\n",
    "    'hanta': 'hantar',\n",
    "    'haritu': 'hari itu',\n",
    "    'hate': 'benci',\n",
    "    'have': 'ada',\n",
    "    'hawau': 'celaka',\n",
    "    'henpon': 'telefon',\n",
    "    'heran': 'hairan',\n",
    "    'him': 'dia',\n",
    "    'his': 'dia',\n",
    "    'hmpa': 'mereka',\n",
    "    'hntr': 'hantar',\n",
    "    'hotak': 'otak',\n",
    "    'hr': 'hari',\n",
    "    'i': 'saya',\n",
    "    'hrga': 'harga',\n",
    "    'hrp': 'harap',\n",
    "    'hu': 'sedih',\n",
    "    'humble': 'merendah diri',\n",
    "    'ibon': 'ikon',\n",
    "    'ichi': 'inci',\n",
    "    'idung': 'hidung',\n",
    "    'if': 'jika',\n",
    "    'ig': 'instagram',\n",
    "    'iklas': 'ikhlas',\n",
    "    'improve': 'menambah baik',\n",
    "    'in': 'masuk',\n",
    "    'isn t': 'tidak',\n",
    "    'isyaallah': 'insyallah',\n",
    "    'ja': 'sahaja',\n",
    "    'japan': 'jepun',\n",
    "    'jd': 'jadi',\n",
    "    'je': 'saja',\n",
    "    'jee': 'saja',\n",
    "    'jek': 'saja',\n",
    "    'jepun': 'jepun',\n",
    "    'jer': 'saja',\n",
    "    'jerr': 'saja',\n",
    "    'jez': 'saja',\n",
    "    'jg': 'juga',\n",
    "    'jgk': 'juga',\n",
    "    'jgn': 'jangan',\n",
    "    'jgnla': 'janganlah',\n",
    "    'jibake': 'celaka',\n",
    "    'jjur': 'jujur',\n",
    "    'job': 'kerja',\n",
    "    'jobscope': 'skop kerja',\n",
    "    'jogja': 'jogjakarta',\n",
    "    'jpam': 'jpam',\n",
    "    'jth': 'jatuh',\n",
    "    'jugak': 'juga',\n",
    "    'ka': 'ke',\n",
    "    'kalo': 'kalau',\n",
    "    'kalu': 'kalau',\n",
    "    'kang': 'nanti',\n",
    "    'kantoi': 'temberang',\n",
    "    'kasi': 'beri',\n",
    "    'kat': 'dekat',\n",
    "    'kbye': 'ok bye',\n",
    "    'kearah': 'ke arah',\n",
    "    'kecik': 'kecil',\n",
    "    'keja': 'kerja',\n",
    "    'keje': 'kerja',\n",
    "    'kejo': 'kerja',\n",
    "    'keksongan': 'kekosongan',\n",
    "    'kemana': 'ke mana',\n",
    "    'kene': 'kena',\n",
    "    'kenekan': 'kenakan',\n",
    "    'kesah': 'kisah',\n",
    "    'ketempat': 'ke tempat',\n",
    "    'kije': 'kerja',\n",
    "    'kijo': 'kerja',\n",
    "    'kiss': 'cium',\n",
    "    'kite': 'kita',\n",
    "    'kito': 'kita',\n",
    "    'kje': 'kerja',\n",
    "    'kjr': 'kerja',\n",
    "    'kk': 'okay',\n",
    "    'kmi': 'kami',\n",
    "    'kt': 'kat',\n",
    "    'tlg': 'tolong',\n",
    "    'kl': 'kuala lumpur',\n",
    "    'klai': 'kalau',\n",
    "    'klau': 'kalau',\n",
    "    'klia': 'klia',\n",
    "    'klo': 'kalau',\n",
    "    'klu': 'kalau',\n",
    "    'kn': 'kan',\n",
    "    'knapa': 'kenapa',\n",
    "    'kne': 'kena',\n",
    "    'ko': 'kau',\n",
    "    'kompom': 'sah',\n",
    "    'korang': 'kamu semua',\n",
    "    'korea': 'korea',\n",
    "    'korg': 'kamu semua',\n",
    "    'kot': 'mungkin',\n",
    "    'krja': 'kerja',\n",
    "    'ksalahan': 'kesalahan',\n",
    "    'kta': 'kita',\n",
    "    'kuar': 'keluar',\n",
    "    'kut': 'mungkin',\n",
    "    'la': 'lah',\n",
    "    'laa': 'lah',\n",
    "    'lahabau': 'celaka',\n",
    "    'lahanat': 'celaka',\n",
    "    'lainda': 'lain dah',\n",
    "    'lak': 'pula',\n",
    "    'last': 'akhir',\n",
    "    'le': 'lah',\n",
    "    'leader': 'ketua',\n",
    "    'leave': 'pergi',\n",
    "    'ler': 'lah',\n",
    "    'less': 'kurang',\n",
    "    'letter': 'surat',\n",
    "    'lg': 'lagi',\n",
    "    'lgi': 'lagi',\n",
    "    'lngsong': 'langsung',\n",
    "    'lol': 'hehe',\n",
    "    'lorr': 'lah',\n",
    "    'low': 'rendah',\n",
    "    'lps': 'lepas',\n",
    "    'luggage': 'bagasi',\n",
    "    'lumbe': 'lumba',\n",
    "    'lyak': 'layak',\n",
    "    'maap': 'maaf',\n",
    "    'maapkan': 'maafkan',\n",
    "    'mahai': 'mahal',\n",
    "    'mampos': 'mampus',\n",
    "    'mart': 'kedai',\n",
    "    'mau': 'mahu',\n",
    "    'mcm': 'macam',\n",
    "    'mcmtu': 'macam itu',\n",
    "    'memerlukn': 'memerlukan',\n",
    "    'mengembirakan': 'menggembirakan',\n",
    "    'mengmbilnyer': 'mengambilnya',\n",
    "    'mengtasi': 'mengatasi',\n",
    "    'mg': 'memang',\n",
    "    'mihak': 'memihak',\n",
    "    'min': 'admin',\n",
    "    'mingu': 'minggu',\n",
    "    'mintak': 'minta',\n",
    "    'mjtuhkn': 'menjatuhkan',\n",
    "    'mkyong': 'mak yong',\n",
    "    'mlibatkn': 'melibatkan',\n",
    "    'mmg': 'memang',\n",
    "    'mmnjang': 'memanjang',\n",
    "    'mmpos': 'mampus',\n",
    "    'mn': 'mana',\n",
    "    'mna': 'mana',\n",
    "    'mntak': 'minta',\n",
    "    'mntk': 'minta',\n",
    "    'mnyusun': 'menyusun',\n",
    "    'mood': 'suasana',\n",
    "    'most': 'paling',\n",
    "    'mr': 'tuan',\n",
    "    'msa': 'masa',\n",
    "    'msia': 'malaysia',\n",
    "    'mst': 'mesti',\n",
    "    'mu': 'awak',\n",
    "    'much': 'banyak',\n",
    "    'muko': 'muka',\n",
    "    'mum': 'emak',\n",
    "    'n': 'dan',\n",
    "    'nah': 'nah',\n",
    "    'nanny': 'nenek',\n",
    "    'napo': 'kenapa',\n",
    "    'nati': 'nanti',\n",
    "    'ngan': 'dengan',\n",
    "    'ngn': 'dengan',\n",
    "    'ni': 'ini',\n",
    "    'nie': 'ini',\n",
    "    'nii': 'ini',\n",
    "    'nk': 'nak',\n",
    "    'nmpk': 'nampak',\n",
    "    'nye': 'nya',\n",
    "    'ofis': 'pejabat',\n",
    "    'ohh': 'oh',\n",
    "    'oii': 'hoi',\n",
    "    'one': 'satu',\n",
    "    'online': 'dalam talian',\n",
    "    'or': 'atau',\n",
    "    'org': 'orang',\n",
    "    'orng': 'orang',\n",
    "    'otek': 'otak',\n",
    "    'p': 'pergi',\n",
    "    'paid': 'dah bayar',\n",
    "    'palabana': 'kepala otak',\n",
    "    'pasni': 'lepas ini',\n",
    "    'passengers': 'penumpang',\n",
    "    'passengger': 'penumpang',\n",
    "    'pastu': 'lepas itu',\n",
    "    'pd': 'pada',\n",
    "    'pegi': 'pergi',\n",
    "    'pekerje': 'pekerja',\n",
    "    'pekrja': 'pekerja',\n",
    "    'perabih': 'perabis',\n",
    "    'perkerja': 'pekerja',\n",
    "    'pg': 'pergi',\n",
    "    'phuii': 'puih',\n",
    "    'pikir': 'fikir',\n",
    "    'pilot': 'juruterbang',\n",
    "    'pk': 'fikir',\n",
    "    'pkerja': 'pekerja',\n",
    "    'pkerjaan': 'pekerjaan',\n",
    "    'pki': 'pakai',\n",
    "    'please': 'tolong',\n",
    "    'pls': 'tolong',\n",
    "    'pn': 'pun',\n",
    "    'pnh': 'pernah',\n",
    "    'pnt': 'penat',\n",
    "    'pnya': 'punya',\n",
    "    'pon': 'pun',\n",
    "    'priority': 'keutamaan',\n",
    "    'properties': 'harta benda',\n",
    "    'ptugas': 'petugas',\n",
    "    'pub': 'kelab malam',\n",
    "    'pulak': 'pula',\n",
    "    'puye': 'punya',\n",
    "    'pwrcuma': 'percuma',\n",
    "    'pyahnya': 'payahnya',\n",
    "    'quality': 'kualiti',\n",
    "    'quit': 'keluar',\n",
    "    'ramly': 'ramly',\n",
    "    'rege': 'harga',\n",
    "    'reger': 'harga',\n",
    "    'report': 'laporan',\n",
    "    'resigned': 'meletakkan jawatan',\n",
    "    'respect': 'hormat',\n",
    "    'rizal': 'rizal',\n",
    "    'rosak': 'rosak',\n",
    "    'rosok': 'rosak',\n",
    "    'rse': 'rasa',\n",
    "    'sacked': 'buang',\n",
    "    'sado': 'tegap',\n",
    "    'salute': 'sanjung',\n",
    "    'sam': 'sama',\n",
    "    'same': 'sama',\n",
    "    'samp': 'sampah',\n",
    "    'sbb': 'sebab',\n",
    "    'sbgai': 'sebagai',\n",
    "    'sblm': 'sebelum',\n",
    "    'sblum': 'sebelum',\n",
    "    'sbnarnya': 'sebenarnya',\n",
    "    'sbum': 'sebelum',\n",
    "    'sdg': 'sedang',\n",
    "    'sebb': 'sebab',\n",
    "    'sebijik': 'sebiji',\n",
    "    'see': 'lihat',\n",
    "    'seen': 'dilihat',\n",
    "    'selangor': 'selangor',\n",
    "    'selfie': 'swafoto',\n",
    "    'sempoi': 'cantik',\n",
    "    'senaraihitam': 'senarai hitam',\n",
    "    'seorg': 'seorang',\n",
    "    'service': 'perkhidmatan',\n",
    "    'sgt': 'sangat',\n",
    "    'shared': 'kongsi',\n",
    "    'shirt': 'kemeja',\n",
    "    'shut': 'tutup',\n",
    "    'sib': 'nasib',\n",
    "    'skali': 'sekali',\n",
    "    'sket': 'sikit',\n",
    "    'sma': 'sama',\n",
    "    'smoga': 'semoga',\n",
    "    'smpoi': 'cantik',\n",
    "    'sndiri': 'sendiri',\n",
    "    'sndr': 'sendiri',\n",
    "    'sndri': 'sendiri',\n",
    "    'sne': 'sana',\n",
    "    'so': 'jadi',\n",
    "    'sop': 'tatacara pengendalian piawai',\n",
    "    'sorang': 'seorang',\n",
    "    'spoting': 'pembintikan',\n",
    "    'sronok': 'seronok',\n",
    "    'ssh': 'susah',\n",
    "    'staff': 'staf',\n",
    "    'standing': 'berdiri',\n",
    "    'start': 'mula',\n",
    "    'steady': 'mantap',\n",
    "    'stiap': 'setiap',\n",
    "    'stress': 'stres',\n",
    "    'student': 'pelajar',\n",
    "    'study': 'belajar',\n",
    "    'studycase': 'kajian kes',\n",
    "    'sure': 'pasti',\n",
    "    'sykt': 'syarikat',\n",
    "    'tah': 'entah',\n",
    "    'taik': 'tahi',\n",
    "    'takan': 'tak akan',\n",
    "    'takat': 'setakat',\n",
    "    'takde': 'tak ada',\n",
    "    'takkan': 'tak akan',\n",
    "    'taknak': 'tak nak',\n",
    "    'tang': 'tentang',\n",
    "    'tanggungjawab': 'bertanggungjawab',\n",
    "    'taraa': 'sementara',\n",
    "    'tau': 'tahu',\n",
    "    'tbabit': 'terbabit',\n",
    "    'team': 'pasukan',\n",
    "    'terbaekk': 'terbaik',\n",
    "    'teruknye': 'teruknya',\n",
    "    'tgk': 'tengok',\n",
    "    'that': 'itu',\n",
    "    'thinking': 'fikir',\n",
    "    'those': 'itu',\n",
    "    'time': 'masa',\n",
    "    'tk': 'tak',\n",
    "    'tnggongjwb': 'tanggungjawab',\n",
    "    'tngok': 'tengok',\n",
    "    'tngu': 'tunggu',\n",
    "    'to': 'kepada',\n",
    "    'tosak': 'rosak',\n",
    "    'tp': 'tapi',\n",
    "    'tpi': 'tapi',\n",
    "    'tpon': 'telefon',\n",
    "    'transfer': 'pindah',\n",
    "    'trgelak': 'tergelak',\n",
    "    'ts': 'tan sri',\n",
    "    'tstony': 'tan sri tony',\n",
    "    'tu': 'itu',\n",
    "    'tuh': 'itu',\n",
    "    'tula': 'itulah',\n",
    "    'umeno': 'umno',\n",
    "    'unfortunately': 'malangnya',\n",
    "    'unhappy': 'tidak gembira',\n",
    "    'up': 'naik',\n",
    "    'upkan': 'naikkan',\n",
    "    'ur': 'awak',\n",
    "    'utk': 'untuk',\n",
    "    'very': 'sangat',\n",
    "    'viral': 'tular',\n",
    "    'vote': 'undi',\n",
    "    'warning': 'amaran',\n",
    "    'warranty': 'waranti',\n",
    "    'wassap': 'whatsapp',\n",
    "    'wat': 'apa',\n",
    "    'weii': 'wei',\n",
    "    'well': 'maklumlah',\n",
    "    'win': 'menang',\n",
    "    'with': 'dengan',\n",
    "    'wt': 'buat',\n",
    "    'x': 'tak',\n",
    "    'tw': 'tahu',\n",
    "    'ye': 'ya',\n",
    "    'yee': 'ya',\n",
    "    'yg': 'yang',\n",
    "    'yng': 'yang',\n",
    "    'you': 'awak',\n",
    "    'your': 'awak',\n",
    "    'sakai': 'selekeh',\n",
    "    'rmb': 'billion ringgit',\n",
    "    'rmj': 'juta ringgit',\n",
    "    'rmk': 'ribu ringgit',\n",
    "    'rm': 'ringgit',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "permulaan = [\n",
    "    'bel',\n",
    "    'se',\n",
    "    'ter',\n",
    "    'men',\n",
    "    'meng',\n",
    "    'mem',\n",
    "    'memper',\n",
    "    'di',\n",
    "    'pe',\n",
    "    'me',\n",
    "    'ke',\n",
    "    'ber',\n",
    "    'pen',\n",
    "    'per',\n",
    "]\n",
    "\n",
    "hujung = ['kan', 'kah', 'lah', 'tah', 'nya', 'an', 'wan', 'wati', 'ita']\n",
    "\n",
    "def naive_stemmer(word):\n",
    "    assert isinstance(word, str), 'input must be a string'\n",
    "    hujung_result = [e for e in hujung if word.endswith(e)]\n",
    "    if len(hujung_result):\n",
    "        hujung_result = max(hujung_result, key = len)\n",
    "        if len(hujung_result):\n",
    "            word = word[: -len(hujung_result)]\n",
    "    permulaan_result = [e for e in permulaan if word.startswith(e)]\n",
    "    if len(permulaan_result):\n",
    "        permulaan_result = max(permulaan_result, key = len)\n",
    "        if len(permulaan_result):\n",
    "            word = word[len(permulaan_result) :]\n",
    "    return word\n",
    "\n",
    "def build_dataset(words, n_words):\n",
    "    count = [['GO', 0], ['PAD', 1], ['EOS', 2], ['UNK', 3]]\n",
    "    counter = collections.Counter(words).most_common(n_words)\n",
    "    count.extend(counter)\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    data = list()\n",
    "    unk_count = 0\n",
    "    for word in words:\n",
    "        index = dictionary.get(word, 3)\n",
    "        if index == 0:\n",
    "            unk_count += 1\n",
    "        data.append(index)\n",
    "    count[0][1] = unk_count\n",
    "    reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return data, count, dictionary, reversed_dictionary\n",
    "\n",
    "\n",
    "def classification_textcleaning(string):\n",
    "    string = re.sub(\n",
    "        'http\\S+|www.\\S+',\n",
    "        '',\n",
    "        ' '.join(\n",
    "            [i for i in string.split() if i.find('#') < 0 and i.find('@') < 0]\n",
    "        ),\n",
    "    )\n",
    "    string = unidecode(string).replace('.', ' . ').replace(',', ' , ')\n",
    "    string = re.sub('[^A-Za-z ]+', ' ', string)\n",
    "    string = re.sub(r'[ ]+', ' ', string.lower()).strip()\n",
    "    string = [rules_normalizer.get(w, w) for w in string.split()]\n",
    "    string = [naive_stemmer(word) for word in string]\n",
    "    return ' '.join([word for word in string if len(word) > 1])\n",
    "\n",
    "\n",
    "def str_idx(corpus, dic, maxlen, UNK = 3):\n",
    "    X = np.zeros((len(corpus), maxlen))\n",
    "    for i in range(len(corpus)):\n",
    "        for no, k in enumerate(corpus[i].split()[:maxlen][::-1]):\n",
    "            X[i, -1 - no] = dic.get(k, UNK)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'raja benar sangat benci rakyat minyak naik gala'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_textcleaning('kerajaan sebenarnya sangat bencikan rakyatnya, minyak naik dan segalanya')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('sentiment-data-v2.csv')\n",
    "Y = LabelEncoder().fit_transform(df.label)\n",
    "with open('polarity-negative-translated.txt','r') as fopen:\n",
    "    texts = fopen.read().split('\\n')\n",
    "labels = [0] * len(texts)\n",
    "\n",
    "with open('polarity-positive-translated.txt','r') as fopen:\n",
    "    positive_texts = fopen.read().split('\\n')\n",
    "labels += [1] * len(positive_texts)\n",
    "texts += positive_texts\n",
    "texts += df.iloc[:,1].tolist()\n",
    "labels += Y.tolist()\n",
    "\n",
    "assert len(labels) == len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('bm-amazon.json') as fopen:\n",
    "    amazon = json.load(fopen)\n",
    "    \n",
    "with open('bm-imdb.json') as fopen:\n",
    "    imdb = json.load(fopen)\n",
    "    \n",
    "with open('bm-yelp.json') as fopen:\n",
    "    yelp = json.load(fopen)\n",
    "    \n",
    "texts += amazon['negative']\n",
    "labels += [0] * len(amazon['negative'])\n",
    "texts += amazon['positive']\n",
    "labels += [1] * len(amazon['positive'])\n",
    "\n",
    "texts += imdb['negative']\n",
    "labels += [0] * len(imdb['negative'])\n",
    "texts += imdb['positive']\n",
    "labels += [1] * len(imdb['positive'])\n",
    "\n",
    "texts += yelp['negative']\n",
    "labels += [0] * len(yelp['negative'])\n",
    "texts += yelp['positive']\n",
    "labels += [1] * len(yelp['positive'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "for i in [i for i in os.listdir('negative') if 'Store' not in i]:\n",
    "    with open('negative/'+i) as fopen:\n",
    "        a = json.load(fopen)\n",
    "        texts += a\n",
    "        labels += [0] * len(a)\n",
    "        \n",
    "import os\n",
    "for i in [i for i in os.listdir('positive') if 'Store' not in i]:\n",
    "    with open('positive/'+i) as fopen:\n",
    "        a = json.load(fopen)\n",
    "        texts += a\n",
    "        labels += [1] * len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(texts)):\n",
    "    texts[i] = classification_textcleaning(texts[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab from size: 120097\n",
      "Most common words [('saya', 533028), ('yang', 204446), ('tidak', 164296), ('untuk', 129707), ('anda', 126091), ('hari', 88975)]\n",
      "Sample data [2672, 229, 363, 235, 235, 94, 1360, 5, 78, 679] ['ringkas', 'bodoh', 'bosan', 'kanak', 'kanak', 'lelaki', 'remaja', 'yang', 'begitu', 'muda']\n"
     ]
    }
   ],
   "source": [
    "concat = ' '.join(texts).split()\n",
    "vocabulary_size = len(list(set(concat)))\n",
    "data, count, dictionary, rev_dictionary = build_dataset(concat, vocabulary_size)\n",
    "print('vocab from size: %d'%(vocabulary_size))\n",
    "print('Most common words', count[4:10])\n",
    "print('Sample data', data[:10], [rev_dictionary[i] for i in data[:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_idx(corpus, dic, maxlen, UNK = 3):\n",
    "    X = np.zeros((len(corpus), maxlen))\n",
    "    for i in range(len(corpus)):\n",
    "        for no, k in enumerate(corpus[i].split()[:maxlen][::-1]):\n",
    "            val = dic[k] if k in dic else UNK\n",
    "            X[i, -1 - no] = val\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(inputs, attention_size):\n",
    "    hidden_size = inputs.shape[2].value\n",
    "    w_omega = tf.Variable(\n",
    "        tf.random_normal([hidden_size, attention_size], stddev = 0.1)\n",
    "    )\n",
    "    b_omega = tf.Variable(tf.random_normal([attention_size], stddev = 0.1))\n",
    "    u_omega = tf.Variable(tf.random_normal([attention_size], stddev = 0.1))\n",
    "    with tf.name_scope('v'):\n",
    "        v = tf.tanh(tf.tensordot(inputs, w_omega, axes = 1) + b_omega)\n",
    "    vu = tf.tensordot(v, u_omega, axes = 1, name = 'vu')\n",
    "    alphas = tf.nn.softmax(vu, name = 'alphas')\n",
    "    output = tf.reduce_sum(inputs * tf.expand_dims(alphas, -1), 1)\n",
    "    return output, alphas\n",
    "\n",
    "\n",
    "class Model:\n",
    "    def __init__(\n",
    "        self,\n",
    "        size_layer,\n",
    "        num_layers,\n",
    "        dimension_output,\n",
    "        learning_rate,\n",
    "        dropout,\n",
    "        dict_size,\n",
    "    ):\n",
    "        def cells(size, reuse = False):\n",
    "            return tf.contrib.rnn.DropoutWrapper(\n",
    "                tf.nn.rnn_cell.LSTMCell(\n",
    "                    size,\n",
    "                    initializer = tf.orthogonal_initializer(),\n",
    "                    reuse = reuse,\n",
    "                ),\n",
    "                state_keep_prob = dropout,\n",
    "                output_keep_prob = dropout,\n",
    "            )\n",
    "\n",
    "        self.X = tf.placeholder(tf.int32, [None, None])\n",
    "        self.Y = tf.placeholder(tf.int32, [None])\n",
    "        encoder_embeddings = tf.Variable(\n",
    "            tf.random_uniform([dict_size, size_layer], -1, 1)\n",
    "        )\n",
    "        encoder_embedded = tf.nn.embedding_lookup(encoder_embeddings, self.X)\n",
    "\n",
    "        for n in range(num_layers):\n",
    "            (out_fw, out_bw), (\n",
    "                state_fw,\n",
    "                state_bw,\n",
    "            ) = tf.nn.bidirectional_dynamic_rnn(\n",
    "                cell_fw = cells(size_layer),\n",
    "                cell_bw = cells(size_layer),\n",
    "                inputs = encoder_embedded,\n",
    "                dtype = tf.float32,\n",
    "                scope = 'bidirectional_rnn_%d' % (n),\n",
    "            )\n",
    "            encoder_embedded = tf.concat((out_fw, out_bw), 2)\n",
    "        self.outputs, self.attention = attention(encoder_embedded, size_layer)\n",
    "        W = tf.get_variable(\n",
    "            'w',\n",
    "            shape = (size_layer * 2, 2),\n",
    "            initializer = tf.orthogonal_initializer(),\n",
    "        )\n",
    "        b = tf.get_variable(\n",
    "            'b', shape = (2), initializer = tf.zeros_initializer()\n",
    "        )\n",
    "        self.logits = tf.add(tf.matmul(self.outputs, W), b, name = 'logits')\n",
    "        self.cost = tf.reduce_mean(\n",
    "            tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                logits = self.logits, labels = self.Y\n",
    "            )\n",
    "        )\n",
    "        self.optimizer = tf.train.AdamOptimizer(\n",
    "            learning_rate = learning_rate\n",
    "        ).minimize(self.cost)\n",
    "        self.accuracy = tf.reduce_mean(\n",
    "            tf.cast(tf.nn.in_top_k(self.logits, self.Y, 1), tf.float32)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hierarchical/model.ckpt'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size_layer = 256\n",
    "num_layers = 2\n",
    "dropout = 0.8\n",
    "dimension_output = 2\n",
    "learning_rate = 1e-4\n",
    "batch_size = 32\n",
    "maxlen = 100\n",
    "dropout = 0.8\n",
    "\n",
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "model = Model(\n",
    "    size_layer,\n",
    "    num_layers,\n",
    "    dimension_output,\n",
    "    learning_rate,\n",
    "    dropout,\n",
    "    len(dictionary),\n",
    ")\n",
    "sess.run(tf.global_variables_initializer())\n",
    "saver = tf.train.Saver(tf.trainable_variables())\n",
    "saver.save(sess, 'hierarchical/model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "strings = ','.join(\n",
    "    [\n",
    "        n.name\n",
    "        for n in tf.get_default_graph().as_graph_def().node\n",
    "        if ('Variable' in n.op\n",
    "        or 'Placeholder' in n.name\n",
    "        or 'logits' in n.name\n",
    "        or 'alphas' in n.name)\n",
    "        and 'Adam' not in n.name\n",
    "        and 'beta' not in n.name\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Placeholder',\n",
       " 'Placeholder_1',\n",
       " 'Variable',\n",
       " 'bidirectional_rnn_0/fw/lstm_cell/kernel',\n",
       " 'bidirectional_rnn_0/fw/lstm_cell/bias',\n",
       " 'bidirectional_rnn_0/bw/lstm_cell/kernel',\n",
       " 'bidirectional_rnn_0/bw/lstm_cell/bias',\n",
       " 'bidirectional_rnn_1/fw/lstm_cell/kernel',\n",
       " 'bidirectional_rnn_1/fw/lstm_cell/bias',\n",
       " 'bidirectional_rnn_1/bw/lstm_cell/kernel',\n",
       " 'bidirectional_rnn_1/bw/lstm_cell/bias',\n",
       " 'Variable_1',\n",
       " 'Variable_2',\n",
       " 'Variable_3',\n",
       " 'alphas',\n",
       " 'w',\n",
       " 'b',\n",
       " 'logits',\n",
       " 'gradients/logits_grad/Shape',\n",
       " 'gradients/logits_grad/Shape_1',\n",
       " 'gradients/logits_grad/BroadcastGradientArgs',\n",
       " 'gradients/logits_grad/Sum',\n",
       " 'gradients/logits_grad/Reshape',\n",
       " 'gradients/logits_grad/Sum_1',\n",
       " 'gradients/logits_grad/Reshape_1',\n",
       " 'gradients/logits_grad/tuple/group_deps',\n",
       " 'gradients/logits_grad/tuple/control_dependency',\n",
       " 'gradients/logits_grad/tuple/control_dependency_1',\n",
       " 'gradients/alphas_grad/mul',\n",
       " 'gradients/alphas_grad/Sum/reduction_indices',\n",
       " 'gradients/alphas_grad/Sum',\n",
       " 'gradients/alphas_grad/sub',\n",
       " 'gradients/alphas_grad/mul_1']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strings.split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'Variable:0' shape=(120101, 256) dtype=float32_ref>,\n",
       " <tf.Variable 'bidirectional_rnn_0/fw/lstm_cell/kernel:0' shape=(512, 1024) dtype=float32_ref>,\n",
       " <tf.Variable 'bidirectional_rnn_0/fw/lstm_cell/bias:0' shape=(1024,) dtype=float32_ref>,\n",
       " <tf.Variable 'bidirectional_rnn_0/bw/lstm_cell/kernel:0' shape=(512, 1024) dtype=float32_ref>,\n",
       " <tf.Variable 'bidirectional_rnn_0/bw/lstm_cell/bias:0' shape=(1024,) dtype=float32_ref>,\n",
       " <tf.Variable 'bidirectional_rnn_1/fw/lstm_cell/kernel:0' shape=(768, 1024) dtype=float32_ref>,\n",
       " <tf.Variable 'bidirectional_rnn_1/fw/lstm_cell/bias:0' shape=(1024,) dtype=float32_ref>,\n",
       " <tf.Variable 'bidirectional_rnn_1/bw/lstm_cell/kernel:0' shape=(768, 1024) dtype=float32_ref>,\n",
       " <tf.Variable 'bidirectional_rnn_1/bw/lstm_cell/bias:0' shape=(1024,) dtype=float32_ref>,\n",
       " <tf.Variable 'Variable_1:0' shape=(512, 256) dtype=float32_ref>,\n",
       " <tf.Variable 'Variable_2:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'Variable_3:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'w:0' shape=(512, 2) dtype=float32_ref>,\n",
       " <tf.Variable 'b:0' shape=(2,) dtype=float32_ref>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.trainable_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, test_X, train_Y, test_Y = train_test_split(texts, \n",
    "                                                    labels,\n",
    "                                                    test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 16876/16876 [2:08:20<00:00,  2.19it/s, accuracy=0.944, cost=0.249] \n",
      "test minibatch loop: 100%|██████████| 4219/4219 [13:26<00:00,  5.26it/s, accuracy=0.759, cost=0.419]\n",
      "train minibatch loop:   0%|          | 0/16876 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, pass acc: 0.000000, current acc: 0.784551\n",
      "time taken: 8507.157941102982\n",
      "epoch: 0, training loss: 0.489663, training acc: 0.760077, valid loss: 0.454907, valid acc: 0.784551\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 16876/16876 [2:08:07<00:00,  2.20it/s, accuracy=0.944, cost=0.221] \n",
      "test minibatch loop: 100%|██████████| 4219/4219 [13:30<00:00,  5.17it/s, accuracy=0.828, cost=0.454]\n",
      "train minibatch loop:   0%|          | 0/16876 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, pass acc: 0.784551, current acc: 0.795596\n",
      "time taken: 8497.88771367073\n",
      "epoch: 1, training loss: 0.434585, training acc: 0.796648, valid loss: 0.437492, valid acc: 0.795596\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 16876/16876 [2:08:25<00:00,  2.19it/s, accuracy=0.944, cost=0.179] \n",
      "test minibatch loop: 100%|██████████| 4219/4219 [13:31<00:00,  5.19it/s, accuracy=0.828, cost=0.466]\n",
      "train minibatch loop:   0%|          | 0/16876 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2, pass acc: 0.795596, current acc: 0.799085\n",
      "time taken: 8516.746159553528\n",
      "epoch: 2, training loss: 0.410119, training acc: 0.811333, valid loss: 0.432583, valid acc: 0.799085\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 16876/16876 [2:08:33<00:00,  2.22it/s, accuracy=0.889, cost=0.185] \n",
      "test minibatch loop: 100%|██████████| 4219/4219 [13:31<00:00,  5.18it/s, accuracy=0.793, cost=0.478]\n",
      "train minibatch loop:   0%|          | 0/16876 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3, pass acc: 0.799085, current acc: 0.800788\n",
      "time taken: 8524.52553153038\n",
      "epoch: 3, training loss: 0.389934, training acc: 0.823092, valid loss: 0.433733, valid acc: 0.800788\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 16876/16876 [2:39:57<00:00,  2.21it/s, accuracy=0.944, cost=0.185]  \n",
      "test minibatch loop: 100%|██████████| 4219/4219 [13:30<00:00,  5.18it/s, accuracy=0.828, cost=0.443]\n",
      "train minibatch loop:   0%|          | 0/16876 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken: 10408.229004621506\n",
      "epoch: 4, training loss: 0.369901, training acc: 0.834267, valid loss: 0.441326, valid acc: 0.799878\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 16876/16876 [2:08:11<00:00,  2.21it/s, accuracy=1, cost=0.142]     \n",
      "test minibatch loop: 100%|██████████| 4219/4219 [13:28<00:00,  5.20it/s, accuracy=0.793, cost=0.497]\n",
      "train minibatch loop:   0%|          | 0/16876 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken: 8500.625365018845\n",
      "epoch: 5, training loss: 0.347550, training acc: 0.846581, valid loss: 0.458445, valid acc: 0.797055\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 16876/16876 [2:08:37<00:00,  2.21it/s, accuracy=1, cost=0.0739]    \n",
      "test minibatch loop: 100%|██████████| 4219/4219 [13:26<00:00,  5.18it/s, accuracy=0.862, cost=0.447]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken: 8523.755964040756\n",
      "epoch: 6, training loss: 0.324115, training acc: 0.859231, valid loss: 0.477738, valid acc: 0.795056\n",
      "\n",
      "break epoch:7\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "EARLY_STOPPING, CURRENT_CHECKPOINT, CURRENT_ACC, EPOCH = 3, 0, 0, 0\n",
    "\n",
    "while True:\n",
    "    lasttime = time.time()\n",
    "    if CURRENT_CHECKPOINT == EARLY_STOPPING:\n",
    "        print('break epoch:%d\\n' % (EPOCH))\n",
    "        break\n",
    "\n",
    "    train_acc, train_loss, test_acc, test_loss = 0, 0, 0, 0\n",
    "    pbar = tqdm(\n",
    "        range(0, len(train_X), batch_size), desc = 'train minibatch loop'\n",
    "    )\n",
    "    for i in pbar:\n",
    "        batch_x = str_idx(train_X[i : min(i + batch_size, len(train_X))], dictionary, maxlen)\n",
    "        batch_y = train_Y[i : min(i + batch_size, len(train_X))]\n",
    "        batch_x_expand = np.expand_dims(batch_x,axis = 1)\n",
    "        acc, cost, _ = sess.run(\n",
    "            [model.accuracy, model.cost, model.optimizer],\n",
    "            feed_dict = {\n",
    "                model.Y: batch_y,\n",
    "                model.X: batch_x\n",
    "            },\n",
    "        )\n",
    "        assert not np.isnan(cost)\n",
    "        train_loss += cost\n",
    "        train_acc += acc\n",
    "        pbar.set_postfix(cost = cost, accuracy = acc)\n",
    "\n",
    "    pbar = tqdm(range(0, len(test_X), batch_size), desc = 'test minibatch loop')\n",
    "    for i in pbar:\n",
    "        batch_x = str_idx(test_X[i : min(i + batch_size, len(test_X))], dictionary, maxlen)\n",
    "        batch_y = test_Y[i : min(i + batch_size, len(test_X))]\n",
    "        batch_x_expand = np.expand_dims(batch_x,axis = 1)\n",
    "        acc, cost = sess.run(\n",
    "            [model.accuracy, model.cost],\n",
    "            feed_dict = {\n",
    "                model.Y: batch_y,\n",
    "                model.X: batch_x\n",
    "            },\n",
    "        )\n",
    "        test_loss += cost\n",
    "        test_acc += acc\n",
    "        pbar.set_postfix(cost = cost, accuracy = acc)\n",
    "\n",
    "    train_loss /= len(train_X) / batch_size\n",
    "    train_acc /= len(train_X) / batch_size\n",
    "    test_loss /= len(test_X) / batch_size\n",
    "    test_acc /= len(test_X) / batch_size\n",
    "\n",
    "    if test_acc > CURRENT_ACC:\n",
    "        print(\n",
    "            'epoch: %d, pass acc: %f, current acc: %f'\n",
    "            % (EPOCH, CURRENT_ACC, test_acc)\n",
    "        )\n",
    "        CURRENT_ACC = test_acc\n",
    "        CURRENT_CHECKPOINT = 0\n",
    "    else:\n",
    "        CURRENT_CHECKPOINT += 1\n",
    "\n",
    "    print('time taken:', time.time() - lasttime)\n",
    "    print(\n",
    "        'epoch: %d, training loss: %f, training acc: %f, valid loss: %f, valid acc: %f\\n'\n",
    "        % (EPOCH, train_loss, train_acc, test_loss, test_acc)\n",
    "    )\n",
    "    EPOCH += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "validation minibatch loop: 100%|██████████| 4219/4219 [13:22<00:00,  5.33it/s]\n"
     ]
    }
   ],
   "source": [
    "real_Y, predict_Y = [], []\n",
    "\n",
    "pbar = tqdm(\n",
    "    range(0, len(test_X), batch_size), desc = 'validation minibatch loop'\n",
    ")\n",
    "for i in pbar:\n",
    "    batch_x = str_idx(test_X[i : min(i + batch_size, len(test_X))], dictionary, maxlen)\n",
    "    batch_y = test_Y[i : min(i + batch_size, len(test_X))]\n",
    "    predict_Y += np.argmax(\n",
    "        sess.run(\n",
    "            model.logits, feed_dict = {model.X: batch_x, model.Y: batch_y}\n",
    "        ),\n",
    "        1,\n",
    "    ).tolist()\n",
    "    real_Y += batch_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.80      0.80     70440\n",
      "    positive       0.78      0.79      0.79     64565\n",
      "\n",
      "   micro avg       0.80      0.80      0.80    135005\n",
      "   macro avg       0.80      0.80      0.80    135005\n",
      "weighted avg       0.80      0.80      0.80    135005\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    metrics.classification_report(\n",
    "        real_Y, predict_Y, target_names = ['negative', 'positive']\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.93546665, 0.06453338]], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = classification_textcleaning('kerajaan sebenarnya sangat bencikan rakyatnya, minyak naik dan segalanya')\n",
    "new_vector = str_idx([text], dictionary, len(text.split()))\n",
    "sess.run(tf.nn.softmax(model.logits), feed_dict={model.X:new_vector})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('hierarchical-sentiment.json','w') as fopen:\n",
    "    fopen.write(json.dumps({'dictionary':dictionary,'reverse_dictionary':rev_dictionary}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hierarchical/model.ckpt'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saver.save(sess, 'hierarchical/model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_graph(model_dir, output_node_names):\n",
    "\n",
    "    if not tf.gfile.Exists(model_dir):\n",
    "        raise AssertionError(\n",
    "            \"Export directory doesn't exists. Please specify an export \"\n",
    "            'directory: %s' % model_dir\n",
    "        )\n",
    "\n",
    "    checkpoint = tf.train.get_checkpoint_state(model_dir)\n",
    "    input_checkpoint = checkpoint.model_checkpoint_path\n",
    "\n",
    "    absolute_model_dir = '/'.join(input_checkpoint.split('/')[:-1])\n",
    "    output_graph = absolute_model_dir + '/frozen_model.pb'\n",
    "    clear_devices = True\n",
    "    with tf.Session(graph = tf.Graph()) as sess:\n",
    "        saver = tf.train.import_meta_graph(\n",
    "            input_checkpoint + '.meta', clear_devices = clear_devices\n",
    "        )\n",
    "        saver.restore(sess, input_checkpoint)\n",
    "        output_graph_def = tf.graph_util.convert_variables_to_constants(\n",
    "            sess,\n",
    "            tf.get_default_graph().as_graph_def(),\n",
    "            output_node_names.split(','),\n",
    "        )\n",
    "        with tf.gfile.GFile(output_graph, 'wb') as f:\n",
    "            f.write(output_graph_def.SerializeToString())\n",
    "        print('%d ops in the final graph.' % len(output_graph_def.node))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from hierarchical/model.ckpt\n",
      "INFO:tensorflow:Froze 14 variables.\n",
      "INFO:tensorflow:Converted 14 variables to const ops.\n",
      "756 ops in the final graph.\n"
     ]
    }
   ],
   "source": [
    "freeze_graph('hierarchical', strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_graph(frozen_graph_filename):\n",
    "    with tf.gfile.GFile(frozen_graph_filename, 'rb') as f:\n",
    "        graph_def = tf.GraphDef()\n",
    "        graph_def.ParseFromString(f.read())\n",
    "    with tf.Graph().as_default() as graph:\n",
    "        tf.import_graph_def(graph_def)\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    }
   ],
   "source": [
    "g = load_graph('hierarchical/frozen_model.pb')\n",
    "x = g.get_tensor_by_name('import/Placeholder:0')\n",
    "logits = g.get_tensor_by_name('import/logits:0')\n",
    "alphas = g.get_tensor_by_name('import/alphas:0')\n",
    "test_sess = tf.InteractiveSession(graph = g)\n",
    "result = test_sess.run([logits, alphas], feed_dict = {x: new_vector})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
