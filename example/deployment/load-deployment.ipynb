{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import malaya"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Disable file validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you deployed some of Malaya models on persist, short-life (auto-restart to reduce memory consumption) and async / multiprocess workers, you might get errors related to file checking. You can skip this error as long you able to persist malaya models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### download model\n",
    "\n",
    "So, first you need to download the model into your local machine / environment, run this on different script,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loading sentence piece model\n"
     ]
    }
   ],
   "source": [
    "model = malaya.zero_shot.classification.transformer(model = 'tiny-albert')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load model\n",
    "\n",
    "Load model without need to check model, run this on top of fastapi / flask / gunicorn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loading sentence piece model\n"
     ]
    }
   ],
   "source": [
    "model = malaya.zero_shot.classification.transformer(model = 'tiny-albert', validate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This loaded model able to share among multi-workers / multi-threads."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## disable type checking\n",
    "\n",
    "Make sure you already install latest version herpetologist,\n",
    "\n",
    "```bash\n",
    "pip install herpetologist -U\n",
    "```\n",
    "\n",
    "If you pick Malaya source code, you can see we check parameters on function / method definition, https://github.com/huseinzol05/Malaya/blob/master/malaya/model/bert.py#L232\n",
    "\n",
    "We use herpetologist to check passed variables, https://github.com/huseinzol05/herpetologist\n",
    "\n",
    "```python\n",
    "@check_type\n",
    "def predict(self, strings: List[str], add_neutral: bool = True):\n",
    "    \"\"\"\n",
    "    classify a string.\n",
    "    Parameters\n",
    "    ----------\n",
    "    strings: List[str]\n",
    "    add_neutral: bool, optional (default=True)\n",
    "        if True, it will add neutral probability.\n",
    "    Returns\n",
    "    -------\n",
    "    result: List[str]\n",
    "    \"\"\"\n",
    "```\n",
    "\n",
    "`@check_type` will check `strings` is a `List[str]` or not, if not, it will throw an error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But this `@check_type` will become expensive if you have massive list of strings. So you can disable to this type checking by simply set bash environment.\n",
    "\n",
    "Some of our environments we want to enable it, some of it also we want to disable, and we do not want herpetologist to keep check the variables. So to disable it, simply set bash environment,\n",
    "\n",
    "```bash\n",
    "export ENABLE_HERPETOLOGIST=false\n",
    "```\n",
    "\n",
    "Or, using python,\n",
    "\n",
    "```python\n",
    "import os\n",
    "os.environ['ENABLE_HERPETOLOGIST'] = 'false'\n",
    "```\n",
    "\n",
    "You can see impact of time execution in this [example](https://github.com/huseinzol05/herpetologist/blob/master/example.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use smaller model\n",
    "\n",
    "Stacking multiple smaller models much faster than a single big model. But this cannot ensure the accuracy will be same as the big model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## docker example\n",
    "\n",
    "You can check some docker examples and benchmarks at here, https://github.com/huseinzol05/Malaya/tree/master/misc/deployment.\n",
    "\n",
    "The purpose of these benchmarks, how fast and how much requests for a model able to serve on perfect minibatch realtime, let say live streaming data from social media to detect sentiment, whether a text is a negative or a positive. Tested on ALBERT-BASE sentiment model.\n",
    "\n",
    "These are my machine specifications,\n",
    "\n",
    "1. Intel(R) Core(TM) i7-8557U CPU @ 1.70GHz\n",
    "2. 16 GB 2133 MHz LPDDR3\n",
    "\n",
    "And I use same wrk command,\n",
    "\n",
    "```bash\n",
    "wrk -t15 -c600 -d1m --timeout=15s http://localhost:8080/?string=husein%20sangat%20comel%20dan%20handsome%20tambahan%20lagi%20ketiak%20wangi\n",
    "```\n",
    "\n",
    "Some constraints,\n",
    "\n",
    "1. ALBERT BASE is around 43MB.\n",
    "2. Limit memory is 2GB, set by Docker itself.\n",
    "3. batch size of 50 strings, duplicate 50 times of `husein sangat comel dan handsome tambahan lagi ketiak wangi`, can check every deployment in app.py or main.py.\n",
    "4. No limit on CPU usage.\n",
    "5. no caching."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### fast-api\n",
    "\n",
    "workers automatically calculated by fast-api, https://github.com/huseinzol05/Malaya/tree/master/misc/deployment/fast-api\n",
    "\n",
    "```text\n",
    "Running 1m test @ http://localhost:8080/?string=husein%20sangat%20comel%20dan%20handsome%20tambahan%20lagi%20ketiak%20wangi\n",
    "  15 threads and 600 connections\n",
    "  Thread Stats   Avg      Stdev     Max   +/- Stdev\n",
    "    Latency     0.00us    0.00us   0.00us     nan%\n",
    "    Req/Sec     0.24      1.16     9.00     95.52%\n",
    "  68 requests in 1.00m, 8.96KB read\n",
    "  Socket errors: connect 364, read 293, write 0, timeout 68\n",
    "Requests/sec:      1.13\n",
    "Transfer/sec:     152.75B\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gunicorn Flask\n",
    "\n",
    "5 sync workers, https://github.com/huseinzol05/Malaya/tree/master/misc/deployment/gunicorn-flask\n",
    "\n",
    "```text\n",
    "Running 1m test @ http://localhost:8080/?string=husein%20sangat%20comel%20dan%20handsome%20tambahan%20lagi%20ketiak%20wangi\n",
    "  15 threads and 600 connections\n",
    "  Thread Stats   Avg      Stdev     Max   +/- Stdev\n",
    "    Latency     7.98s     3.25s   12.71s    41.67%\n",
    "    Req/Sec     0.49      1.51     9.00     90.91%\n",
    "  59 requests in 1.00m, 9.10KB read\n",
    "  Socket errors: connect 364, read 39, write 0, timeout 47\n",
    "Requests/sec:      0.98\n",
    "Transfer/sec:     155.12B\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UWSGI Flask + Auto scaling\n",
    "\n",
    "Min 2 worker, Max 10 workers, spare2 algorithm, https://github.com/huseinzol05/Malaya/tree/master/misc/deployment/uwsgi-flask-cheaper\n",
    "    \n",
    "```text\n",
    "Running 1m test @ http://localhost:8080/?string=husein%20sangat%20comel%20dan%20handsome%20tambahan%20lagi%20ketiak%20wangi\n",
    "  15 threads and 600 connections\n",
    "  Thread Stats   Avg      Stdev     Max   +/- Stdev\n",
    "    Latency     8.80s     4.16s   14.73s    62.50%\n",
    "    Req/Sec     0.75      2.60     9.00     91.67%\n",
    "  12 requests in 1.00m, 0.90KB read\n",
    "  Socket errors: connect 364, read 105, write 0, timeout 4\n",
    "Requests/sec:      0.20\n",
    "Transfer/sec:      15.37B\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UWSGI Flask\n",
    "\n",
    "4 Workers, https://github.com/huseinzol05/Malaya/tree/master/misc/deployment/uwsgi-flask-fork\n",
    "\n",
    "```text\n",
    "Running 1m test @ http://localhost:8080/?string=husein%20sangat%20comel%20dan%20handsome%20tambahan%20lagi%20ketiak%20wangi\n",
    "  15 threads and 600 connections\n",
    "  Thread Stats   Avg      Stdev     Max   +/- Stdev\n",
    "    Latency     8.79s     4.13s   14.87s    53.33%\n",
    "    Req/Sec     1.06      3.16    20.00     92.59%\n",
    "  56 requests in 1.00m, 4.21KB read\n",
    "  Socket errors: connect 364, read 345, write 0, timeout 41\n",
    "Requests/sec:      0.93\n",
    "Transfer/sec:      71.74B\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
