{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('gsd-ud-train.conllu.txt') as fopen:\n",
    "    corpus = fopen.read().split('\\n')\n",
    "    \n",
    "with open('gsd-ud-test.conllu.txt') as fopen:\n",
    "    corpus.extend(fopen.read().split('\\n'))\n",
    "    \n",
    "with open('gsd-ud-dev.conllu.txt') as fopen:\n",
    "    corpus.extend(fopen.read().split('\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_string(string):\n",
    "    string = re.sub('[^A-Za-z0-9\\-\\/ ]+', ' ', string).split()\n",
    "    return [to_title(y.strip()) for y in string]\n",
    "\n",
    "def to_title(string):\n",
    "    if string.isupper():\n",
    "        string = string.title()\n",
    "    return string\n",
    "\n",
    "def process_corpus(corpus, until = None):\n",
    "    sentences, words, depends, labels = [], [], [], []\n",
    "    temp_sentence, temp_word, temp_depend, temp_label = [], [], [], []\n",
    "    for sentence in corpus:\n",
    "        if len(sentence):\n",
    "            if sentence[0] == '#':\n",
    "                continue\n",
    "            sentence = sentence.split('\\t')\n",
    "            temp = process_string(sentence[1])\n",
    "            if not len(temp):\n",
    "                sentence[1] = 'EMPTY'\n",
    "            sentence[1] = process_string(sentence[1])[0]\n",
    "            temp_word.append(sentence[1])\n",
    "            temp_depend.append(str(int(sentence[6])))\n",
    "            temp_label.append(sentence[7])\n",
    "            temp_sentence.append(sentence[1])\n",
    "        else:\n",
    "            words.append(temp_word)\n",
    "            depends.append(temp_depend)\n",
    "            labels.append(temp_label)\n",
    "            sentences.append(temp_sentence)\n",
    "            temp_word = []\n",
    "            temp_depend = []\n",
    "            temp_label = []\n",
    "            temp_sentence = []\n",
    "    return sentences[:-1], words[:-1], depends[:-1], labels[:-1]\n",
    "\n",
    "sentences, words, depends, labels = process_corpus(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag.util import untag\n",
    "\n",
    "def features(sentence, index):\n",
    "    \"\"\" sentence: [w1, w2, ...], index: the index of the word \"\"\"\n",
    "    return {\n",
    "        'word': sentence[index],\n",
    "        'is_first': index == 0,\n",
    "        'is_last': index == len(sentence) - 1,\n",
    "        'prefix-1': sentence[index][0],\n",
    "        'prefix-2': sentence[index][:2],\n",
    "        'prefix-3': sentence[index][:3],\n",
    "        'suffix-1': sentence[index][-1],\n",
    "        'suffix-2': sentence[index][-2:],\n",
    "        'suffix-3': sentence[index][-3:],\n",
    "        'prev_word': '' if index == 0 else sentence[index - 1],\n",
    "        'prev_word-prefix-1': '' if index == 0 else sentence[index - 1][0],\n",
    "        'prev_word-prefix-2': '' if index == 0 else sentence[index - 1][:2],\n",
    "        'prev_word-prefix-3': '' if index == 0 else sentence[index - 1][:3],\n",
    "        'prev_word-suffix-1': '' if index == 0 else sentence[index - 1][-1],\n",
    "        'prev_word-suffix-2': '' if index == 0 else sentence[index - 1][-2:],\n",
    "        'prev_word-suffix-3': '' if index == 0 else sentence[index - 1][-3:],\n",
    "        'next_word-prefix-1': '' if index == len(sentence) - 1 else sentence[index + 1][0],\n",
    "        'next_word-prefix-2': '' if index == len(sentence) - 1 else sentence[index + 1][:2],\n",
    "        'next_word-prefix-3': '' if index == len(sentence) - 1 else sentence[index + 1][:3],\n",
    "        'next_word-suffix-1': '' if index == len(sentence) - 1 else sentence[index + 1][-1],\n",
    "        'next_word-suffix-2': '' if index == len(sentence) - 1 else sentence[index + 1][-2:],\n",
    "        'next_word-suffix-3': '' if index == len(sentence) - 1 else sentence[index + 1][-3:],\n",
    "        'next_word': '' if index == len(sentence) - 1 else sentence[index + 1],\n",
    "        'has_hyphen': '-' in sentence[index],\n",
    "        'is_numeric': sentence[index].isdigit(),\n",
    "    }\n",
    "\n",
    "def features_crf_dependency(sentence, tag, index):\n",
    "    return {\n",
    "        'word': sentence[index],\n",
    "        'tag': tag[index],\n",
    "        'is_first': index == 0,\n",
    "        'is_last': index == len(sentence) - 1,\n",
    "        'prefix-1': sentence[index][0],\n",
    "        'prefix-2': sentence[index][:2],\n",
    "        'prefix-3': sentence[index][:3],\n",
    "        'suffix-1': sentence[index][-1],\n",
    "        'suffix-2': sentence[index][-2:],\n",
    "        'suffix-3': sentence[index][-3:],\n",
    "        'prev_word': '' if index == 0 else sentence[index - 1],\n",
    "        'prev_word-prefix-1': '' if index == 0 else sentence[index - 1][0],\n",
    "        'prev_word-prefix-2': '' if index == 0 else sentence[index - 1][:2],\n",
    "        'prev_word-prefix-3': '' if index == 0 else sentence[index - 1][:3],\n",
    "        'prev_word-suffix-1': '' if index == 0 else sentence[index - 1][-1],\n",
    "        'prev_word-suffix-2': '' if index == 0 else sentence[index - 1][-2:],\n",
    "        'prev_word-suffix-3': '' if index == 0 else sentence[index - 1][-3:],\n",
    "        'next_word-prefix-1': ''\n",
    "        if index == len(sentence) - 1\n",
    "        else sentence[index + 1][0],\n",
    "        'next_word-prefix-2': ''\n",
    "        if index == len(sentence) - 1\n",
    "        else sentence[index + 1][:2],\n",
    "        'next_word-prefix-3': ''\n",
    "        if index == len(sentence) - 1\n",
    "        else sentence[index + 1][:3],\n",
    "        'next_word-suffix-1': ''\n",
    "        if index == len(sentence) - 1\n",
    "        else sentence[index + 1][-1],\n",
    "        'next_word-suffix-2': ''\n",
    "        if index == len(sentence) - 1\n",
    "        else sentence[index + 1][-2:],\n",
    "        'next_word-suffix-3': ''\n",
    "        if index == len(sentence) - 1\n",
    "        else sentence[index + 1][-3:],\n",
    "        'next_word': '' if index == len(sentence) - 1 else sentence[index + 1],\n",
    "        'has_hyphen': '-' in sentence[index],\n",
    "        'is_numeric': sentence[index].isdigit(),\n",
    "    }\n",
    "\n",
    "def transform_to_dataset(words, labels):\n",
    "    X, y = [], []\n",
    " \n",
    "    for no, tagged in enumerate(labels):\n",
    "        X.append([features(words[no], index) for index in range(len(words[no]))])\n",
    "        y.append([tag for tag in tagged])\n",
    " \n",
    "    return X, y\n",
    "\n",
    "def transform_to_dataset_depend(words, labels, depends):\n",
    "    X, y = [], []\n",
    " \n",
    "    for no, tagged in enumerate(labels):\n",
    "        X.append([features_crf_dependency(words[no], depends[no], index) for index in range(len(words[no]))])\n",
    "        y.append([tag for tag in tagged])\n",
    " \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = transform_to_dataset(words, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn_crfsuite\n",
    "from sklearn_crfsuite import scorers\n",
    "from sklearn_crfsuite import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 3s, sys: 347 ms, total: 2min 3s\n",
      "Wall time: 2min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "crf = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs',\n",
    "    c1=0.1,\n",
    "    c2=0.1,\n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=True\n",
    ")\n",
    "crf.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9541090910035434"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = crf.predict(X)\n",
    "metrics.flat_f1_score(Y, y_pred,\n",
    "                      average='weighted', labels = list(crf.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "        nsubj      0.931     0.962     0.946      7125\n",
      "          cop      0.998     1.000     0.999      1055\n",
      "          det      0.969     0.968     0.969      4475\n",
      "         root      0.869     0.953     0.909      5593\n",
      "   nsubj:pass      0.929     0.872     0.900      2216\n",
      "          acl      0.935     0.890     0.912      3346\n",
      "         case      0.992     0.994     0.993     11897\n",
      "          obl      0.876     0.906     0.891      6346\n",
      "         flat      0.978     0.985     0.982     11402\n",
      "        punct      0.994     1.000     0.997     18483\n",
      "        appos      0.933     0.921     0.927      2662\n",
      "         amod      0.932     0.935     0.934      4566\n",
      "     compound      0.962     0.956     0.959      7432\n",
      "       advmod      0.966     0.974     0.970      5288\n",
      "           cc      0.993     0.987     0.990      3571\n",
      "          obj      0.948     0.948     0.948      5795\n",
      "         conj      0.952     0.931     0.941      4806\n",
      "         mark      0.944     0.956     0.950      1574\n",
      "        advcl      0.936     0.815     0.871      1356\n",
      "         nmod      0.882     0.837     0.859      4442\n",
      "       nummod      0.975     0.980     0.978      4316\n",
      "          dep      0.970     0.683     0.802       565\n",
      "        xcomp      0.920     0.911     0.915      1373\n",
      "        ccomp      0.970     0.747     0.844       482\n",
      "    parataxis      0.932     0.718     0.811       404\n",
      "compound:plur      0.974     0.996     0.985       679\n",
      "        fixed      0.944     0.899     0.921       615\n",
      "          aux      1.000     1.000     1.000         3\n",
      "        csubj      1.000     0.923     0.960        26\n",
      "         iobj      1.000     0.864     0.927        22\n",
      "   csubj:pass      0.857     0.750     0.800         8\n",
      "\n",
      "  avg / total      0.955     0.954     0.954    121923\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.flat_classification_report(\n",
    "    Y, y_pred, labels=list(crf.classes_), digits=3\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('crf-label.pkl','wb') as fopen:\n",
    "    pickle.dump(crf,fopen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_depend, Y_depend = transform_to_dataset_depend(words, depends, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 26min 5s, sys: 2.18 s, total: 26min 7s\n",
      "Wall time: 26min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "crf_depend = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs',\n",
    "    c1=0.1,\n",
    "    c2=0.1,\n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=True\n",
    ")\n",
    "crf_depend.fit(X_depend, Y_depend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7909675818232262"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = crf_depend.predict(X_depend)\n",
    "metrics.flat_f1_score(Y_depend, y_pred,\n",
    "                      average='weighted', labels = list(crf_depend.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = 'Dr Mahathir menasihati mereka supaya berhenti berehat dan tidur sebentar sekiranya mengantuk ketika memandu.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dr det\n",
      "Mahathir nsubj\n",
      "menasihati root\n",
      "mereka obj\n",
      "supaya mark\n",
      "berhenti advcl\n",
      "berehat xcomp\n",
      "dan cc\n",
      "tidur conj\n",
      "sebentar case\n",
      "sekiranya nmod\n",
      "mengantuk acl\n",
      "ketika mark\n",
      "memandu advcl\n"
     ]
    }
   ],
   "source": [
    "processed = process_string(string)\n",
    "result = crf.predict_single([features(processed, index) for index in range(len(processed))])\n",
    "for no, i in enumerate(result):\n",
    "    print(processed[no],i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_d = crf_depend.predict_single([features_crf_dependency(processed, result, index) for index in range(len(processed))])\n",
    "result_d = [int(i) for i in result_d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(result)):\n",
    "    if result_d[i] == 0 and result[i] != 'root':\n",
    "        result[i] = 'UNK'\n",
    "    if result_d[i] != 0 and result[i] == 'root':\n",
    "        result[i] = 'UNK'\n",
    "    if result_d[i] > len(result):\n",
    "        result_d[i] = len(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dr 2 det\n",
      "Mahathir 3 nsubj\n",
      "menasihati 0 root\n",
      "mereka 4 obj\n",
      "supaya 9 mark\n",
      "berhenti 9 advcl\n",
      "berehat 9 xcomp\n",
      "dan 9 cc\n",
      "tidur 7 conj\n",
      "sebentar 7 case\n",
      "sekiranya 7 nmod\n",
      "mengantuk 1 acl\n",
      "ketika 3 mark\n",
      "memandu 3 advcl\n"
     ]
    }
   ],
   "source": [
    "for no, i in enumerate(result):\n",
    "    print(processed[no],result_d[no], i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('crf-depend.pkl','wb') as fopen:\n",
    "    pickle.dump(crf_depend,fopen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
