{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import collections\n",
    "from sklearn import metrics\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from unidecode import unidecode\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules_normalizer = {\n",
    "    'experience': 'pengalaman',\n",
    "    'bagasi': 'bagasi',\n",
    "    'kg': 'kampung',\n",
    "    'kilo': 'kilogram',\n",
    "    'g': 'gram',\n",
    "    'grm': 'gram',\n",
    "    'k': 'okay',\n",
    "    'abgkat': 'abang dekat',\n",
    "    'abis': 'habis',\n",
    "    'ade': 'ada',\n",
    "    'adoi': 'aduh',\n",
    "    'adoii': 'aduhh',\n",
    "    'aerodarat': 'kapal darat',\n",
    "    'agkt': 'angkat',\n",
    "    'ahh': 'ah',\n",
    "    'ailior': 'air liur',\n",
    "    'airasia': 'air asia x',\n",
    "    'airasiax': 'penerbangan',\n",
    "    'airline': 'penerbangan',\n",
    "    'airlines': 'penerbangan',\n",
    "    'airport': 'lapangan terbang',\n",
    "    'airpot': 'lapangan terbang',\n",
    "    'aje': 'sahaja',\n",
    "    'ajelah': 'sahajalah',\n",
    "    'ajer': 'sahaja',\n",
    "    'ak': 'aku',\n",
    "    'aq': 'aku',\n",
    "    'all': 'semua',\n",
    "    'ambik': 'ambil',\n",
    "    'amek': 'ambil',\n",
    "    'amer': 'amir',\n",
    "    'amik': 'ambil',\n",
    "    'ana': 'saya',\n",
    "    'angkt': 'angkat',\n",
    "    'anual': 'tahunan',\n",
    "    'apapun': 'apa pun',\n",
    "    'ape': 'apa',\n",
    "    'arab': 'arab',\n",
    "    'area': 'kawasan',\n",
    "    'aritu': 'hari itu',\n",
    "    'ask': 'tanya',\n",
    "    'astro': 'astro',\n",
    "    'at': 'pada',\n",
    "    'attitude': 'sikap',\n",
    "    'babi': 'khinzir',\n",
    "    'back': 'belakang',\n",
    "    'bag': 'beg',\n",
    "    'bang': 'abang',\n",
    "    'bangla': 'bangladesh',\n",
    "    'banyk': 'banyak',\n",
    "    'bard': 'pujangga',\n",
    "    'bargasi': 'bagasi',\n",
    "    'bawak': 'bawa',\n",
    "    'bawanges': 'bawang',\n",
    "    'be': 'jadi',\n",
    "    'behave': 'berkelakuan baik',\n",
    "    'belagak': 'berlagak',\n",
    "    'berdisiplin': 'berdisplin',\n",
    "    'berenti': 'berhenti',\n",
    "    'beskal': 'basikal',\n",
    "    'bff': 'rakan karib',\n",
    "    'bg': 'bagi',\n",
    "    'bgi': 'bagi',\n",
    "    'biase': 'biasa',\n",
    "    'big': 'besar',\n",
    "    'bike': 'basikal',\n",
    "    'bile': 'bila',\n",
    "    'binawe': 'binatang',\n",
    "    'bini': 'isteri',\n",
    "    'bkn': 'bukan',\n",
    "    'bla': 'bila',\n",
    "    'blom': 'belum',\n",
    "    'bnyak': 'banyak',\n",
    "    'body': 'tubuh',\n",
    "    'bole': 'boleh',\n",
    "    'boss': 'bos',\n",
    "    'bowling': 'boling',\n",
    "    'bpe': 'berapa',\n",
    "    'brand': 'jenama',\n",
    "    'brg': 'barang',\n",
    "    'briefing': 'taklimat',\n",
    "    'brng': 'barang',\n",
    "    'bro': 'abang',\n",
    "    'bru': 'baru',\n",
    "    'bruntung': 'beruntung',\n",
    "    'bsikal': 'basikal',\n",
    "    'btnggjwb': 'bertanggungjawab',\n",
    "    'btul': 'betul',\n",
    "    'buatlh': 'buatlah',\n",
    "    'buh': 'letak',\n",
    "    'buka': 'buka',\n",
    "    'but': 'tetapi',\n",
    "    'bwk': 'bawa',\n",
    "    'by': 'dengan',\n",
    "    'byr': 'bayar',\n",
    "    'bz': 'sibuk',\n",
    "    'camera': 'kamera',\n",
    "    'camni': 'macam ini',\n",
    "    'cane': 'macam mana',\n",
    "    'cant': 'tak boleh',\n",
    "    'carakerja': 'cara kerja',\n",
    "    'care': 'jaga',\n",
    "    'cargo': 'kargo',\n",
    "    'cctv': 'kamera litar tertutup',\n",
    "    'celako': 'celaka',\n",
    "    'cer': 'cerita',\n",
    "    'cheap': 'murah',\n",
    "    'check': 'semak',\n",
    "    'ciput': 'sedikit',\n",
    "    'cite': 'cerita',\n",
    "    'citer': 'cerita',\n",
    "    'ckit': 'sikit',\n",
    "    'ckp': 'cakap',\n",
    "    'class': 'kelas',\n",
    "    'cm': 'macam',\n",
    "    'cmni': 'macam ini',\n",
    "    'cmpak': 'campak',\n",
    "    'committed': 'komited',\n",
    "    'company': 'syarikat',\n",
    "    'complain': 'aduan',\n",
    "    'corn': 'jagung',\n",
    "    'couldnt': 'tak boleh',\n",
    "    'cr': 'cari',\n",
    "    'crew': 'krew',\n",
    "    'cube': 'cuba',\n",
    "    'cuma': 'cuma',\n",
    "    'curinyaa': 'curinya',\n",
    "    'cust': 'pelanggan',\n",
    "    'customer': 'pelanggan',\n",
    "    'd': 'di',\n",
    "    'da': 'dah',\n",
    "    'dn': 'dan',\n",
    "    'dahh': 'dah',\n",
    "    'damaged': 'rosak',\n",
    "    'dapek': 'dapat',\n",
    "    'day': 'hari',\n",
    "    'dazrin': 'dazrin',\n",
    "    'dbalingnya': 'dibalingnya',\n",
    "    'de': 'ada',\n",
    "    'deep': 'dalam',\n",
    "    'deliberately': 'sengaja',\n",
    "    'depa': 'mereka',\n",
    "    'dessa': 'desa',\n",
    "    'dgn': 'dengan',\n",
    "    'dh': 'dah',\n",
    "    'didunia': 'di dunia',\n",
    "    'diorang': 'mereka',\n",
    "    'diorng': 'mereka',\n",
    "    'direct': 'secara terus',\n",
    "    'diving': 'junam',\n",
    "    'dkt': 'dekat',\n",
    "    'dlempar': 'dilempar',\n",
    "    'dlm': 'dalam',\n",
    "    'dlt': 'padam',\n",
    "    'dlu': 'dulu',\n",
    "    'done': 'siap',\n",
    "    'dont': 'jangan',\n",
    "    'dorg': 'mereka',\n",
    "    'dpermudhkn': 'dipermudahkan',\n",
    "    'dpt': 'dapat',\n",
    "    'dr': 'dari',\n",
    "    'dri': 'dari',\n",
    "    'dsb': 'dan sebagainya',\n",
    "    'dy': 'dia',\n",
    "    'educate': 'mendidik',\n",
    "    'ensure': 'memastikan',\n",
    "    'everything': 'semua',\n",
    "    'ewahh': 'wah',\n",
    "    'expect': 'sangka',\n",
    "    'fb': 'facebook',\n",
    "    'fired': 'pecat',\n",
    "    'first': 'pertama',\n",
    "    'fkr': 'fikir',\n",
    "    'flight': 'kapal terbang',\n",
    "    'for': 'untuk',\n",
    "    'free': 'percuma',\n",
    "    'friend': 'kawan',\n",
    "    'fyi': 'untuk pengetahuan anda',\n",
    "    'gantila': 'gantilah',\n",
    "    'gantirugi': 'ganti rugi',\n",
    "    'gentlemen': 'lelaki budiman',\n",
    "    'gerenti': 'jaminan',\n",
    "    'gile': 'gila',\n",
    "    'gk': 'juga',\n",
    "    'gnti': 'ganti',\n",
    "    'go': 'pergi',\n",
    "    'gomen': 'kerajaan',\n",
    "    'goment': 'kerajaan',\n",
    "    'good': 'baik',\n",
    "    'ground': 'tanah',\n",
    "    'guarno': 'macam mana',\n",
    "    'hampa': 'mereka',\n",
    "    'hampeh': 'teruk',\n",
    "    'hanat': 'jahanam',\n",
    "    'handle': 'kawal',\n",
    "    'handling': 'kawalan',\n",
    "    'hanta': 'hantar',\n",
    "    'haritu': 'hari itu',\n",
    "    'hate': 'benci',\n",
    "    'have': 'ada',\n",
    "    'hawau': 'celaka',\n",
    "    'henpon': 'telefon',\n",
    "    'heran': 'hairan',\n",
    "    'him': 'dia',\n",
    "    'his': 'dia',\n",
    "    'hmpa': 'mereka',\n",
    "    'hntr': 'hantar',\n",
    "    'hotak': 'otak',\n",
    "    'hr': 'hari',\n",
    "    'i': 'saya',\n",
    "    'hrga': 'harga',\n",
    "    'hrp': 'harap',\n",
    "    'hu': 'sedih',\n",
    "    'humble': 'merendah diri',\n",
    "    'ibon': 'ikon',\n",
    "    'ichi': 'inci',\n",
    "    'idung': 'hidung',\n",
    "    'if': 'jika',\n",
    "    'ig': 'instagram',\n",
    "    'iklas': 'ikhlas',\n",
    "    'improve': 'menambah baik',\n",
    "    'in': 'masuk',\n",
    "    'isn t': 'tidak',\n",
    "    'isyaallah': 'insyallah',\n",
    "    'ja': 'sahaja',\n",
    "    'japan': 'jepun',\n",
    "    'jd': 'jadi',\n",
    "    'je': 'saja',\n",
    "    'jee': 'saja',\n",
    "    'jek': 'saja',\n",
    "    'jepun': 'jepun',\n",
    "    'jer': 'saja',\n",
    "    'jerr': 'saja',\n",
    "    'jez': 'saja',\n",
    "    'jg': 'juga',\n",
    "    'jgk': 'juga',\n",
    "    'jgn': 'jangan',\n",
    "    'jgnla': 'janganlah',\n",
    "    'jibake': 'celaka',\n",
    "    'jjur': 'jujur',\n",
    "    'job': 'kerja',\n",
    "    'jobscope': 'skop kerja',\n",
    "    'jogja': 'jogjakarta',\n",
    "    'jpam': 'jpam',\n",
    "    'jth': 'jatuh',\n",
    "    'jugak': 'juga',\n",
    "    'ka': 'ke',\n",
    "    'kalo': 'kalau',\n",
    "    'kalu': 'kalau',\n",
    "    'kang': 'nanti',\n",
    "    'kantoi': 'temberang',\n",
    "    'kasi': 'beri',\n",
    "    'kat': 'dekat',\n",
    "    'kbye': 'ok bye',\n",
    "    'kearah': 'ke arah',\n",
    "    'kecik': 'kecil',\n",
    "    'keja': 'kerja',\n",
    "    'keje': 'kerja',\n",
    "    'kejo': 'kerja',\n",
    "    'keksongan': 'kekosongan',\n",
    "    'kemana': 'ke mana',\n",
    "    'kene': 'kena',\n",
    "    'kenekan': 'kenakan',\n",
    "    'kesah': 'kisah',\n",
    "    'ketempat': 'ke tempat',\n",
    "    'kije': 'kerja',\n",
    "    'kijo': 'kerja',\n",
    "    'kiss': 'cium',\n",
    "    'kite': 'kita',\n",
    "    'kito': 'kita',\n",
    "    'kje': 'kerja',\n",
    "    'kjr': 'kerja',\n",
    "    'kk': 'okay',\n",
    "    'kmi': 'kami',\n",
    "    'kt': 'kat',\n",
    "    'tlg': 'tolong',\n",
    "    'kl': 'kuala lumpur',\n",
    "    'klai': 'kalau',\n",
    "    'klau': 'kalau',\n",
    "    'klia': 'klia',\n",
    "    'klo': 'kalau',\n",
    "    'klu': 'kalau',\n",
    "    'kn': 'kan',\n",
    "    'knapa': 'kenapa',\n",
    "    'kne': 'kena',\n",
    "    'ko': 'kau',\n",
    "    'kompom': 'sah',\n",
    "    'korang': 'kamu semua',\n",
    "    'korea': 'korea',\n",
    "    'korg': 'kamu semua',\n",
    "    'kot': 'mungkin',\n",
    "    'krja': 'kerja',\n",
    "    'ksalahan': 'kesalahan',\n",
    "    'kta': 'kita',\n",
    "    'kuar': 'keluar',\n",
    "    'kut': 'mungkin',\n",
    "    'la': 'lah',\n",
    "    'laa': 'lah',\n",
    "    'lahabau': 'celaka',\n",
    "    'lahanat': 'celaka',\n",
    "    'lainda': 'lain dah',\n",
    "    'lak': 'pula',\n",
    "    'last': 'akhir',\n",
    "    'le': 'lah',\n",
    "    'leader': 'ketua',\n",
    "    'leave': 'pergi',\n",
    "    'ler': 'lah',\n",
    "    'less': 'kurang',\n",
    "    'letter': 'surat',\n",
    "    'lg': 'lagi',\n",
    "    'lgi': 'lagi',\n",
    "    'lngsong': 'langsung',\n",
    "    'lol': 'hehe',\n",
    "    'lorr': 'lah',\n",
    "    'low': 'rendah',\n",
    "    'lps': 'lepas',\n",
    "    'luggage': 'bagasi',\n",
    "    'lumbe': 'lumba',\n",
    "    'lyak': 'layak',\n",
    "    'maap': 'maaf',\n",
    "    'maapkan': 'maafkan',\n",
    "    'mahai': 'mahal',\n",
    "    'mampos': 'mampus',\n",
    "    'mart': 'kedai',\n",
    "    'mau': 'mahu',\n",
    "    'mcm': 'macam',\n",
    "    'mcmtu': 'macam itu',\n",
    "    'memerlukn': 'memerlukan',\n",
    "    'mengembirakan': 'menggembirakan',\n",
    "    'mengmbilnyer': 'mengambilnya',\n",
    "    'mengtasi': 'mengatasi',\n",
    "    'mg': 'memang',\n",
    "    'mihak': 'memihak',\n",
    "    'min': 'admin',\n",
    "    'mingu': 'minggu',\n",
    "    'mintak': 'minta',\n",
    "    'mjtuhkn': 'menjatuhkan',\n",
    "    'mkyong': 'mak yong',\n",
    "    'mlibatkn': 'melibatkan',\n",
    "    'mmg': 'memang',\n",
    "    'mmnjang': 'memanjang',\n",
    "    'mmpos': 'mampus',\n",
    "    'mn': 'mana',\n",
    "    'mna': 'mana',\n",
    "    'mntak': 'minta',\n",
    "    'mntk': 'minta',\n",
    "    'mnyusun': 'menyusun',\n",
    "    'mood': 'suasana',\n",
    "    'most': 'paling',\n",
    "    'mr': 'tuan',\n",
    "    'msa': 'masa',\n",
    "    'msia': 'malaysia',\n",
    "    'mst': 'mesti',\n",
    "    'mu': 'awak',\n",
    "    'much': 'banyak',\n",
    "    'muko': 'muka',\n",
    "    'mum': 'emak',\n",
    "    'n': 'dan',\n",
    "    'nah': 'nah',\n",
    "    'nanny': 'nenek',\n",
    "    'napo': 'kenapa',\n",
    "    'nati': 'nanti',\n",
    "    'ngan': 'dengan',\n",
    "    'ngn': 'dengan',\n",
    "    'ni': 'ini',\n",
    "    'nie': 'ini',\n",
    "    'nii': 'ini',\n",
    "    'nk': 'nak',\n",
    "    'nmpk': 'nampak',\n",
    "    'nye': 'nya',\n",
    "    'ofis': 'pejabat',\n",
    "    'ohh': 'oh',\n",
    "    'oii': 'hoi',\n",
    "    'one': 'satu',\n",
    "    'online': 'dalam talian',\n",
    "    'or': 'atau',\n",
    "    'org': 'orang',\n",
    "    'orng': 'orang',\n",
    "    'otek': 'otak',\n",
    "    'p': 'pergi',\n",
    "    'paid': 'dah bayar',\n",
    "    'palabana': 'kepala otak',\n",
    "    'pasni': 'lepas ini',\n",
    "    'passengers': 'penumpang',\n",
    "    'passengger': 'penumpang',\n",
    "    'pastu': 'lepas itu',\n",
    "    'pd': 'pada',\n",
    "    'pegi': 'pergi',\n",
    "    'pekerje': 'pekerja',\n",
    "    'pekrja': 'pekerja',\n",
    "    'perabih': 'perabis',\n",
    "    'perkerja': 'pekerja',\n",
    "    'pg': 'pergi',\n",
    "    'phuii': 'puih',\n",
    "    'pikir': 'fikir',\n",
    "    'pilot': 'juruterbang',\n",
    "    'pk': 'fikir',\n",
    "    'pkerja': 'pekerja',\n",
    "    'pkerjaan': 'pekerjaan',\n",
    "    'pki': 'pakai',\n",
    "    'please': 'tolong',\n",
    "    'pls': 'tolong',\n",
    "    'pn': 'pun',\n",
    "    'pnh': 'pernah',\n",
    "    'pnt': 'penat',\n",
    "    'pnya': 'punya',\n",
    "    'pon': 'pun',\n",
    "    'priority': 'keutamaan',\n",
    "    'properties': 'harta benda',\n",
    "    'ptugas': 'petugas',\n",
    "    'pub': 'kelab malam',\n",
    "    'pulak': 'pula',\n",
    "    'puye': 'punya',\n",
    "    'pwrcuma': 'percuma',\n",
    "    'pyahnya': 'payahnya',\n",
    "    'quality': 'kualiti',\n",
    "    'quit': 'keluar',\n",
    "    'ramly': 'ramly',\n",
    "    'rege': 'harga',\n",
    "    'reger': 'harga',\n",
    "    'report': 'laporan',\n",
    "    'resigned': 'meletakkan jawatan',\n",
    "    'respect': 'hormat',\n",
    "    'rizal': 'rizal',\n",
    "    'rosak': 'rosak',\n",
    "    'rosok': 'rosak',\n",
    "    'rse': 'rasa',\n",
    "    'sacked': 'buang',\n",
    "    'sado': 'tegap',\n",
    "    'salute': 'sanjung',\n",
    "    'sam': 'sama',\n",
    "    'same': 'sama',\n",
    "    'samp': 'sampah',\n",
    "    'sbb': 'sebab',\n",
    "    'sbgai': 'sebagai',\n",
    "    'sblm': 'sebelum',\n",
    "    'sblum': 'sebelum',\n",
    "    'sbnarnya': 'sebenarnya',\n",
    "    'sbum': 'sebelum',\n",
    "    'sdg': 'sedang',\n",
    "    'sebb': 'sebab',\n",
    "    'sebijik': 'sebiji',\n",
    "    'see': 'lihat',\n",
    "    'seen': 'dilihat',\n",
    "    'selangor': 'selangor',\n",
    "    'selfie': 'swafoto',\n",
    "    'sempoi': 'cantik',\n",
    "    'senaraihitam': 'senarai hitam',\n",
    "    'seorg': 'seorang',\n",
    "    'service': 'perkhidmatan',\n",
    "    'sgt': 'sangat',\n",
    "    'shared': 'kongsi',\n",
    "    'shirt': 'kemeja',\n",
    "    'shut': 'tutup',\n",
    "    'sib': 'nasib',\n",
    "    'skali': 'sekali',\n",
    "    'sket': 'sikit',\n",
    "    'sma': 'sama',\n",
    "    'smoga': 'semoga',\n",
    "    'smpoi': 'cantik',\n",
    "    'sndiri': 'sendiri',\n",
    "    'sndr': 'sendiri',\n",
    "    'sndri': 'sendiri',\n",
    "    'sne': 'sana',\n",
    "    'so': 'jadi',\n",
    "    'sop': 'tatacara pengendalian piawai',\n",
    "    'sorang': 'seorang',\n",
    "    'spoting': 'pembintikan',\n",
    "    'sronok': 'seronok',\n",
    "    'ssh': 'susah',\n",
    "    'staff': 'staf',\n",
    "    'standing': 'berdiri',\n",
    "    'start': 'mula',\n",
    "    'steady': 'mantap',\n",
    "    'stiap': 'setiap',\n",
    "    'stress': 'stres',\n",
    "    'student': 'pelajar',\n",
    "    'study': 'belajar',\n",
    "    'studycase': 'kajian kes',\n",
    "    'sure': 'pasti',\n",
    "    'sykt': 'syarikat',\n",
    "    'tah': 'entah',\n",
    "    'taik': 'tahi',\n",
    "    'takan': 'tak akan',\n",
    "    'takat': 'setakat',\n",
    "    'takde': 'tak ada',\n",
    "    'takkan': 'tak akan',\n",
    "    'taknak': 'tak nak',\n",
    "    'tang': 'tentang',\n",
    "    'tanggungjawab': 'bertanggungjawab',\n",
    "    'taraa': 'sementara',\n",
    "    'tau': 'tahu',\n",
    "    'tbabit': 'terbabit',\n",
    "    'team': 'pasukan',\n",
    "    'terbaekk': 'terbaik',\n",
    "    'teruknye': 'teruknya',\n",
    "    'tgk': 'tengok',\n",
    "    'that': 'itu',\n",
    "    'thinking': 'fikir',\n",
    "    'those': 'itu',\n",
    "    'time': 'masa',\n",
    "    'tk': 'tak',\n",
    "    'tnggongjwb': 'tanggungjawab',\n",
    "    'tngok': 'tengok',\n",
    "    'tngu': 'tunggu',\n",
    "    'to': 'kepada',\n",
    "    'tosak': 'rosak',\n",
    "    'tp': 'tapi',\n",
    "    'tpi': 'tapi',\n",
    "    'tpon': 'telefon',\n",
    "    'transfer': 'pindah',\n",
    "    'trgelak': 'tergelak',\n",
    "    'ts': 'tan sri',\n",
    "    'tstony': 'tan sri tony',\n",
    "    'tu': 'itu',\n",
    "    'tuh': 'itu',\n",
    "    'tula': 'itulah',\n",
    "    'umeno': 'umno',\n",
    "    'unfortunately': 'malangnya',\n",
    "    'unhappy': 'tidak gembira',\n",
    "    'up': 'naik',\n",
    "    'upkan': 'naikkan',\n",
    "    'ur': 'awak',\n",
    "    'utk': 'untuk',\n",
    "    'very': 'sangat',\n",
    "    'viral': 'tular',\n",
    "    'vote': 'undi',\n",
    "    'warning': 'amaran',\n",
    "    'warranty': 'waranti',\n",
    "    'wassap': 'whatsapp',\n",
    "    'wat': 'apa',\n",
    "    'weii': 'wei',\n",
    "    'well': 'maklumlah',\n",
    "    'win': 'menang',\n",
    "    'with': 'dengan',\n",
    "    'wt': 'buat',\n",
    "    'x': 'tak',\n",
    "    'tw': 'tahu',\n",
    "    'ye': 'ya',\n",
    "    'yee': 'ya',\n",
    "    'yg': 'yang',\n",
    "    'yng': 'yang',\n",
    "    'you': 'awak',\n",
    "    'your': 'awak',\n",
    "    'sakai': 'selekeh',\n",
    "    'rmb': 'billion ringgit',\n",
    "    'rmj': 'juta ringgit',\n",
    "    'rmk': 'ribu ringgit',\n",
    "    'rm': 'ringgit',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "permulaan = [\n",
    "    'bel',\n",
    "    'se',\n",
    "    'ter',\n",
    "    'men',\n",
    "    'meng',\n",
    "    'mem',\n",
    "    'memper',\n",
    "    'di',\n",
    "    'pe',\n",
    "    'me',\n",
    "    'ke',\n",
    "    'ber',\n",
    "    'pen',\n",
    "    'per',\n",
    "]\n",
    "\n",
    "hujung = ['kan', 'kah', 'lah', 'tah', 'nya', 'an', 'wan', 'wati', 'ita']\n",
    "\n",
    "def naive_stemmer(word):\n",
    "    assert isinstance(word, str), 'input must be a string'\n",
    "    hujung_result = [e for e in hujung if word.endswith(e)]\n",
    "    if len(hujung_result):\n",
    "        hujung_result = max(hujung_result, key = len)\n",
    "        if len(hujung_result):\n",
    "            word = word[: -len(hujung_result)]\n",
    "    permulaan_result = [e for e in permulaan if word.startswith(e)]\n",
    "    if len(permulaan_result):\n",
    "        permulaan_result = max(permulaan_result, key = len)\n",
    "        if len(permulaan_result):\n",
    "            word = word[len(permulaan_result) :]\n",
    "    return word\n",
    "\n",
    "def build_dataset(words, n_words):\n",
    "    count = [['GO', 0], ['PAD', 1], ['EOS', 2], ['UNK', 3]]\n",
    "    counter = collections.Counter(words).most_common(n_words)\n",
    "    count.extend(counter)\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    data = list()\n",
    "    unk_count = 0\n",
    "    for word in words:\n",
    "        index = dictionary.get(word, 3)\n",
    "        if index == 0:\n",
    "            unk_count += 1\n",
    "        data.append(index)\n",
    "    count[0][1] = unk_count\n",
    "    reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return data, count, dictionary, reversed_dictionary\n",
    "\n",
    "\n",
    "def classification_textcleaning(string):\n",
    "    string = re.sub(\n",
    "        'http\\S+|www.\\S+',\n",
    "        '',\n",
    "        ' '.join(\n",
    "            [i for i in string.split() if i.find('#') < 0 and i.find('@') < 0]\n",
    "        ),\n",
    "    )\n",
    "    string = unidecode(string).replace('.', ' . ').replace(',', ' , ')\n",
    "    string = re.sub('[^A-Za-z ]+', ' ', string)\n",
    "    string = re.sub(r'[ ]+', ' ', string.lower()).strip()\n",
    "    string = [rules_normalizer.get(w, w) for w in string.split()]\n",
    "    string = [naive_stemmer(word) for word in string]\n",
    "    return ' '.join([word for word in string if len(word) > 1])\n",
    "\n",
    "\n",
    "def str_idx(corpus, dic, maxlen, UNK = 3):\n",
    "    X = np.zeros((len(corpus), maxlen))\n",
    "    for i in range(len(corpus)):\n",
    "        for no, k in enumerate(corpus[i].split()[:maxlen][::-1]):\n",
    "            X[i, -1 - no] = dic.get(k, UNK)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'raja benar sangat benci rakyat minyak naik gala'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_textcleaning('kerajaan sebenarnya sangat bencikan rakyatnya, minyak naik dan segalanya')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('sentiment-data-v2.csv')\n",
    "Y = LabelEncoder().fit_transform(df.label)\n",
    "with open('polarity-negative-translated.txt','r') as fopen:\n",
    "    texts = fopen.read().split('\\n')\n",
    "labels = [0] * len(texts)\n",
    "\n",
    "with open('polarity-positive-translated.txt','r') as fopen:\n",
    "    positive_texts = fopen.read().split('\\n')\n",
    "labels += [1] * len(positive_texts)\n",
    "texts += positive_texts\n",
    "texts += df.iloc[:,1].tolist()\n",
    "labels += Y.tolist()\n",
    "\n",
    "assert len(labels) == len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('bm-amazon.json') as fopen:\n",
    "    amazon = json.load(fopen)\n",
    "    \n",
    "with open('bm-imdb.json') as fopen:\n",
    "    imdb = json.load(fopen)\n",
    "    \n",
    "with open('bm-yelp.json') as fopen:\n",
    "    yelp = json.load(fopen)\n",
    "    \n",
    "texts += amazon['negative']\n",
    "labels += [0] * len(amazon['negative'])\n",
    "texts += amazon['positive']\n",
    "labels += [1] * len(amazon['positive'])\n",
    "\n",
    "texts += imdb['negative']\n",
    "labels += [0] * len(imdb['negative'])\n",
    "texts += imdb['positive']\n",
    "labels += [1] * len(imdb['positive'])\n",
    "\n",
    "texts += yelp['negative']\n",
    "labels += [0] * len(yelp['negative'])\n",
    "texts += yelp['positive']\n",
    "labels += [1] * len(yelp['positive'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "for i in [i for i in os.listdir('negative') if 'Store' not in i]:\n",
    "    with open('negative/'+i) as fopen:\n",
    "        a = json.load(fopen)\n",
    "        texts += a\n",
    "        labels += [0] * len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "for i in [i for i in os.listdir('positive') if 'Store' not in i]:\n",
    "    with open('positive/'+i) as fopen:\n",
    "        a = json.load(fopen)\n",
    "        texts += a\n",
    "        labels += [1] * len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(675023, 675023)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts),len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(texts)):\n",
    "    texts[i] = classification_textcleaning(texts[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ada anda yiar tentang gatur tidur yang baru tidak lama lagi yusun mula ada yang baik ntiasa asa begitu jaya lepas itu',\n",
       " 'lamat pagi dunia',\n",
       " 'sini rumah pupu saya',\n",
       " 'mungkin ini lebih anda',\n",
       " 'ima kasih saya erlukan',\n",
       " 'neraka windows luar dari julat harga saya cuali jika tidak',\n",
       " 'neah saya harap ha enang mbali catat dalam tweet akhir saya',\n",
       " 'aww saya benar benar minta maaf tentang itu tidak ada apa apa yang sa deng jadi asi tidak anda lihat quot quot malam kido',\n",
       " 'saya tidak sabar untuk lihat apa kara yang akjub yang anda datang tidak nah lupa anda luar biasa',\n",
       " 'deng cara saya embali dewi matahari malam tadi anda sasha sangat agum saya boleh onton filem itu ulang kali']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab from size: 120097\n",
      "Most common words [('saya', 533028), ('yang', 204446), ('tidak', 164296), ('untuk', 129707), ('anda', 126091), ('hari', 88975)]\n",
      "Sample data [2667, 229, 363, 235, 235, 94, 1357, 5, 78, 678] ['ringkas', 'bodoh', 'bosan', 'kanak', 'kanak', 'lelaki', 'remaja', 'yang', 'begitu', 'muda']\n"
     ]
    }
   ],
   "source": [
    "concat = ' '.join(texts).split()\n",
    "vocabulary_size = len(list(set(concat)))\n",
    "data, count, dictionary, rev_dictionary = build_dataset(concat, vocabulary_size)\n",
    "print('vocab from size: %d'%(vocabulary_size))\n",
    "print('Most common words', count[4:10])\n",
    "print('Sample data', data[:10], [rev_dictionary[i] for i in data[:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Model:\n",
    "    def __init__(\n",
    "        self,\n",
    "        size_layer,\n",
    "        num_layers,\n",
    "        dimension_output,\n",
    "        learning_rate,\n",
    "        dropout,\n",
    "        dict_size,\n",
    "    ):\n",
    "        def cells(size, reuse = False):\n",
    "            return tf.contrib.rnn.DropoutWrapper(\n",
    "                tf.nn.rnn_cell.LSTMCell(\n",
    "                    size,\n",
    "                    initializer = tf.orthogonal_initializer(),\n",
    "                    reuse = reuse,\n",
    "                ),\n",
    "                state_keep_prob = dropout,\n",
    "                output_keep_prob = dropout,\n",
    "            )\n",
    "\n",
    "        self.X = tf.placeholder(tf.int32, [None, None])\n",
    "        self.Y = tf.placeholder(tf.int32, [None])\n",
    "        encoder_embeddings = tf.Variable(\n",
    "            tf.random_uniform([dict_size, size_layer], -1, 1)\n",
    "        )\n",
    "        encoder_embedded = tf.nn.embedding_lookup(encoder_embeddings, self.X)\n",
    "        attention_mechanism = tf.contrib.seq2seq.LuongAttention(\n",
    "            num_units = size_layer, memory = encoder_embedded\n",
    "        )\n",
    "        rnn_cells = tf.contrib.seq2seq.AttentionWrapper(\n",
    "            cell = tf.nn.rnn_cell.MultiRNNCell(\n",
    "                [cells(size_layer) for _ in range(num_layers)]\n",
    "            ),\n",
    "            attention_mechanism = attention_mechanism,\n",
    "            attention_layer_size = size_layer,\n",
    "            alignment_history = True,\n",
    "        )\n",
    "        outputs, last_state = tf.nn.dynamic_rnn(\n",
    "            rnn_cells, encoder_embedded, dtype = tf.float32\n",
    "        )\n",
    "        self.alignments = tf.transpose(\n",
    "            last_state.alignment_history.stack(), [1, 2, 0]\n",
    "        )\n",
    "        W = tf.get_variable(\n",
    "            'w',\n",
    "            shape = (size_layer, dimension_output),\n",
    "            initializer = tf.glorot_uniform_initializer(),\n",
    "        )\n",
    "        b = tf.get_variable(\n",
    "            'b',\n",
    "            shape = (dimension_output),\n",
    "            initializer = tf.zeros_initializer(),\n",
    "        )\n",
    "        self.logits = tf.add(tf.matmul(outputs[:, -1], W), b, name = 'logits')\n",
    "        self.cost = tf.reduce_mean(\n",
    "            tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                logits = self.logits, labels = self.Y\n",
    "            )\n",
    "        )\n",
    "        self.optimizer = tf.train.AdamOptimizer(\n",
    "            learning_rate = learning_rate\n",
    "        ).minimize(self.cost)\n",
    "        correct_pred = tf.equal(\n",
    "            tf.argmax(self.logits, 1, output_type = tf.int32), self.Y\n",
    "        )\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "        self.attention = tf.nn.softmax(\n",
    "            tf.reduce_sum(self.alignments[0], 1), name = 'alphas'\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'luong/model.ckpt'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size_layer = 256\n",
    "num_layers = 2\n",
    "dimension_output = 2\n",
    "learning_rate = 1e-4\n",
    "batch_size = 32\n",
    "dropout = 0.8\n",
    "maxlen = 100\n",
    "\n",
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "model = Model(\n",
    "    size_layer,\n",
    "    num_layers,\n",
    "    dimension_output,\n",
    "    learning_rate,\n",
    "    dropout,\n",
    "    len(dictionary),\n",
    ")\n",
    "sess.run(tf.global_variables_initializer())\n",
    "saver = tf.train.Saver(tf.trainable_variables())\n",
    "saver.save(sess, 'luong/model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "strings = ','.join(\n",
    "    [\n",
    "        n.name\n",
    "        for n in tf.get_default_graph().as_graph_def().node\n",
    "        if ('Variable' in n.op\n",
    "        or 'Placeholder' in n.name\n",
    "        or 'logits' in n.name\n",
    "        or 'alphas' in n.name)\n",
    "        and 'Adam' not in n.name\n",
    "        and 'beta' not in n.name\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Placeholder',\n",
       " 'Placeholder_1',\n",
       " 'Variable',\n",
       " 'memory_layer/kernel',\n",
       " 'rnn/attention_wrapper/multi_rnn_cell/cell_0/lstm_cell/kernel',\n",
       " 'rnn/attention_wrapper/multi_rnn_cell/cell_0/lstm_cell/bias',\n",
       " 'rnn/attention_wrapper/multi_rnn_cell/cell_1/lstm_cell/kernel',\n",
       " 'rnn/attention_wrapper/multi_rnn_cell/cell_1/lstm_cell/bias',\n",
       " 'rnn/attention_wrapper/attention_layer/kernel',\n",
       " 'w',\n",
       " 'b',\n",
       " 'logits',\n",
       " 'gradients/logits_grad/Shape',\n",
       " 'gradients/logits_grad/Shape_1',\n",
       " 'gradients/logits_grad/BroadcastGradientArgs',\n",
       " 'gradients/logits_grad/Sum',\n",
       " 'gradients/logits_grad/Reshape',\n",
       " 'gradients/logits_grad/Sum_1',\n",
       " 'gradients/logits_grad/Reshape_1',\n",
       " 'gradients/logits_grad/tuple/group_deps',\n",
       " 'gradients/logits_grad/tuple/control_dependency',\n",
       " 'gradients/logits_grad/tuple/control_dependency_1',\n",
       " 'alphas']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strings.split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'Variable:0' shape=(120101, 256) dtype=float32_ref>,\n",
       " <tf.Variable 'memory_layer/kernel:0' shape=(256, 256) dtype=float32_ref>,\n",
       " <tf.Variable 'rnn/attention_wrapper/multi_rnn_cell/cell_0/lstm_cell/kernel:0' shape=(768, 1024) dtype=float32_ref>,\n",
       " <tf.Variable 'rnn/attention_wrapper/multi_rnn_cell/cell_0/lstm_cell/bias:0' shape=(1024,) dtype=float32_ref>,\n",
       " <tf.Variable 'rnn/attention_wrapper/multi_rnn_cell/cell_1/lstm_cell/kernel:0' shape=(512, 1024) dtype=float32_ref>,\n",
       " <tf.Variable 'rnn/attention_wrapper/multi_rnn_cell/cell_1/lstm_cell/bias:0' shape=(1024,) dtype=float32_ref>,\n",
       " <tf.Variable 'rnn/attention_wrapper/attention_layer/kernel:0' shape=(512, 256) dtype=float32_ref>,\n",
       " <tf.Variable 'w:0' shape=(256, 2) dtype=float32_ref>,\n",
       " <tf.Variable 'b:0' shape=(2,) dtype=float32_ref>]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.trainable_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, test_X, train_Y, test_Y = train_test_split(\n",
    "    texts, labels, test_size = 0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 16876/16876 [1:03:59<00:00,  4.52it/s, accuracy=0.833, cost=0.398]\n",
      "test minibatch loop: 100%|██████████| 4219/4219 [04:36<00:00, 15.25it/s, accuracy=0.828, cost=0.34] \n",
      "train minibatch loop:   0%|          | 0/16876 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, pass acc: 0.000000, current acc: 0.776427\n",
      "time taken: 4115.763730049133\n",
      "epoch: 0, training loss: 0.503544, training acc: 0.750665, valid loss: 0.467220, valid acc: 0.776427\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 16876/16876 [1:03:52<00:00,  4.54it/s, accuracy=0.889, cost=0.385]\n",
      "test minibatch loop: 100%|██████████| 4219/4219 [04:36<00:00, 15.26it/s, accuracy=0.897, cost=0.332]\n",
      "train minibatch loop:   0%|          | 0/16876 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, pass acc: 0.776427, current acc: 0.787850\n",
      "time taken: 4108.507958173752\n",
      "epoch: 1, training loss: 0.445242, training acc: 0.790726, valid loss: 0.448633, valid acc: 0.787850\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 16876/16876 [1:03:51<00:00,  4.53it/s, accuracy=0.944, cost=0.319]\n",
      "test minibatch loop: 100%|██████████| 4219/4219 [04:36<00:00, 15.25it/s, accuracy=0.862, cost=0.339]\n",
      "train minibatch loop:   0%|          | 0/16876 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2, pass acc: 0.787850, current acc: 0.790782\n",
      "time taken: 4108.066122055054\n",
      "epoch: 2, training loss: 0.415297, training acc: 0.808411, valid loss: 0.445776, valid acc: 0.790782\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 16876/16876 [1:03:55<00:00,  4.50it/s, accuracy=0.944, cost=0.265]\n",
      "test minibatch loop: 100%|██████████| 4219/4219 [04:36<00:00, 15.24it/s, accuracy=0.828, cost=0.307]\n",
      "train minibatch loop:   0%|          | 0/16876 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3, pass acc: 0.790782, current acc: 0.790878\n",
      "time taken: 4111.821011066437\n",
      "epoch: 3, training loss: 0.390190, training acc: 0.823430, valid loss: 0.450071, valid acc: 0.790878\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 16876/16876 [1:03:54<00:00,  4.55it/s, accuracy=0.889, cost=0.23] \n",
      "test minibatch loop: 100%|██████████| 4219/4219 [04:36<00:00, 15.24it/s, accuracy=0.793, cost=0.302]\n",
      "train minibatch loop:   0%|          | 0/16876 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken: 4111.363406419754\n",
      "epoch: 4, training loss: 0.364910, training acc: 0.838021, valid loss: 0.463166, valid acc: 0.787751\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 16876/16876 [1:03:54<00:00,  4.50it/s, accuracy=0.944, cost=0.226]\n",
      "test minibatch loop: 100%|██████████| 4219/4219 [04:36<00:00, 15.25it/s, accuracy=0.724, cost=0.328]\n",
      "train minibatch loop:   0%|          | 0/16876 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken: 4111.266193389893\n",
      "epoch: 5, training loss: 0.339397, training acc: 0.851455, valid loss: 0.484763, valid acc: 0.785091\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 16876/16876 [1:03:54<00:00,  4.54it/s, accuracy=0.889, cost=0.232] \n",
      "test minibatch loop: 100%|██████████| 4219/4219 [04:36<00:00, 15.25it/s, accuracy=0.862, cost=0.315]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken: 4111.205755233765\n",
      "epoch: 6, training loss: 0.314990, training acc: 0.864811, valid loss: 0.512742, valid acc: 0.781435\n",
      "\n",
      "break epoch:7\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "EARLY_STOPPING, CURRENT_CHECKPOINT, CURRENT_ACC, EPOCH = 3, 0, 0, 0\n",
    "\n",
    "while True:\n",
    "    lasttime = time.time()\n",
    "    if CURRENT_CHECKPOINT == EARLY_STOPPING:\n",
    "        print('break epoch:%d\\n' % (EPOCH))\n",
    "        break\n",
    "\n",
    "    train_acc, train_loss, test_acc, test_loss = 0, 0, 0, 0\n",
    "    pbar = tqdm(\n",
    "        range(0, len(train_X), batch_size), desc = 'train minibatch loop'\n",
    "    )\n",
    "    for i in pbar:\n",
    "        batch_x = str_idx(train_X[i : min(i + batch_size, len(train_X))], dictionary, maxlen)\n",
    "        batch_y = train_Y[i : min(i + batch_size, len(train_X))]\n",
    "        batch_x_expand = np.expand_dims(batch_x,axis = 1)\n",
    "        acc, cost, _ = sess.run(\n",
    "            [model.accuracy, model.cost, model.optimizer],\n",
    "            feed_dict = {\n",
    "                model.Y: batch_y,\n",
    "                model.X: batch_x\n",
    "            },\n",
    "        )\n",
    "        assert not np.isnan(cost)\n",
    "        train_loss += cost\n",
    "        train_acc += acc\n",
    "        pbar.set_postfix(cost = cost, accuracy = acc)\n",
    "\n",
    "    pbar = tqdm(range(0, len(test_X), batch_size), desc = 'test minibatch loop')\n",
    "    for i in pbar:\n",
    "        batch_x = str_idx(test_X[i : min(i + batch_size, len(test_X))], dictionary, maxlen)\n",
    "        batch_y = test_Y[i : min(i + batch_size, len(test_X))]\n",
    "        batch_x_expand = np.expand_dims(batch_x,axis = 1)\n",
    "        acc, cost = sess.run(\n",
    "            [model.accuracy, model.cost],\n",
    "            feed_dict = {\n",
    "                model.Y: batch_y,\n",
    "                model.X: batch_x\n",
    "            },\n",
    "        )\n",
    "        test_loss += cost\n",
    "        test_acc += acc\n",
    "        pbar.set_postfix(cost = cost, accuracy = acc)\n",
    "\n",
    "    train_loss /= len(train_X) / batch_size\n",
    "    train_acc /= len(train_X) / batch_size\n",
    "    test_loss /= len(test_X) / batch_size\n",
    "    test_acc /= len(test_X) / batch_size\n",
    "\n",
    "    if test_acc > CURRENT_ACC:\n",
    "        print(\n",
    "            'epoch: %d, pass acc: %f, current acc: %f'\n",
    "            % (EPOCH, CURRENT_ACC, test_acc)\n",
    "        )\n",
    "        CURRENT_ACC = test_acc\n",
    "        CURRENT_CHECKPOINT = 0\n",
    "    else:\n",
    "        CURRENT_CHECKPOINT += 1\n",
    "        \n",
    "    print('time taken:', time.time() - lasttime)\n",
    "    print(\n",
    "        'epoch: %d, training loss: %f, training acc: %f, valid loss: %f, valid acc: %f\\n'\n",
    "        % (EPOCH, train_loss, train_acc, test_loss, test_acc)\n",
    "    )\n",
    "    EPOCH += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'luong/model.ckpt'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saver.save(sess, 'luong/model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.73605937, 0.26394066]], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = classification_textcleaning('kerajaan sebenarnya sangat bencikan rakyatnya, minyak naik dan segalanya')\n",
    "new_vector = str_idx([text[0]], dictionary, len(text[0].split()))\n",
    "sess.run(tf.nn.softmax(model.logits), feed_dict={model.X:new_vector})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "validation minibatch loop: 100%|██████████| 4219/4219 [04:32<00:00, 15.50it/s]\n"
     ]
    }
   ],
   "source": [
    "real_Y, predict_Y = [], []\n",
    "\n",
    "pbar = tqdm(\n",
    "    range(0, len(test_X), batch_size), desc = 'validation minibatch loop'\n",
    ")\n",
    "for i in pbar:\n",
    "    batch_x = str_idx(test_X[i : min(i + batch_size, len(test_X))], dictionary, maxlen)\n",
    "    batch_y = test_Y[i : min(i + batch_size, len(test_X))]\n",
    "    predict_Y += np.argmax(\n",
    "        sess.run(\n",
    "            model.logits, feed_dict = {model.X: batch_x, model.Y: batch_y}\n",
    "        ),\n",
    "        1,\n",
    "    ).tolist()\n",
    "    real_Y += batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "   negative       0.79      0.79      0.79     70613\n",
      "   positive       0.77      0.77      0.77     64392\n",
      "\n",
      "avg / total       0.78      0.78      0.78    135005\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    metrics.classification_report(\n",
    "        real_Y, predict_Y, target_names = ['negative', 'positive']\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('luong-sentiment.json','w') as fopen:\n",
    "    fopen.write(json.dumps({'dictionary':dictionary,'reverse_dictionary':rev_dictionary}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_graph(model_dir, output_node_names):\n",
    "\n",
    "    if not tf.gfile.Exists(model_dir):\n",
    "        raise AssertionError(\n",
    "            \"Export directory doesn't exists. Please specify an export \"\n",
    "            'directory: %s' % model_dir\n",
    "        )\n",
    "\n",
    "    checkpoint = tf.train.get_checkpoint_state(model_dir)\n",
    "    input_checkpoint = checkpoint.model_checkpoint_path\n",
    "\n",
    "    absolute_model_dir = '/'.join(input_checkpoint.split('/')[:-1])\n",
    "    output_graph = absolute_model_dir + '/frozen_model.pb'\n",
    "    clear_devices = True\n",
    "    with tf.Session(graph = tf.Graph()) as sess:\n",
    "        saver = tf.train.import_meta_graph(\n",
    "            input_checkpoint + '.meta', clear_devices = clear_devices\n",
    "        )\n",
    "        saver.restore(sess, input_checkpoint)\n",
    "        output_graph_def = tf.graph_util.convert_variables_to_constants(\n",
    "            sess,\n",
    "            tf.get_default_graph().as_graph_def(),\n",
    "            output_node_names.split(','),\n",
    "        )\n",
    "        with tf.gfile.GFile(output_graph, 'wb') as f:\n",
    "            f.write(output_graph_def.SerializeToString())\n",
    "        print('%d ops in the final graph.' % len(output_graph_def.node))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from luong/model.ckpt\n",
      "INFO:tensorflow:Froze 9 variables.\n",
      "INFO:tensorflow:Converted 9 variables to const ops.\n",
      "393 ops in the final graph.\n"
     ]
    }
   ],
   "source": [
    "freeze_graph('luong', strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_graph(frozen_graph_filename):\n",
    "    with tf.gfile.GFile(frozen_graph_filename, 'rb') as f:\n",
    "        graph_def = tf.GraphDef()\n",
    "        graph_def.ParseFromString(f.read())\n",
    "    with tf.Graph().as_default() as graph:\n",
    "        tf.import_graph_def(graph_def)\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    }
   ],
   "source": [
    "g = load_graph('luong/frozen_model.pb')\n",
    "x = g.get_tensor_by_name('import/Placeholder:0')\n",
    "logits = g.get_tensor_by_name('import/logits:0')\n",
    "alphas = g.get_tensor_by_name('import/alphas:0')\n",
    "test_sess = tf.InteractiveSession(graph = g)\n",
    "result = test_sess.run([logits, alphas], feed_dict = {x: new_vector})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_string = 'kerajaan sebenarnya sangat bencikan rakyatnya, minyak naik dan segalanya'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = classification_textcleaning(news_string)\n",
    "new_vector = str_idx([text], dictionary, len(text.split()))\n",
    "result = test_sess.run([tf.nn.softmax(logits), alphas], feed_dict = {x: new_vector})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize = (15, 7))\n",
    "labels = [word for word in text.split()]\n",
    "val = [val for val in result[1]]\n",
    "plt.bar(np.arange(len(labels)), val)\n",
    "plt.xticks(np.arange(len(labels)), labels, rotation = 'vertical')\n",
    "plt.title('negative %f positive %f' % (result[0][0,0], result[0][0,1]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
