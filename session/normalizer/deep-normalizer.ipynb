{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.utils import shuffle\n",
    "import re\n",
    "import time\n",
    "import collections\n",
    "import os\n",
    "import itertools\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(words, n_words, atleast=1):\n",
    "    count = [['GO', 0], ['PAD', 1], ['EOS', 2], ['UNK', 3]]\n",
    "    counter = collections.Counter(words).most_common(n_words)\n",
    "    counter = [i for i in counter if i[1] >= atleast]\n",
    "    count.extend(counter)\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    data = list()\n",
    "    unk_count = 0\n",
    "    for word in words:\n",
    "        index = dictionary.get(word, 0)\n",
    "        if index == 0:\n",
    "            unk_count += 1\n",
    "        data.append(index)\n",
    "    count[0][1] = unk_count\n",
    "    reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return data, count, dictionary, reversed_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len before 206226\n",
      "len after 114110\n"
     ]
    }
   ],
   "source": [
    "with open('normalizer-data.txt','r') as fopen:\n",
    "    texts = fopen.read().split('\\n')\n",
    "\n",
    "print('len before %d'%(len(texts)))\n",
    "before, after, ins = [], [], []\n",
    "    \n",
    "for i in texts:\n",
    "    splitted = i.split('\\t')\n",
    "    if (len(splitted) < 1) or (len(splitted[0])) == 0 or (len(splitted[1]) > len(splitted[0])*3):\n",
    "        continue\n",
    "    if splitted[0].lower() in ins:\n",
    "        continue\n",
    "    ins.append(splitted[0].lower())\n",
    "    before.append(list(splitted[0].lower()))\n",
    "    after.append(list(splitted[1].lower()))\n",
    "    \n",
    "assert len(before) == len(after)\n",
    "print('len after %d'%(len(before)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab from size: 50\n",
      "Most common words [('n', 97787), ('a', 87288), ('r', 57924), ('e', 51506), ('i', 47830), ('s', 47641)]\n",
      "Sample data [22, 7, 6, 4, 5, 19, 12, 9, 5, 13] ['c', 'e', 'r', 'n', 'a', 'h', 'k', 's', 'a', 'l']\n",
      "filtered vocab size: 54\n",
      "% of vocab used: 108.0%\n"
     ]
    }
   ],
   "source": [
    "concat_from = list(itertools.chain(*before))\n",
    "vocabulary_size_from = len(list(set(concat_from)))\n",
    "data_from, count_from, dictionary_from, rev_dictionary_from = build_dataset(concat_from, vocabulary_size_from)\n",
    "print('vocab from size: %d'%(vocabulary_size_from))\n",
    "print('Most common words', count_from[4:10])\n",
    "print('Sample data', data_from[:10], [rev_dictionary_from[i] for i in data_from[:10]])\n",
    "print('filtered vocab size:',len(dictionary_from))\n",
    "print(\"% of vocab used: {}%\".format(round(len(dictionary_from)/vocabulary_size_from,4)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab from size: 35\n",
      "Most common words [('a', 153205), ('n', 98360), ('e', 90799), ('i', 82118), ('r', 58199), ('s', 47922)]\n",
      "Sample data [22, 6, 8, 7, 10, 4, 5, 4, 20, 12] ['c', 'e', 'r', 'i', 't', 'a', 'n', 'a', 'h', 'k']\n",
      "filtered vocab size: 39\n",
      "% of vocab used: 111.43%\n"
     ]
    }
   ],
   "source": [
    "concat_to = list(itertools.chain(*after))\n",
    "vocabulary_size_to = len(list(set(concat_to)))\n",
    "data_to, count_to, dictionary_to, rev_dictionary_to = build_dataset(concat_to, vocabulary_size_to)\n",
    "print('vocab from size: %d'%(vocabulary_size_to))\n",
    "print('Most common words', count_to[4:10])\n",
    "print('Sample data', data_to[:10], [rev_dictionary_to[i] for i in data_to[:10]])\n",
    "print('filtered vocab size:',len(dictionary_to))\n",
    "print(\"% of vocab used: {}%\".format(round(len(dictionary_to)/vocabulary_size_to,4)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "GO = dictionary_from['GO']\n",
    "PAD = dictionary_from['PAD']\n",
    "EOS = dictionary_from['EOS']\n",
    "UNK = dictionary_from['UNK']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(after)):\n",
    "    after[i].append('EOS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stemmer:\n",
    "    def __init__(self, size_layer, num_layers, embedded_size, \n",
    "                 from_dict_size, to_dict_size, learning_rate, \n",
    "                 dropout = 0.8, beam_width = 15):\n",
    "        \n",
    "        def lstm_cell(size,reuse=False):\n",
    "            return tf.nn.rnn_cell.LSTMCell(size, reuse=reuse)\n",
    "        \n",
    "        self.X = tf.placeholder(tf.int32, [None, None])\n",
    "        self.X_seq_len = tf.count_nonzero(self.X, 1, dtype=tf.int32)\n",
    "        self.Y = tf.placeholder(tf.int32, [None, None])\n",
    "        self.Y_seq_len = tf.count_nonzero(self.Y, 1, dtype=tf.int32)\n",
    "        batch_size = tf.shape(self.X)[0]\n",
    "\n",
    "        encoder_embeddings = tf.Variable(tf.random_uniform([from_dict_size, embedded_size], -1, 1))\n",
    "        encoder_embedded = tf.nn.embedding_lookup(encoder_embeddings, self.X)\n",
    "        \n",
    "        for n in range(num_layers):\n",
    "            (out_fw, out_bw), (state_fw, state_bw) = tf.nn.bidirectional_dynamic_rnn(\n",
    "                cell_fw = lstm_cell(size_layer // 2),\n",
    "                cell_bw = lstm_cell(size_layer // 2),\n",
    "                inputs = encoder_embedded,\n",
    "                sequence_length = self.X_seq_len,\n",
    "                dtype = tf.float32,\n",
    "                scope = 'bidirectional_rnn_%d'%(n))\n",
    "            encoder_embedded = tf.concat((out_fw, out_bw), 2)\n",
    "        \n",
    "        bi_state_c = tf.concat((state_fw.c, state_bw.c), -1)\n",
    "        bi_state_h = tf.concat((state_fw.h, state_bw.h), -1)\n",
    "        bi_lstm_state = tf.nn.rnn_cell.LSTMStateTuple(c=bi_state_c, h=bi_state_h)\n",
    "        self.encoder_state = tuple([bi_lstm_state] * num_layers)\n",
    "        self.encoder_state = tuple(self.encoder_state[-1] for _ in range(num_layers))\n",
    "            \n",
    "        main = tf.strided_slice(self.Y, [0, 0], [batch_size, -1], [1, 1])\n",
    "        decoder_input = tf.concat([tf.fill([batch_size, 1], GO), main], 1)\n",
    "        decoder_embeddings = tf.Variable(tf.random_uniform([to_dict_size, embedded_size], -1, 1))\n",
    "        dense_layer = tf.layers.Dense(to_dict_size)\n",
    "        \n",
    "        decoder_cells = tf.nn.rnn_cell.MultiRNNCell([lstm_cell(size_layer) for _ in range(num_layers)])\n",
    "        \n",
    "        # training session\n",
    "        with tf.variable_scope('decode'):\n",
    "            training_helper = tf.contrib.seq2seq.ScheduledEmbeddingTrainingHelper(\n",
    "                    inputs = tf.nn.embedding_lookup(decoder_embeddings, decoder_input),\n",
    "                    sequence_length = self.Y_seq_len,\n",
    "                    embedding = decoder_embeddings,\n",
    "                    sampling_probability = 0.2,\n",
    "                    time_major = False)\n",
    "            training_decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "                    cell = decoder_cells,\n",
    "                    helper = training_helper,\n",
    "                    initial_state = self.encoder_state,\n",
    "                    output_layer = dense_layer)\n",
    "            training_decoder_output, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "                    decoder = training_decoder,\n",
    "                    impute_finished = True,\n",
    "                    maximum_iterations = tf.reduce_max(self.Y_seq_len))\n",
    "            \n",
    "        # testing session\n",
    "        with tf.variable_scope('decode', reuse=True):\n",
    "            \n",
    "            predicting_decoder = tf.contrib.seq2seq.BeamSearchDecoder(\n",
    "                    cell = decoder_cells,\n",
    "                    embedding = decoder_embeddings,\n",
    "                    start_tokens = tf.tile(tf.constant([GO], dtype=tf.int32), [batch_size]),\n",
    "                    end_token = EOS,\n",
    "                    initial_state = tf.contrib.seq2seq.tile_batch(self.encoder_state, beam_width),\n",
    "                    beam_width = beam_width,\n",
    "                    output_layer = dense_layer,\n",
    "                    length_penalty_weight = 0.0)\n",
    "            predicting_decoder_output, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "                    decoder = predicting_decoder,\n",
    "                    impute_finished = False,\n",
    "                    maximum_iterations = 3 * tf.reduce_max(self.X_seq_len))\n",
    "            \n",
    "        self.training_logits = training_decoder_output.rnn_output\n",
    "        self.predicting_ids = tf.identity(predicting_decoder_output.predicted_ids[:, :, 0],name=\"logits\")\n",
    "        \n",
    "        masks = tf.sequence_mask(self.Y_seq_len, tf.reduce_max(self.Y_seq_len), dtype=tf.float32)\n",
    "        self.cost = tf.contrib.seq2seq.sequence_loss(logits = self.training_logits,\n",
    "                                                     targets = self.Y,\n",
    "                                                     weights = masks)\n",
    "        \n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate).minimize(self.cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_layer = 256\n",
    "num_layers = 2\n",
    "embedded_size = 128\n",
    "learning_rate = 5e-4\n",
    "batch_size = 128\n",
    "epoch = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gradients_impl.py:97: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "model = Stemmer(size_layer, num_layers, embedded_size, len(dictionary_from), \n",
    "                len(dictionary_to), learning_rate)\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_idx(corpus, dic, UNK=3):\n",
    "    X = []\n",
    "    for i in corpus:\n",
    "        ints = []\n",
    "        for k in i:\n",
    "            try:\n",
    "                ints.append(dic[k])\n",
    "            except Exception as e:\n",
    "                ints.append(UNK)\n",
    "        X.append(ints)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = str_idx(before, dictionary_from)\n",
    "Y = str_idx(after, dictionary_to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sentence_batch(sentence_batch, pad_int):\n",
    "    padded_seqs = []\n",
    "    seq_lens = []\n",
    "    max_sentence_len = max([len(sentence) for sentence in sentence_batch])\n",
    "    for sentence in sentence_batch:\n",
    "        padded_seqs.append(sentence + [pad_int] * (max_sentence_len - len(sentence)))\n",
    "        seq_lens.append(len(sentence))\n",
    "    return padded_seqs, seq_lens\n",
    "\n",
    "def check_accuracy(logits, Y):\n",
    "    acc = 0\n",
    "    for i in range(logits.shape[0]):\n",
    "        internal_acc = 0\n",
    "        count = 0\n",
    "        for k in range(len(Y[i])):\n",
    "            try:\n",
    "                if Y[i][k] == logits[i][k]:\n",
    "                    internal_acc += 1\n",
    "                count += 1\n",
    "                if Y[i][k] == EOS:\n",
    "                    break\n",
    "            except:\n",
    "                break\n",
    "        acc += (internal_acc / count)\n",
    "    return acc / logits.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 892/892 [05:06<00:00,  2.91it/s, accuracy=0.594, cost=0.483]\n",
      "train minibatch loop:   0%|          | 0/892 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, avg loss: 0.695482, avg accuracy: 0.516531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 892/892 [05:43<00:00,  2.60it/s, accuracy=0.716, cost=0.428]\n",
      "train minibatch loop:   0%|          | 0/892 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2, avg loss: 0.356663, avg accuracy: 0.716523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 892/892 [05:44<00:00,  2.59it/s, accuracy=0.732, cost=0.238]\n",
      "train minibatch loop:   0%|          | 0/892 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3, avg loss: 0.281400, avg accuracy: 0.760839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 892/892 [05:43<00:00,  2.60it/s, accuracy=0.726, cost=0.311] \n",
      "train minibatch loop:   0%|          | 0/892 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4, avg loss: 0.242011, avg accuracy: 0.782182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 892/892 [05:49<00:00,  2.56it/s, accuracy=0.85, cost=0.24]   \n",
      "train minibatch loop:   0%|          | 0/892 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5, avg loss: 0.217453, avg accuracy: 0.797168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 892/892 [05:43<00:00,  2.59it/s, accuracy=0.842, cost=0.206] \n",
      "train minibatch loop:   0%|          | 0/892 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6, avg loss: 0.198011, avg accuracy: 0.809455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 892/892 [05:43<00:00,  2.59it/s, accuracy=0.77, cost=0.251]  \n",
      "train minibatch loop:   0%|          | 0/892 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7, avg loss: 0.181745, avg accuracy: 0.820828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 892/892 [05:45<00:00,  2.58it/s, accuracy=0.846, cost=0.15]  \n",
      "train minibatch loop:   0%|          | 0/892 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8, avg loss: 0.165816, avg accuracy: 0.831306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 892/892 [05:45<00:00,  2.58it/s, accuracy=0.806, cost=0.167] \n",
      "train minibatch loop:   0%|          | 0/892 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9, avg loss: 0.152959, avg accuracy: 0.842309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 892/892 [05:43<00:00,  2.60it/s, accuracy=0.825, cost=0.202] \n",
      "train minibatch loop:   0%|          | 0/892 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10, avg loss: 0.140761, avg accuracy: 0.852545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 892/892 [05:44<00:00,  2.59it/s, accuracy=0.844, cost=0.152] \n",
      "train minibatch loop:   0%|          | 0/892 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 11, avg loss: 0.128879, avg accuracy: 0.863080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 892/892 [05:43<00:00,  2.60it/s, accuracy=0.841, cost=0.121] \n",
      "train minibatch loop:   0%|          | 0/892 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 12, avg loss: 0.119069, avg accuracy: 0.872432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 892/892 [05:45<00:00,  2.58it/s, accuracy=0.871, cost=0.0944]\n",
      "train minibatch loop:   0%|          | 0/892 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 13, avg loss: 0.109670, avg accuracy: 0.882296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 892/892 [05:44<00:00,  2.59it/s, accuracy=0.919, cost=0.13]  \n",
      "train minibatch loop:   0%|          | 0/892 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 14, avg loss: 0.101104, avg accuracy: 0.891272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 892/892 [05:43<00:00,  2.59it/s, accuracy=0.877, cost=0.129] \n",
      "train minibatch loop:   0%|          | 0/892 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 15, avg loss: 0.092002, avg accuracy: 0.898951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 892/892 [05:46<00:00,  2.57it/s, accuracy=0.844, cost=0.129] \n",
      "train minibatch loop:   0%|          | 0/892 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 16, avg loss: 0.084631, avg accuracy: 0.908217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 892/892 [05:38<00:00,  2.63it/s, accuracy=0.894, cost=0.109] \n",
      "train minibatch loop:   0%|          | 0/892 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 17, avg loss: 0.078061, avg accuracy: 0.914839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 892/892 [05:39<00:00,  2.62it/s, accuracy=0.919, cost=0.0776]\n",
      "train minibatch loop:   0%|          | 0/892 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 18, avg loss: 0.071318, avg accuracy: 0.922429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 892/892 [05:45<00:00,  2.58it/s, accuracy=0.933, cost=0.0522]\n",
      "train minibatch loop:   0%|          | 0/892 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 19, avg loss: 0.065093, avg accuracy: 0.929155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 892/892 [05:44<00:00,  2.59it/s, accuracy=0.957, cost=0.0473]\n",
      "train minibatch loop:   0%|          | 0/892 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 20, avg loss: 0.060398, avg accuracy: 0.935381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 892/892 [05:45<00:00,  2.59it/s, accuracy=0.926, cost=0.0515]\n",
      "train minibatch loop:   0%|          | 0/892 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 21, avg loss: 0.055276, avg accuracy: 0.940644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 892/892 [05:45<00:00,  2.58it/s, accuracy=0.954, cost=0.0693]\n",
      "train minibatch loop:   0%|          | 0/892 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 22, avg loss: 0.051781, avg accuracy: 0.945184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 892/892 [05:43<00:00,  2.60it/s, accuracy=0.945, cost=0.0392]\n",
      "train minibatch loop:   0%|          | 0/892 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 23, avg loss: 0.046855, avg accuracy: 0.951056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 892/892 [05:45<00:00,  2.58it/s, accuracy=0.982, cost=0.0285]\n",
      "train minibatch loop:   0%|          | 0/892 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 24, avg loss: 0.043199, avg accuracy: 0.955613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 892/892 [05:45<00:00,  2.59it/s, accuracy=0.989, cost=0.0535]\n",
      "train minibatch loop:   0%|          | 0/892 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 25, avg loss: 0.039693, avg accuracy: 0.959653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 892/892 [05:52<00:00,  2.53it/s, accuracy=0.973, cost=0.0299]\n",
      "train minibatch loop:   0%|          | 0/892 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 26, avg loss: 0.036768, avg accuracy: 0.963155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 892/892 [05:45<00:00,  2.58it/s, accuracy=0.951, cost=0.0358]\n",
      "train minibatch loop:   0%|          | 0/892 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 27, avg loss: 0.033757, avg accuracy: 0.967068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 892/892 [05:45<00:00,  2.58it/s, accuracy=0.993, cost=0.0221] \n",
      "train minibatch loop:   0%|          | 0/892 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 28, avg loss: 0.031591, avg accuracy: 0.970121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 892/892 [05:42<00:00,  2.60it/s, accuracy=0.992, cost=0.0494]\n",
      "train minibatch loop:   0%|          | 0/892 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 29, avg loss: 0.030114, avg accuracy: 0.972151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 892/892 [05:44<00:00,  2.59it/s, accuracy=0.979, cost=0.0135] \n",
      "train minibatch loop:   0%|          | 0/892 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 30, avg loss: 0.027564, avg accuracy: 0.975314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 892/892 [05:44<00:00,  2.59it/s, accuracy=0.975, cost=0.0498] \n",
      "train minibatch loop:   0%|          | 0/892 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 31, avg loss: 0.025892, avg accuracy: 0.977682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 892/892 [05:44<00:00,  2.59it/s, accuracy=0.918, cost=0.0698] \n",
      "train minibatch loop:   0%|          | 0/892 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 32, avg loss: 0.024069, avg accuracy: 0.979250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 892/892 [05:44<00:00,  2.59it/s, accuracy=0.959, cost=0.0744] \n",
      "train minibatch loop:   0%|          | 0/892 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 33, avg loss: 0.023140, avg accuracy: 0.981469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 892/892 [05:45<00:00,  2.58it/s, accuracy=0.959, cost=0.0458] \n",
      "train minibatch loop:   0%|          | 0/892 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 34, avg loss: 0.020938, avg accuracy: 0.982726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 892/892 [05:44<00:00,  2.59it/s, accuracy=0.995, cost=0.0348] \n",
      "train minibatch loop:   0%|          | 0/892 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 35, avg loss: 0.020584, avg accuracy: 0.983847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 892/892 [05:44<00:00,  2.59it/s, accuracy=0.98, cost=0.0404]  \n",
      "train minibatch loop:   0%|          | 0/892 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 36, avg loss: 0.018204, avg accuracy: 0.985815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 892/892 [05:51<00:00,  2.54it/s, accuracy=0.997, cost=0.00758]\n",
      "train minibatch loop:   0%|          | 0/892 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 37, avg loss: 0.019663, avg accuracy: 0.985068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 892/892 [05:46<00:00,  2.57it/s, accuracy=0.989, cost=0.0161] \n",
      "train minibatch loop:   0%|          | 0/892 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 38, avg loss: 0.017290, avg accuracy: 0.987544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 892/892 [05:45<00:00,  2.58it/s, accuracy=0.988, cost=0.0235] \n",
      "train minibatch loop:   0%|          | 0/892 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 39, avg loss: 0.016680, avg accuracy: 0.988373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 892/892 [05:44<00:00,  2.59it/s, accuracy=0.985, cost=0.0138] \n",
      "train minibatch loop:   0%|          | 0/892 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 40, avg loss: 0.017076, avg accuracy: 0.988090\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 892/892 [05:45<00:00,  2.58it/s, accuracy=0.967, cost=0.0166] \n",
      "train minibatch loop:   0%|          | 0/892 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 41, avg loss: 0.015203, avg accuracy: 0.990019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 892/892 [05:44<00:00,  2.59it/s, accuracy=0.987, cost=0.00969]\n",
      "train minibatch loop:   0%|          | 0/892 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 42, avg loss: 0.014750, avg accuracy: 0.990250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 892/892 [05:43<00:00,  2.60it/s, accuracy=0.966, cost=0.0154] \n",
      "train minibatch loop:   0%|          | 0/892 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 43, avg loss: 0.013783, avg accuracy: 0.990331\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 892/892 [05:44<00:00,  2.59it/s, accuracy=0.996, cost=0.00563]\n",
      "train minibatch loop:   0%|          | 0/892 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 44, avg loss: 0.013778, avg accuracy: 0.990722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 892/892 [05:44<00:00,  2.59it/s, accuracy=0.977, cost=0.0176] \n",
      "train minibatch loop:   0%|          | 0/892 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 45, avg loss: 0.013877, avg accuracy: 0.990801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 892/892 [05:50<00:00,  2.54it/s, accuracy=0.981, cost=0.0237] \n",
      "train minibatch loop:   0%|          | 0/892 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 46, avg loss: 0.013370, avg accuracy: 0.991398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 892/892 [05:52<00:00,  2.53it/s, accuracy=0.976, cost=0.014]  \n",
      "train minibatch loop:   0%|          | 0/892 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 47, avg loss: 0.012350, avg accuracy: 0.991903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 892/892 [05:46<00:00,  2.57it/s, accuracy=0.991, cost=0.0097] \n",
      "train minibatch loop:   0%|          | 0/892 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 48, avg loss: 0.011992, avg accuracy: 0.992415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 892/892 [05:45<00:00,  2.58it/s, accuracy=0.998, cost=0.00385]\n",
      "train minibatch loop:   0%|          | 0/892 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 49, avg loss: 0.012817, avg accuracy: 0.991546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 892/892 [05:44<00:00,  2.59it/s, accuracy=0.976, cost=0.00929]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 50, avg loss: 0.012196, avg accuracy: 0.992369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(epoch):\n",
    "    total_loss, total_accuracy = 0, 0\n",
    "    X, Y = shuffle(X, Y)\n",
    "    pbar = tqdm(range(0, len(before), batch_size), desc='train minibatch loop')\n",
    "    for k in pbar:\n",
    "        batch_x, _ = pad_sentence_batch(X[k: min(k+batch_size,len(before))], PAD)\n",
    "        batch_y, _ = pad_sentence_batch(Y[k: min(k+batch_size,len(before))], PAD)\n",
    "        predicted, loss, _ = sess.run([model.predicting_ids, model.cost, model.optimizer], \n",
    "                                      feed_dict={model.X:batch_x,\n",
    "                                                model.Y:batch_y})\n",
    "        acc = check_accuracy(predicted,batch_y)\n",
    "        total_loss += loss\n",
    "        total_accuracy += acc\n",
    "        pbar.set_postfix(cost=loss, accuracy = acc)\n",
    "        \n",
    "    total_loss /= (len(before) / batch_size)\n",
    "    total_accuracy /= (len(before) / batch_size)\n",
    "    print('epoch: %d, avg loss: %f, avg accuracy: %f'%(i+1, total_loss, total_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = sess.run(model.predicting_ids,feed_dict={model.X:batch_x})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row 1\n",
      "BEFORE: mlysin\n",
      "REAL AFTER: malaysian\n",
      "PREDICTED AFTER: malaysian \n",
      "\n",
      "row 2\n",
      "BEFORE: l-duny\n",
      "REAL AFTER: al-dunya\n",
      "PREDICTED AFTER: al-dunya \n",
      "\n",
      "row 3\n",
      "BEFORE: misri\n",
      "REAL AFTER: misri\n",
      "PREDICTED AFTER: misri \n",
      "\n",
      "row 4\n",
      "BEFORE: nteraks\n",
      "REAL AFTER: interaksi\n",
      "PREDICTED AFTER: interaksi \n",
      "\n",
      "row 5\n",
      "BEFORE: arlngtn\n",
      "REAL AFTER: arlington\n",
      "PREDICTED AFTER: arlington \n",
      "\n",
      "row 6\n",
      "BEFORE: knsng\n",
      "REAL AFTER: kuansing\n",
      "PREDICTED AFTER: kuansing \n",
      "\n",
      "row 7\n",
      "BEFORE: bn-sekijng\n",
      "REAL AFTER: bn-sekijang\n",
      "PREDICTED AFTER: bn-sekijang \n",
      "\n",
      "row 8\n",
      "BEFORE: ysr\n",
      "REAL AFTER: yasir\n",
      "PREDICTED AFTER: yasir \n",
      "\n",
      "row 9\n",
      "BEFORE: mmnul\n",
      "REAL AFTER: emmanuel\n",
      "PREDICTED AFTER: emmanuel \n",
      "\n",
      "row 10\n",
      "BEFORE: msladng\n",
      "REAL AFTER: misleading\n",
      "PREDICTED AFTER: misleading \n",
      "\n",
      "row 11\n",
      "BEFORE: inflate\n",
      "REAL AFTER: inflate\n",
      "PREDICTED AFTER: inflate \n",
      "\n",
      "row 12\n",
      "BEFORE: disebalik\n",
      "REAL AFTER: disebalik\n",
      "PREDICTED AFTER: disebalik \n",
      "\n",
      "row 13\n",
      "BEFORE: perjdin\n",
      "REAL AFTER: perjudian\n",
      "PREDICTED AFTER: perjudian \n",
      "\n",
      "row 14\n",
      "BEFORE: abandn\n",
      "REAL AFTER: abandon\n",
      "PREDICTED AFTER: abandon \n",
      "\n",
      "row 15\n",
      "BEFORE: trkawalbagi\n",
      "REAL AFTER: terkawalbagi\n",
      "PREDICTED AFTER: terkawalbagi \n",
      "\n",
      "row 16\n",
      "BEFORE: hopeflly\n",
      "REAL AFTER: hopefully\n",
      "PREDICTED AFTER: hopefully \n",
      "\n",
      "row 17\n",
      "BEFORE: merogol\n",
      "REAL AFTER: merogol\n",
      "PREDICTED AFTER: merogol \n",
      "\n",
      "row 18\n",
      "BEFORE: keriuhan\n",
      "REAL AFTER: keriuhan\n",
      "PREDICTED AFTER: keriuhan \n",
      "\n",
      "row 19\n",
      "BEFORE: spcialists\n",
      "REAL AFTER: specialists\n",
      "PREDICTED AFTER: specialists \n",
      "\n",
      "row 20\n",
      "BEFORE: rgg\n",
      "REAL AFTER: reggae\n",
      "PREDICTED AFTER: reggae \n",
      "\n",
      "row 21\n",
      "BEFORE: rosyidsanter\n",
      "REAL AFTER: rosyidsanter\n",
      "PREDICTED AFTER: rosyidsanter \n",
      "\n",
      "row 22\n",
      "BEFORE: sumiani\n",
      "REAL AFTER: sumiani\n",
      "PREDICTED AFTER: sumiani \n",
      "\n",
      "row 23\n",
      "BEFORE: penjananaan\n",
      "REAL AFTER: penjananaan\n",
      "PREDICTED AFTER: penjananaan \n",
      "\n",
      "row 24\n",
      "BEFORE: plpis\n",
      "REAL AFTER: pelapis\n",
      "PREDICTED AFTER: pelapis \n",
      "\n",
      "row 25\n",
      "BEFORE: azaltmak\n",
      "REAL AFTER: azaltmak\n",
      "PREDICTED AFTER: azaltmak \n",
      "\n",
      "row 26\n",
      "BEFORE: masla\n",
      "REAL AFTER: masela\n",
      "PREDICTED AFTER: masela \n",
      "\n",
      "row 27\n",
      "BEFORE: terdaftarterimakasih\n",
      "REAL AFTER: terdaftarterimakasih\n",
      "PREDICTED AFTER: terdaftarterimakasih \n",
      "\n",
      "row 28\n",
      "BEFORE: pmasukan\n",
      "REAL AFTER: pemasukan\n",
      "PREDICTED AFTER: pemasukan \n",
      "\n",
      "row 29\n",
      "BEFORE: simbangkan\n",
      "REAL AFTER: seimbangkan\n",
      "PREDICTED AFTER: seimbangkan \n",
      "\n",
      "row 30\n",
      "BEFORE: prtarhkan\n",
      "REAL AFTER: pertaruhkan\n",
      "PREDICTED AFTER: pertaruhkan \n",
      "\n",
      "row 31\n",
      "BEFORE: directive\n",
      "REAL AFTER: directive\n",
      "PREDICTED AFTER: directive \n",
      "\n",
      "row 32\n",
      "BEFORE: stabltas\n",
      "REAL AFTER: stabilitas\n",
      "PREDICTED AFTER: stabilitas \n",
      "\n",
      "row 33\n",
      "BEFORE: zeven\n",
      "REAL AFTER: zeven\n",
      "PREDICTED AFTER: zeven \n",
      "\n",
      "row 34\n",
      "BEFORE: shes\n",
      "REAL AFTER: shoes\n",
      "PREDICTED AFTER: shoes \n",
      "\n",
      "row 35\n",
      "BEFORE: mbilh\n",
      "REAL AFTER: ambilah\n",
      "PREDICTED AFTER: ambilah \n",
      "\n",
      "row 36\n",
      "BEFORE: srakah\n",
      "REAL AFTER: serakah\n",
      "PREDICTED AFTER: serakah \n",
      "\n",
      "row 37\n",
      "BEFORE: obyek\n",
      "REAL AFTER: obyek\n",
      "PREDICTED AFTER: obyek \n",
      "\n",
      "row 38\n",
      "BEFORE: prnspny\n",
      "REAL AFTER: prinsipnya\n",
      "PREDICTED AFTER: prinsipnya \n",
      "\n",
      "row 39\n",
      "BEFORE: makanisme\n",
      "REAL AFTER: makanisme\n",
      "PREDICTED AFTER: makanisme \n",
      "\n",
      "row 40\n",
      "BEFORE: djmli\n",
      "REAL AFTER: djumali\n",
      "PREDICTED AFTER: djumali \n",
      "\n",
      "row 41\n",
      "BEFORE: tergerk\n",
      "REAL AFTER: tergerak\n",
      "PREDICTED AFTER: tergerak \n",
      "\n",
      "row 42\n",
      "BEFORE: ihsg\n",
      "REAL AFTER: ihsg\n",
      "PREDICTED AFTER: ihsg \n",
      "\n",
      "row 43\n",
      "BEFORE: israel-gaza\n",
      "REAL AFTER: israel-gaza\n",
      "PREDICTED AFTER: israel-gaza \n",
      "\n",
      "row 44\n",
      "BEFORE: brahm\n",
      "REAL AFTER: ibrahim\n",
      "PREDICTED AFTER: ibrahim \n",
      "\n",
      "row 45\n",
      "BEFORE: drft\n",
      "REAL AFTER: draft\n",
      "PREDICTED AFTER: draft \n",
      "\n",
      "row 46\n",
      "BEFORE: sndh\n",
      "REAL AFTER: sandah\n",
      "PREDICTED AFTER: sandah \n",
      "\n",
      "row 47\n",
      "BEFORE: rmbang\n",
      "REAL AFTER: rembang\n",
      "PREDICTED AFTER: rembang \n",
      "\n",
      "row 48\n",
      "BEFORE: dbntngkn\n",
      "REAL AFTER: dibentangkan\n",
      "PREDICTED AFTER: dibentangkan \n",
      "\n",
      "row 49\n",
      "BEFORE: wht\n",
      "REAL AFTER: what\n",
      "PREDICTED AFTER: what \n",
      "\n",
      "row 50\n",
      "BEFORE: dirahsiakan\n",
      "REAL AFTER: dirahsiakan\n",
      "PREDICTED AFTER: dirahsiakan \n",
      "\n",
      "row 51\n",
      "BEFORE: brhadap-hadapan\n",
      "REAL AFTER: berhadap-hadapan\n",
      "PREDICTED AFTER: berhadap-hadapan \n",
      "\n",
      "row 52\n",
      "BEFORE: rangrama\n",
      "REAL AFTER: orangramai\n",
      "PREDICTED AFTER: orangramai \n",
      "\n",
      "row 53\n",
      "BEFORE: scntsts\n",
      "REAL AFTER: scientists\n",
      "PREDICTED AFTER: scientists \n",
      "\n",
      "row 54\n",
      "BEFORE: berwaspada\n",
      "REAL AFTER: berwaspada\n",
      "PREDICTED AFTER: berwaspada \n",
      "\n",
      "row 55\n",
      "BEFORE: fghn\n",
      "REAL AFTER: afghan\n",
      "PREDICTED AFTER: afghan \n",
      "\n",
      "row 56\n",
      "BEFORE: arifinjatimnw\n",
      "REAL AFTER: arifinjatimnow\n",
      "PREDICTED AFTER: arifinjatimnow \n",
      "\n",
      "row 57\n",
      "BEFORE: perseimbangan\n",
      "REAL AFTER: perseimbangan\n",
      "PREDICTED AFTER: perseimbangan \n",
      "\n",
      "row 58\n",
      "BEFORE: ditumbuk\n",
      "REAL AFTER: ditumbuk\n",
      "PREDICTED AFTER: ditumbuk \n",
      "\n",
      "row 59\n",
      "BEFORE: shin\n",
      "REAL AFTER: shin\n",
      "PREDICTED AFTER: shin \n",
      "\n",
      "row 60\n",
      "BEFORE: dserp\n",
      "REAL AFTER: diserapi\n",
      "PREDICTED AFTER: diserapi \n",
      "\n",
      "row 61\n",
      "BEFORE: adba\n",
      "REAL AFTER: adeeba\n",
      "PREDICTED AFTER: adeeba \n",
      "\n",
      "row 62\n",
      "BEFORE: ritz-carltn\n",
      "REAL AFTER: ritz-carlton\n",
      "PREDICTED AFTER: ritz-carlton \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(batch_x)):\n",
    "    print('row %d'%(i+1))\n",
    "    print('BEFORE:',''.join([rev_dictionary_from[n] for n in batch_x[i] if n not in [0,1,2,3]]))\n",
    "    print('REAL AFTER:',''.join([rev_dictionary_to[n] for n in batch_y[i] if n not in[0,1,2,3]]))\n",
    "    print('PREDICTED AFTER:',''.join([rev_dictionary_to[n] for n in predicted[i] if n not in[0,1,2,3]]),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICTED AFTER: milyu\n"
     ]
    }
   ],
   "source": [
    "predicted = sess.run(model.predicting_ids,feed_dict={model.X:str_idx(['mly'],dictionary_from)})[0]\n",
    "print('PREDICTED AFTER:',''.join([rev_dictionary_to[n] for n in predicted if n not in[0,1,2,3]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICTED AFTER: nikomati\n"
     ]
    }
   ],
   "source": [
    "predicted = sess.run(model.predicting_ids,feed_dict={model.X:str_idx(['nikmt'],dictionary_from)})[0]\n",
    "print('PREDICTED AFTER:',''.join([rev_dictionary_to[n] for n in predicted if n not in[0,1,2,3]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICTED AFTER: pemusukan\n"
     ]
    }
   ],
   "source": [
    "predicted = sess.run(model.predicting_ids,feed_dict={model.X:str_idx(['pmsukan'],dictionary_from)})[0]\n",
    "print('PREDICTED AFTER:',''.join([rev_dictionary_to[n] for n in predicted if n not in[0,1,2,3]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICTED AFTER: bsekuk\n"
     ]
    }
   ],
   "source": [
    "predicted = sess.run(model.predicting_ids,feed_dict={model.X:str_idx(['bsuk'],dictionary_from)})[0]\n",
    "print('PREDICTED AFTER:',''.join([rev_dictionary_to[n] for n in predicted if n not in[0,1,2,3]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICTED AFTER: comelio\n"
     ]
    }
   ],
   "source": [
    "predicted = sess.run(model.predicting_ids,feed_dict={model.X:str_idx(['cmel'],dictionary_from)})[0]\n",
    "print('PREDICTED AFTER:',''.join([rev_dictionary_to[n] for n in predicted if n not in[0,1,2,3]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'normalizer/model.ckpt'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saver = tf.train.Saver(tf.global_variables())\n",
    "saver.save(sess, \"normalizer/model.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "strings=','.join([n.name for n in tf.get_default_graph().as_graph_def().node if \"Variable\" in n.op or n.name.find('Placeholder') >= 0 or n.name.find('logits') == 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_graph(model_dir, output_node_names):\n",
    "\n",
    "    if not tf.gfile.Exists(model_dir):\n",
    "        raise AssertionError(\n",
    "            \"Export directory doesn't exists. Please specify an export \"\n",
    "            \"directory: %s\" % model_dir)\n",
    "\n",
    "    checkpoint = tf.train.get_checkpoint_state(model_dir)\n",
    "    input_checkpoint = checkpoint.model_checkpoint_path\n",
    "    \n",
    "    absolute_model_dir = \"/\".join(input_checkpoint.split('/')[:-1])\n",
    "    output_graph = absolute_model_dir + \"/frozen_model.pb\"\n",
    "    clear_devices = True\n",
    "    with tf.Session(graph=tf.Graph()) as sess:\n",
    "        saver = tf.train.import_meta_graph(input_checkpoint + '.meta', clear_devices=clear_devices)\n",
    "        saver.restore(sess, input_checkpoint)\n",
    "        output_graph_def = tf.graph_util.convert_variables_to_constants(\n",
    "            sess,\n",
    "            tf.get_default_graph().as_graph_def(),\n",
    "            output_node_names.split(\",\")\n",
    "        ) \n",
    "        with tf.gfile.GFile(output_graph, \"wb\") as f:\n",
    "            f.write(output_graph_def.SerializeToString())\n",
    "        print(\"%d ops in the final graph.\" % len(output_graph_def.node))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from normalizer/model.ckpt\n",
      "INFO:tensorflow:Froze 50 variables.\n",
      "Converted 50 variables to const ops.\n",
      "1295 ops in the final graph.\n"
     ]
    }
   ],
   "source": [
    "freeze_graph(\"normalizer\", strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_graph(frozen_graph_filename):\n",
    "    with tf.gfile.GFile(frozen_graph_filename, \"rb\") as f:\n",
    "        graph_def = tf.GraphDef()\n",
    "        graph_def.ParseFromString(f.read())\n",
    "    with tf.Graph().as_default() as graph:\n",
    "        tf.import_graph_def(graph_def)\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.contrib.seq2seq.python.ops import beam_search_ops\n",
    "g=load_graph('normalizer/frozen_model.pb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICTED AFTER: bujalan\n"
     ]
    }
   ],
   "source": [
    "x = g.get_tensor_by_name('import/Placeholder:0')\n",
    "logits = g.get_tensor_by_name('import/logits:0')\n",
    "test_sess = tf.InteractiveSession(graph=g)\n",
    "predicted = test_sess.run(logits,feed_dict={x:str_idx(['bjalan'],dictionary_from)})[0]\n",
    "print('PREDICTED AFTER:',''.join([rev_dictionary_to[n] for n in predicted if n not in[0,1,2,3]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('normalizer-deep.json','w') as fopen:\n",
    "    fopen.write(json.dumps({'dictionary_from':dictionary_from,\n",
    "                'dictionary_to':dictionary_to,\n",
    "                'rev_dictionary_to':rev_dictionary_to,\n",
    "                'rev_dictionary_from':rev_dictionary_from}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
