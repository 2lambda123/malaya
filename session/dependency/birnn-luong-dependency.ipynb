{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('gsd-ud-train.conllu.txt') as fopen:\n",
    "    corpus = fopen.read().split('\\n')\n",
    "    \n",
    "with open('gsd-ud-test.conllu.txt') as fopen:\n",
    "    corpus.extend(fopen.read().split('\\n'))\n",
    "    \n",
    "with open('gsd-ud-dev.conllu.txt') as fopen:\n",
    "    corpus.extend(fopen.read().split('\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = {'PAD': 0,'NUM':1,'UNK':2}\n",
    "tag2idx = {'PAD': 0}\n",
    "char2idx = {'PAD': 0,'NUM':1,'UNK':2}\n",
    "word_idx = 3\n",
    "tag_idx = 1\n",
    "char_idx = 3\n",
    "\n",
    "def process_string(string):\n",
    "    string = re.sub('[^A-Za-z0-9\\-\\/ ]+', ' ', string).split()\n",
    "    return [to_title(y.strip()) for y in string]\n",
    "\n",
    "def to_title(string):\n",
    "    if string.isupper():\n",
    "        string = string.title()\n",
    "    return string\n",
    "\n",
    "def process_corpus(corpus, until = None):\n",
    "    global word2idx, tag2idx, char2idx, word_idx, tag_idx, char_idx\n",
    "    sentences, words, depends, labels = [], [], [], []\n",
    "    temp_sentence, temp_word, temp_depend, temp_label = [], [], [], []\n",
    "    for sentence in corpus:\n",
    "        if len(sentence):\n",
    "            if sentence[0] == '#':\n",
    "                continue\n",
    "            sentence = sentence.split('\\t')\n",
    "            temp = process_string(sentence[1])\n",
    "            if not len(temp):\n",
    "                sentence[1] = 'EMPTY'\n",
    "            sentence[1] = process_string(sentence[1])[0]\n",
    "            for c in sentence[1]:\n",
    "                if c not in char2idx:\n",
    "                    char2idx[c] = char_idx\n",
    "                    char_idx += 1\n",
    "            if sentence[7] not in tag2idx:\n",
    "                tag2idx[sentence[7]] = tag_idx\n",
    "                tag_idx += 1\n",
    "            if sentence[1] not in word2idx:\n",
    "                word2idx[sentence[1]] = word_idx\n",
    "                word_idx += 1\n",
    "            temp_word.append(word2idx[sentence[1]])\n",
    "            temp_depend.append(int(sentence[6]) + 1)\n",
    "            temp_label.append(tag2idx[sentence[7]])\n",
    "            temp_sentence.append(sentence[1])\n",
    "        else:\n",
    "            words.append(temp_word)\n",
    "            depends.append(temp_depend)\n",
    "            labels.append(temp_label)\n",
    "            sentences.append(temp_sentence)\n",
    "            temp_word = []\n",
    "            temp_depend = []\n",
    "            temp_label = []\n",
    "            temp_sentence = []\n",
    "    return sentences[:-1], words[:-1], depends[:-1], labels[:-1]\n",
    "        \n",
    "sentences, words, depends, labels = process_corpus(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5595, 189)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = pad_sequences(words,padding='post')\n",
    "depends = pad_sequences(depends,padding='post')\n",
    "labels = pad_sequences(labels,padding='post')\n",
    "words.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_char_seq(batch, UNK = 2):\n",
    "    maxlen_c = max([len(k) for k in batch])\n",
    "    x = [[len(i) for i in k] for k in batch]\n",
    "    maxlen = max([j for i in x for j in i])\n",
    "    temp = np.zeros((len(batch),maxlen_c,maxlen),dtype=np.int32)\n",
    "    for i in range(len(batch)):\n",
    "        for k in range(len(batch[i])):\n",
    "            for no, c in enumerate(batch[i][k][:maxlen][::-1]):\n",
    "                temp[i,k,-1-no] = char2idx.get(c, UNK)\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2word = {idx: tag for tag, idx in word2idx.items()}\n",
    "idx2tag = {i: w for w, i in tag2idx.items()}\n",
    "char = generate_char_seq(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "train_X, test_X, train_Y, test_Y, train_depends, test_depends, train_char, test_char = train_test_split(words,\n",
    "                                                                           labels,\n",
    "                                                                           depends,\n",
    "                                                                           char,\n",
    "                                                                           test_size=0.1)\n",
    "train_X = words\n",
    "train_Y = labels\n",
    "train_depends = depends\n",
    "train_char = char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim_word,\n",
    "        dim_char,\n",
    "        dropout,\n",
    "        learning_rate,\n",
    "        hidden_size_char,\n",
    "        hidden_size_word,\n",
    "        num_layers,\n",
    "        maxlen\n",
    "    ):\n",
    "        def cells(size, reuse = False):\n",
    "            return tf.contrib.rnn.DropoutWrapper(\n",
    "                tf.nn.rnn_cell.LSTMCell(\n",
    "                    size,\n",
    "                    initializer = tf.orthogonal_initializer(),\n",
    "                    reuse = reuse,\n",
    "                ),\n",
    "                output_keep_prob = dropout,\n",
    "            )\n",
    "\n",
    "        def bahdanau(embedded, size):\n",
    "            attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(\n",
    "                num_units = hidden_size_word, memory = embedded\n",
    "            )\n",
    "            return tf.contrib.seq2seq.AttentionWrapper(\n",
    "                cell = cells(hidden_size_word),\n",
    "                attention_mechanism = attention_mechanism,\n",
    "                attention_layer_size = hidden_size_word,\n",
    "            )\n",
    "\n",
    "        self.word_ids = tf.placeholder(tf.int32, shape = [None, None])\n",
    "        self.char_ids = tf.placeholder(tf.int32, shape = [None, None, None])\n",
    "        self.labels = tf.placeholder(tf.int32, shape = [None, None])\n",
    "        self.depends = tf.placeholder(tf.int32, shape = [None, None])\n",
    "        self.maxlen = tf.shape(self.word_ids)[1]\n",
    "        self.lengths = tf.count_nonzero(self.word_ids, 1)\n",
    "\n",
    "        self.word_embeddings = tf.Variable(\n",
    "            tf.truncated_normal(\n",
    "                [len(word2idx), dim_word], stddev = 1.0 / np.sqrt(dim_word)\n",
    "            )\n",
    "        )\n",
    "        self.char_embeddings = tf.Variable(\n",
    "            tf.truncated_normal(\n",
    "                [len(char2idx), dim_char], stddev = 1.0 / np.sqrt(dim_char)\n",
    "            )\n",
    "        )\n",
    "\n",
    "        word_embedded = tf.nn.embedding_lookup(\n",
    "            self.word_embeddings, self.word_ids\n",
    "        )\n",
    "        char_embedded = tf.nn.embedding_lookup(\n",
    "            self.char_embeddings, self.char_ids\n",
    "        )\n",
    "        s = tf.shape(char_embedded)\n",
    "        char_embedded = tf.reshape(\n",
    "            char_embedded, shape = [s[0] * s[1], s[-2], dim_char]\n",
    "        )\n",
    "\n",
    "        for n in range(num_layers):\n",
    "            (out_fw, out_bw), (\n",
    "                state_fw,\n",
    "                state_bw,\n",
    "            ) = tf.nn.bidirectional_dynamic_rnn(\n",
    "                cell_fw = cells(hidden_size_char),\n",
    "                cell_bw = cells(hidden_size_char),\n",
    "                inputs = char_embedded,\n",
    "                dtype = tf.float32,\n",
    "                scope = 'bidirectional_rnn_char_%d' % (n),\n",
    "            )\n",
    "            char_embedded = tf.concat((out_fw, out_bw), 2)\n",
    "        output = tf.reshape(\n",
    "            char_embedded[:, -1], shape = [s[0], s[1], 2 * hidden_size_char]\n",
    "        )\n",
    "        word_embedded = tf.concat([word_embedded, output], axis = -1)\n",
    "\n",
    "        for n in range(num_layers):\n",
    "            (out_fw, out_bw), (\n",
    "                state_fw,\n",
    "                state_bw,\n",
    "            ) = tf.nn.bidirectional_dynamic_rnn(\n",
    "                cell_fw = bahdanau(word_embedded, hidden_size_word),\n",
    "                cell_bw = bahdanau(word_embedded, hidden_size_word),\n",
    "                inputs = word_embedded,\n",
    "                dtype = tf.float32,\n",
    "                scope = 'bidirectional_rnn_word_%d' % (n),\n",
    "            )\n",
    "            word_embedded = tf.concat((out_fw, out_bw), 2)\n",
    "\n",
    "        logits = tf.layers.dense(word_embedded, len(idx2tag))\n",
    "        \n",
    "        tag_embeddings = tf.Variable(\n",
    "            tf.truncated_normal(\n",
    "                [len(idx2tag), dim_word], stddev = 1.0 / np.sqrt(dim_word)\n",
    "            )\n",
    "        )\n",
    "        logits_max = tf.argmax(logits,axis=2,output_type=tf.int32)\n",
    "        lookup_logits = tf.nn.embedding_lookup(\n",
    "            tag_embeddings, logits_max\n",
    "        )\n",
    "        (out_fw, out_bw), _ = tf.nn.bidirectional_dynamic_rnn(\n",
    "                cell_fw = cells(hidden_size_word),\n",
    "                cell_bw = cells(hidden_size_word),\n",
    "                inputs = word_embedded,\n",
    "                dtype = tf.float32,\n",
    "                scope = 'bidirectional_rnn_word_%d' % (n),\n",
    "            )\n",
    "        \n",
    "        cast_mask = tf.cast(tf.sequence_mask(self.lengths + 1, maxlen = maxlen), dtype = tf.float32)\n",
    "        cast_mask = tf.tile(tf.expand_dims(cast_mask,axis=1),[1,self.maxlen,1]) * 10\n",
    "        \n",
    "        lookup_logits = tf.concat((out_fw, out_bw), 2)\n",
    "        logits_depends = tf.layers.dense(lookup_logits, maxlen)\n",
    "        \n",
    "        logits_depends = tf.multiply(logits_depends, cast_mask)\n",
    "        \n",
    "        log_likelihood, transition_params = tf.contrib.crf.crf_log_likelihood(\n",
    "            logits, self.labels, self.lengths\n",
    "        )\n",
    "        with tf.variable_scope(\"depends\"):\n",
    "            log_likelihood_depends, transition_params_depends = tf.contrib.crf.crf_log_likelihood(\n",
    "                logits_depends, self.depends, self.lengths\n",
    "            )\n",
    "        self.cost = tf.reduce_mean(-log_likelihood) + tf.reduce_mean(-log_likelihood_depends)\n",
    "        self.optimizer = tf.train.AdamOptimizer(\n",
    "            learning_rate = learning_rate\n",
    "        ).minimize(self.cost)\n",
    "        \n",
    "        mask = tf.sequence_mask(self.lengths, maxlen = self.maxlen)\n",
    "        \n",
    "        self.tags_seq, _ = tf.contrib.crf.crf_decode(\n",
    "            logits, transition_params, self.lengths\n",
    "        )\n",
    "        self.tags_seq = tf.identity(self.tags_seq, name = 'logits')\n",
    "        \n",
    "        self.tags_seq_depends, _ = tf.contrib.crf.crf_decode(\n",
    "            logits_depends, transition_params_depends, self.lengths\n",
    "        )\n",
    "        self.tags_seq_depends = tf.identity(self.tags_seq_depends, name = 'logits_depends')\n",
    "\n",
    "        self.prediction = tf.boolean_mask(self.tags_seq, mask)\n",
    "        mask_label = tf.boolean_mask(self.labels, mask)\n",
    "        correct_pred = tf.equal(self.prediction, mask_label)\n",
    "        correct_index = tf.cast(correct_pred, tf.float32)\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "        \n",
    "        self.prediction = tf.boolean_mask(self.tags_seq_depends, mask)\n",
    "        mask_label = tf.boolean_mask(self.depends, mask)\n",
    "        correct_pred = tf.equal(self.prediction, mask_label)\n",
    "        correct_index = tf.cast(correct_pred, tf.float32)\n",
    "        self.accuracy_depends = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "dim_word = 128\n",
    "dim_char = 256\n",
    "dropout = 0.9\n",
    "learning_rate = 1e-3\n",
    "hidden_size_char = 64\n",
    "hidden_size_word = 64\n",
    "num_layers = 2\n",
    "batch_size = 32\n",
    "\n",
    "model = Model(dim_word,dim_char,dropout,learning_rate,hidden_size_char,hidden_size_word,num_layers,\n",
    "             words.shape[1])\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 175/175 [08:57<00:00,  2.89s/it, accuracy=0.333, accuracy_depends=0.156, cost=104] \n",
      "test minibatch loop: 100%|██████████| 18/18 [00:20<00:00,  1.02s/it, accuracy=0.295, accuracy_depends=0.142, cost=121] \n",
      "train minibatch loop:   0%|          | 0/175 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken: 558.3575489521027\n",
      "epoch: 0, training loss: 127.057262, training acc: 0.181174, training depends: 0.114003, valid loss: 115.380958, valid acc: 0.323096, valid depends: 0.148569\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 175/175 [08:50<00:00,  2.89s/it, accuracy=0.66, accuracy_depends=0.224, cost=75.4] \n",
      "test minibatch loop: 100%|██████████| 18/18 [00:20<00:00,  1.03s/it, accuracy=0.661, accuracy_depends=0.199, cost=86.4]\n",
      "train minibatch loop:   0%|          | 0/175 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken: 551.6215944290161\n",
      "epoch: 1, training loss: 92.544422, training acc: 0.494969, training depends: 0.173707, valid loss: 83.178265, valid acc: 0.665197, valid depends: 0.224705\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 175/175 [08:51<00:00,  2.89s/it, accuracy=0.763, accuracy_depends=0.347, cost=58]  \n",
      "test minibatch loop: 100%|██████████| 18/18 [00:20<00:00,  1.01s/it, accuracy=0.762, accuracy_depends=0.313, cost=67.5]\n",
      "train minibatch loop:   0%|          | 0/175 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken: 551.5247929096222\n",
      "epoch: 2, training loss: 70.121650, training acc: 0.700845, training depends: 0.276474, valid loss: 64.725662, valid acc: 0.792769, valid depends: 0.346397\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 175/175 [08:51<00:00,  2.88s/it, accuracy=0.799, accuracy_depends=0.43, cost=49.5] \n",
      "test minibatch loop: 100%|██████████| 18/18 [00:21<00:00,  1.05s/it, accuracy=0.822, accuracy_depends=0.39, cost=57.1] \n",
      "train minibatch loop:   0%|          | 0/175 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken: 552.7808158397675\n",
      "epoch: 3, training loss: 57.159094, training acc: 0.781434, training depends: 0.372488, valid loss: 55.749143, valid acc: 0.847064, valid depends: 0.417716\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 175/175 [08:52<00:00,  2.90s/it, accuracy=0.834, accuracy_depends=0.497, cost=43.7]\n",
      "test minibatch loop: 100%|██████████| 18/18 [00:20<00:00,  1.02s/it, accuracy=0.845, accuracy_depends=0.447, cost=50.4]\n",
      "train minibatch loop:   0%|          | 0/175 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken: 552.8781440258026\n",
      "epoch: 4, training loss: 49.477134, training acc: 0.822382, training depends: 0.443484, valid loss: 48.126305, valid acc: 0.869223, valid depends: 0.494021\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 175/175 [08:52<00:00,  2.90s/it, accuracy=0.835, accuracy_depends=0.572, cost=37.9]\n",
      "test minibatch loop: 100%|██████████| 18/18 [00:20<00:00,  1.02s/it, accuracy=0.868, accuracy_depends=0.478, cost=46.4]\n",
      "train minibatch loop:   0%|          | 0/175 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken: 553.6048016548157\n",
      "epoch: 5, training loss: 43.898549, training acc: 0.843285, training depends: 0.502730, valid loss: 44.958184, valid acc: 0.886983, valid depends: 0.505943\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 175/175 [08:54<00:00,  2.90s/it, accuracy=0.858, accuracy_depends=0.551, cost=35.8]\n",
      "test minibatch loop: 100%|██████████| 18/18 [00:20<00:00,  1.02s/it, accuracy=0.889, accuracy_depends=0.568, cost=39.8]\n",
      "train minibatch loop:   0%|          | 0/175 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken: 554.7248446941376\n",
      "epoch: 6, training loss: 39.451955, training acc: 0.862932, training depends: 0.549447, valid loss: 39.604314, valid acc: 0.890252, valid depends: 0.581912\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 175/175 [08:54<00:00,  2.89s/it, accuracy=0.86, accuracy_depends=0.603, cost=34.4] \n",
      "test minibatch loop: 100%|██████████| 18/18 [00:20<00:00,  1.02s/it, accuracy=0.894, accuracy_depends=0.576, cost=37.1]\n",
      "train minibatch loop:   0%|          | 0/175 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken: 554.9294853210449\n",
      "epoch: 7, training loss: 36.070555, training acc: 0.873861, training depends: 0.586304, valid loss: 35.843321, valid acc: 0.915011, valid depends: 0.621563\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 175/175 [08:53<00:00,  2.90s/it, accuracy=0.88, accuracy_depends=0.659, cost=29.4] \n",
      "test minibatch loop: 100%|██████████| 18/18 [00:20<00:00,  1.02s/it, accuracy=0.912, accuracy_depends=0.558, cost=35.4]\n",
      "train minibatch loop:   0%|          | 0/175 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken: 554.0591886043549\n",
      "epoch: 8, training loss: 32.714707, training acc: 0.883740, training depends: 0.624559, valid loss: 34.587814, valid acc: 0.928932, valid depends: 0.606608\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 175/175 [08:52<00:00,  2.91s/it, accuracy=0.893, accuracy_depends=0.614, cost=30.2]\n",
      "test minibatch loop: 100%|██████████| 18/18 [00:20<00:00,  1.03s/it, accuracy=0.902, accuracy_depends=0.633, cost=30.9]\n",
      "train minibatch loop:   0%|          | 0/175 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken: 553.5390350818634\n",
      "epoch: 9, training loss: 29.959949, training acc: 0.894363, training depends: 0.654938, valid loss: 29.847070, valid acc: 0.932327, valid depends: 0.684108\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 175/175 [08:54<00:00,  2.89s/it, accuracy=0.917, accuracy_depends=0.711, cost=23.7]\n",
      "test minibatch loop: 100%|██████████| 18/18 [00:20<00:00,  1.02s/it, accuracy=0.904, accuracy_depends=0.62, cost=32.4] \n",
      "train minibatch loop:   0%|          | 0/175 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken: 555.5344078540802\n",
      "epoch: 10, training loss: 27.314152, training acc: 0.902848, training depends: 0.684169, valid loss: 29.510490, valid acc: 0.935458, valid depends: 0.673691\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 175/175 [08:54<00:00,  2.90s/it, accuracy=0.903, accuracy_depends=0.737, cost=22.2]\n",
      "test minibatch loop: 100%|██████████| 18/18 [00:20<00:00,  1.03s/it, accuracy=0.925, accuracy_depends=0.736, cost=24.7]\n",
      "train minibatch loop:   0%|          | 0/175 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken: 555.2886633872986\n",
      "epoch: 11, training loss: 25.153192, training acc: 0.911327, training depends: 0.707428, valid loss: 25.020954, valid acc: 0.948164, valid depends: 0.749139\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 175/175 [08:54<00:00,  2.93s/it, accuracy=0.912, accuracy_depends=0.744, cost=20.6]\n",
      "test minibatch loop: 100%|██████████| 18/18 [00:21<00:00,  1.03s/it, accuracy=0.928, accuracy_depends=0.757, cost=22.8]\n",
      "train minibatch loop:   0%|          | 0/175 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken: 555.6348447799683\n",
      "epoch: 12, training loss: 23.113035, training acc: 0.917519, training depends: 0.734917, valid loss: 23.136805, valid acc: 0.951224, valid depends: 0.762697\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 175/175 [08:58<00:00,  2.93s/it, accuracy=0.934, accuracy_depends=0.764, cost=18.8]\n",
      "test minibatch loop: 100%|██████████| 18/18 [00:20<00:00,  1.03s/it, accuracy=0.92, accuracy_depends=0.778, cost=21.7] \n",
      "train minibatch loop:   0%|          | 0/175 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken: 559.2290847301483\n",
      "epoch: 13, training loss: 20.231450, training acc: 0.924965, training depends: 0.770418, valid loss: 21.125830, valid acc: 0.963642, valid depends: 0.789239\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 175/175 [08:58<00:00,  2.95s/it, accuracy=0.943, accuracy_depends=0.828, cost=15.7]\n",
      "test minibatch loop: 100%|██████████| 18/18 [00:20<00:00,  1.03s/it, accuracy=0.951, accuracy_depends=0.767, cost=20.4]\n",
      "train minibatch loop:   0%|          | 0/175 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken: 559.6122906208038\n",
      "epoch: 14, training loss: 18.605789, training acc: 0.930739, training depends: 0.786606, valid loss: 19.762061, valid acc: 0.963161, valid depends: 0.804913\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 175/175 [08:56<00:00,  2.89s/it, accuracy=0.939, accuracy_depends=0.82, cost=14.1] \n",
      "test minibatch loop: 100%|██████████| 18/18 [00:20<00:00,  1.02s/it, accuracy=0.946, accuracy_depends=0.765, cost=19.4]\n",
      "train minibatch loop:   0%|          | 0/175 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken: 556.6700410842896\n",
      "epoch: 15, training loss: 18.015131, training acc: 0.936161, training depends: 0.787019, valid loss: 20.387789, valid acc: 0.970386, valid depends: 0.797880\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 175/175 [08:55<00:00,  2.91s/it, accuracy=0.946, accuracy_depends=0.809, cost=14.3]\n",
      "test minibatch loop: 100%|██████████| 18/18 [00:20<00:00,  1.03s/it, accuracy=0.951, accuracy_depends=0.793, cost=17.6]\n",
      "train minibatch loop:   0%|          | 0/175 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken: 556.2308747768402\n",
      "epoch: 16, training loss: 16.398371, training acc: 0.944262, training depends: 0.805975, valid loss: 17.875515, valid acc: 0.977298, valid depends: 0.810636\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 175/175 [08:55<00:00,  2.89s/it, accuracy=0.96, accuracy_depends=0.839, cost=12.3] \n",
      "test minibatch loop: 100%|██████████| 18/18 [00:20<00:00,  1.08s/it, accuracy=0.964, accuracy_depends=0.809, cost=15.1]\n",
      "train minibatch loop:   0%|          | 0/175 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken: 556.5172815322876\n",
      "epoch: 17, training loss: 14.402380, training acc: 0.950167, training depends: 0.831203, valid loss: 15.801414, valid acc: 0.980971, valid depends: 0.841222\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 175/175 [08:54<00:00,  2.92s/it, accuracy=0.943, accuracy_depends=0.868, cost=11.7]\n",
      "test minibatch loop: 100%|██████████| 18/18 [00:20<00:00,  1.03s/it, accuracy=0.969, accuracy_depends=0.786, cost=16.7]\n",
      "train minibatch loop:   0%|          | 0/175 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken: 555.6660966873169\n",
      "epoch: 18, training loss: 13.564262, training acc: 0.955922, training depends: 0.837798, valid loss: 16.738385, valid acc: 0.988350, valid depends: 0.815528\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 175/175 [08:56<00:00,  2.91s/it, accuracy=0.946, accuracy_depends=0.841, cost=12.8]\n",
      "test minibatch loop: 100%|██████████| 18/18 [00:20<00:00,  1.02s/it, accuracy=0.951, accuracy_depends=0.809, cost=17]  \n",
      "train minibatch loop:   0%|          | 0/175 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken: 557.5348522663116\n",
      "epoch: 19, training loss: 12.664963, training acc: 0.960651, training depends: 0.845649, valid loss: 15.546729, valid acc: 0.987737, valid depends: 0.836474\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 175/175 [08:55<00:00,  2.91s/it, accuracy=0.965, accuracy_depends=0.906, cost=8.91]\n",
      "test minibatch loop: 100%|██████████| 18/18 [00:20<00:00,  1.02s/it, accuracy=0.956, accuracy_depends=0.842, cost=12.1]\n",
      "train minibatch loop:   0%|          | 0/175 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken: 556.4496295452118\n",
      "epoch: 20, training loss: 11.079982, training acc: 0.964616, training depends: 0.867020, valid loss: 12.391548, valid acc: 0.994884, valid depends: 0.876719\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 175/175 [08:55<00:00,  2.93s/it, accuracy=0.962, accuracy_depends=0.901, cost=8.57]\n",
      "test minibatch loop: 100%|██████████| 18/18 [00:20<00:00,  1.02s/it, accuracy=0.974, accuracy_depends=0.873, cost=11.9]\n",
      "train minibatch loop:   0%|          | 0/175 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken: 556.6404461860657\n",
      "epoch: 21, training loss: 10.501838, training acc: 0.967369, training depends: 0.873317, valid loss: 12.322381, valid acc: 0.995629, valid depends: 0.874118\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 175/175 [08:55<00:00,  2.91s/it, accuracy=0.955, accuracy_depends=0.868, cost=11]  \n",
      "test minibatch loop: 100%|██████████| 18/18 [00:20<00:00,  1.02s/it, accuracy=0.987, accuracy_depends=0.881, cost=9.26]\n",
      "train minibatch loop:   0%|          | 0/175 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken: 556.3338551521301\n",
      "epoch: 22, training loss: 9.719140, training acc: 0.968852, training depends: 0.883401, valid loss: 12.303984, valid acc: 0.998993, valid depends: 0.875089\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 175/175 [08:55<00:00,  2.91s/it, accuracy=0.962, accuracy_depends=0.879, cost=8.82]\n",
      "test minibatch loop: 100%|██████████| 18/18 [00:20<00:00,  1.02s/it, accuracy=0.966, accuracy_depends=0.894, cost=9.35]\n",
      "train minibatch loop:   0%|          | 0/175 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken: 556.6658236980438\n",
      "epoch: 23, training loss: 9.652018, training acc: 0.970605, training depends: 0.880603, valid loss: 10.448509, valid acc: 1.000401, valid depends: 0.895559\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 175/175 [08:55<00:00,  2.93s/it, accuracy=0.974, accuracy_depends=0.905, cost=7.22]\n",
      "test minibatch loop: 100%|██████████| 18/18 [00:20<00:00,  1.03s/it, accuracy=0.969, accuracy_depends=0.855, cost=12.4]\n",
      "train minibatch loop:   0%|          | 0/175 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken: 556.4406621456146\n",
      "epoch: 24, training loss: 9.503980, training acc: 0.972713, training depends: 0.880009, valid loss: 13.553925, valid acc: 0.999245, valid depends: 0.860306\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 175/175 [08:56<00:00,  2.92s/it, accuracy=0.976, accuracy_depends=0.887, cost=8.15]\n",
      "test minibatch loop: 100%|██████████| 18/18 [00:20<00:00,  1.03s/it, accuracy=0.977, accuracy_depends=0.938, cost=7.55]\n",
      "train minibatch loop:   0%|          | 0/175 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken: 556.8788993358612\n",
      "epoch: 25, training loss: 8.932340, training acc: 0.974598, training depends: 0.888902, valid loss: 9.621113, valid acc: 1.002071, valid depends: 0.913567\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 175/175 [08:56<00:00,  2.95s/it, accuracy=0.976, accuracy_depends=0.91, cost=6.95] \n",
      "test minibatch loop: 100%|██████████| 18/18 [00:20<00:00,  1.03s/it, accuracy=0.984, accuracy_depends=0.922, cost=7.96]\n",
      "train minibatch loop:   0%|          | 0/175 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken: 557.5436239242554\n",
      "epoch: 26, training loss: 7.763038, training acc: 0.976600, training depends: 0.906384, valid loss: 9.311355, valid acc: 1.002204, valid depends: 0.916223\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 175/175 [08:54<00:00,  2.90s/it, accuracy=0.972, accuracy_depends=0.941, cost=5.32]\n",
      "test minibatch loop: 100%|██████████| 18/18 [00:20<00:00,  1.02s/it, accuracy=0.972, accuracy_depends=0.894, cost=9.01]\n",
      "train minibatch loop:   0%|          | 0/175 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken: 555.3644354343414\n",
      "epoch: 27, training loss: 7.169568, training acc: 0.978475, training depends: 0.912633, valid loss: 8.532252, valid acc: 1.004756, valid depends: 0.924155\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 175/175 [08:56<00:00,  2.91s/it, accuracy=0.976, accuracy_depends=0.929, cost=5.9] \n",
      "test minibatch loop: 100%|██████████| 18/18 [00:20<00:00,  1.02s/it, accuracy=0.972, accuracy_depends=0.93, cost=7.81] \n",
      "train minibatch loop:   0%|          | 0/175 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken: 556.7860813140869\n",
      "epoch: 28, training loss: 7.115701, training acc: 0.980013, training depends: 0.911473, valid loss: 7.503770, valid acc: 1.007342, valid depends: 0.942110\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 175/175 [08:56<00:00,  2.93s/it, accuracy=0.977, accuracy_depends=0.931, cost=5.13]\n",
      "test minibatch loop: 100%|██████████| 18/18 [00:20<00:00,  1.03s/it, accuracy=0.979, accuracy_depends=0.935, cost=6.98]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken: 556.8666005134583\n",
      "epoch: 29, training loss: 7.283201, training acc: 0.979966, training depends: 0.908750, valid loss: 7.462735, valid acc: 1.008739, valid depends: 0.939194\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "for e in range(30):\n",
    "    lasttime = time.time()\n",
    "    train_acc, train_loss, test_acc, test_loss, train_acc_depends, test_acc_depends = 0, 0, 0, 0, 0, 0\n",
    "    pbar = tqdm(\n",
    "        range(0, len(train_X), batch_size), desc = 'train minibatch loop'\n",
    "    )\n",
    "    for i in pbar:\n",
    "        batch_x = train_X[i : min(i + batch_size, train_X.shape[0])]\n",
    "        batch_char = train_char[i : min(i + batch_size, train_X.shape[0])]\n",
    "        batch_y = train_Y[i : min(i + batch_size, train_X.shape[0])]\n",
    "        batch_depends = train_depends[i : min(i + batch_size, train_X.shape[0])]\n",
    "        acc_depends, acc, cost, _ = sess.run(\n",
    "            [model.accuracy_depends, model.accuracy, model.cost, model.optimizer],\n",
    "            feed_dict = {\n",
    "                model.word_ids: batch_x,\n",
    "                model.char_ids: batch_char,\n",
    "                model.labels: batch_y,\n",
    "                model.depends: batch_depends\n",
    "            },\n",
    "        )\n",
    "        assert not np.isnan(cost)\n",
    "        train_loss += cost\n",
    "        train_acc += acc\n",
    "        train_acc_depends += acc_depends\n",
    "        pbar.set_postfix(cost = cost, accuracy = acc, accuracy_depends = acc_depends)\n",
    "        \n",
    "    pbar = tqdm(\n",
    "        range(0, len(test_X), batch_size), desc = 'test minibatch loop'\n",
    "    )\n",
    "    for i in pbar:\n",
    "        batch_x = test_X[i : min(i + batch_size, test_X.shape[0])]\n",
    "        batch_char = test_char[i : min(i + batch_size, test_X.shape[0])]\n",
    "        batch_y = test_Y[i : min(i + batch_size, test_X.shape[0])]\n",
    "        batch_depends = test_depends[i : min(i + batch_size, test_X.shape[0])]\n",
    "        acc_depends, acc, cost = sess.run(\n",
    "            [model.accuracy_depends, model.accuracy, model.cost],\n",
    "            feed_dict = {\n",
    "                model.word_ids: batch_x,\n",
    "                model.char_ids: batch_char,\n",
    "                model.labels: batch_y,\n",
    "                model.depends: batch_depends\n",
    "            },\n",
    "        )\n",
    "        assert not np.isnan(cost)\n",
    "        test_loss += cost\n",
    "        test_acc += acc\n",
    "        test_acc_depends += acc_depends\n",
    "        pbar.set_postfix(cost = cost, accuracy = acc, accuracy_depends = acc_depends)\n",
    "    \n",
    "    train_loss /= len(train_X) / batch_size\n",
    "    train_acc /= len(train_X) / batch_size\n",
    "    train_acc_depends /= len(train_X) / batch_size\n",
    "    test_loss /= len(test_X) / batch_size\n",
    "    test_acc /= len(test_X) / batch_size\n",
    "    test_acc_depends /= len(test_X) / batch_size\n",
    "\n",
    "    print('time taken:', time.time() - lasttime)\n",
    "    print(\n",
    "        'epoch: %d, training loss: %f, training acc: %f, training depends: %f, valid loss: %f, valid acc: %f, valid depends: %f\\n'\n",
    "        % (e, train_loss, train_acc, train_acc_depends, test_loss, test_acc, test_acc_depends)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq, deps = sess.run([model.tags_seq, model.tags_seq_depends],\n",
    "        feed_dict={model.word_ids:batch_x[:1],\n",
    "                  model.char_ids:batch_char[:1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = seq[0]\n",
    "deps = deps[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag2idx['root']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 7,  8,  9, 10,  5,  6,  7,  8, 21, 15, 13, 21,  4, 12, 16, 12, 18,\n",
       "        1, 13, 15, 17,  9, 24, 16,  1, 14,  6, 10], dtype=int32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq[seq>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 7,  8,  9, 10,  5,  6,  7,  8, 21, 15, 17, 21,  4, 12, 16, 12, 18,\n",
       "        1, 13, 15, 17,  9, 24, 16,  1, 14,  6, 10], dtype=int32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_y[0][seq>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3, 14,  3,  3,  7,  3,  9,  7,  9, 12,  9, 12,  1, 14, 14, 16, 14,\n",
       "       24, 19, 22, 20, 22, 16, 24, 28, 28, 25, 14], dtype=int32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deps[seq>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3, 14,  3,  3,  7,  3,  9,  7,  9, 12,  9, 12,  1, 14, 14, 16, 24,\n",
       "       24, 19, 22, 20, 22, 16, 24, 28, 28, 25, 14], dtype=int32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_depends[0][seq>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = 'tolong tangkap gambar kami'\n",
    "\n",
    "def char_str_idx(corpus, dic, UNK = 0):\n",
    "    maxlen = max([len(i) for i in corpus])\n",
    "    X = np.zeros((len(corpus), maxlen))\n",
    "    for i in range(len(corpus)):\n",
    "        for no, k in enumerate(corpus[i][:maxlen]):\n",
    "            val = dic[k] if k in dic else UNK\n",
    "            X[i, no] = val\n",
    "    return X\n",
    "\n",
    "def generate_char_seq(batch, UNK = 2):\n",
    "    maxlen_c = max([len(k) for k in batch])\n",
    "    x = [[len(i) for i in k] for k in batch]\n",
    "    maxlen = max([j for i in x for j in i])\n",
    "    temp = np.zeros((len(batch),maxlen_c,maxlen),dtype=np.int32)\n",
    "    for i in range(len(batch)):\n",
    "        for k in range(len(batch[i])):\n",
    "            for no, c in enumerate(batch[i][k][::-1]):\n",
    "                temp[i,k,-1-no] = char2idx.get(c, UNK)\n",
    "    return temp\n",
    "\n",
    "sequence = process_string(string)[:150]\n",
    "X_seq = char_str_idx([sequence], word2idx, 2)\n",
    "X_char_seq = generate_char_seq([sequence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq, deps = sess.run([model.tags_seq, model.tags_seq_depends],\n",
    "        feed_dict={model.word_ids:X_seq,\n",
    "                  model.char_ids:X_char_seq})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 2, 3]], dtype=int32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deps -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tolong', 'tangkap', 'gambar', 'kami']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nsubj', 'compound', 'compound', 'det']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[idx2tag[i] for i in seq[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = []\n",
    "for i in range(len(seq[0])):\n",
    "    string.append('%d\\t%s\\t_\\t_\\t_\\t_\\t%d\\t%s'%(i+1,sequence[i],deps[0,i],idx2tag[seq[0,i]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1\\ttolong\\t_\\t_\\t_\\t_\\t5\\tnsubj',\n",
       " '2\\ttangkap\\t_\\t_\\t_\\t_\\t2\\tacl',\n",
       " '3\\tgambar\\t_\\t_\\t_\\t_\\t3\\tobj',\n",
       " '4\\tkami\\t_\\t_\\t_\\t_\\t4\\tdet']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Placeholder',\n",
       " 'Placeholder_1',\n",
       " 'Placeholder_2',\n",
       " 'Placeholder_3',\n",
       " 'Variable',\n",
       " 'Variable_1',\n",
       " 'bidirectional_rnn_char_0/fw/lstm_cell/kernel',\n",
       " 'bidirectional_rnn_char_0/fw/lstm_cell/bias',\n",
       " 'bidirectional_rnn_char_0/bw/lstm_cell/kernel',\n",
       " 'bidirectional_rnn_char_0/bw/lstm_cell/bias',\n",
       " 'bidirectional_rnn_char_1/fw/lstm_cell/kernel',\n",
       " 'bidirectional_rnn_char_1/fw/lstm_cell/bias',\n",
       " 'bidirectional_rnn_char_1/bw/lstm_cell/kernel',\n",
       " 'bidirectional_rnn_char_1/bw/lstm_cell/bias',\n",
       " 'memory_layer/kernel',\n",
       " 'memory_layer_1/kernel',\n",
       " 'bidirectional_rnn_word_0/fw/attention_wrapper/lstm_cell/kernel',\n",
       " 'bidirectional_rnn_word_0/fw/attention_wrapper/lstm_cell/bias',\n",
       " 'bidirectional_rnn_word_0/fw/attention_wrapper/bahdanau_attention/query_layer/kernel',\n",
       " 'bidirectional_rnn_word_0/fw/attention_wrapper/bahdanau_attention/attention_v',\n",
       " 'bidirectional_rnn_word_0/fw/attention_wrapper/attention_layer/kernel',\n",
       " 'bidirectional_rnn_word_0/bw/attention_wrapper/lstm_cell/kernel',\n",
       " 'bidirectional_rnn_word_0/bw/attention_wrapper/lstm_cell/bias',\n",
       " 'bidirectional_rnn_word_0/bw/attention_wrapper/bahdanau_attention/query_layer/kernel',\n",
       " 'bidirectional_rnn_word_0/bw/attention_wrapper/bahdanau_attention/attention_v',\n",
       " 'bidirectional_rnn_word_0/bw/attention_wrapper/attention_layer/kernel',\n",
       " 'memory_layer_2/kernel',\n",
       " 'memory_layer_3/kernel',\n",
       " 'bidirectional_rnn_word_1/fw/attention_wrapper/lstm_cell/kernel',\n",
       " 'bidirectional_rnn_word_1/fw/attention_wrapper/lstm_cell/bias',\n",
       " 'bidirectional_rnn_word_1/fw/attention_wrapper/bahdanau_attention/query_layer/kernel',\n",
       " 'bidirectional_rnn_word_1/fw/attention_wrapper/bahdanau_attention/attention_v',\n",
       " 'bidirectional_rnn_word_1/fw/attention_wrapper/attention_layer/kernel',\n",
       " 'bidirectional_rnn_word_1/bw/attention_wrapper/lstm_cell/kernel',\n",
       " 'bidirectional_rnn_word_1/bw/attention_wrapper/lstm_cell/bias',\n",
       " 'bidirectional_rnn_word_1/bw/attention_wrapper/bahdanau_attention/query_layer/kernel',\n",
       " 'bidirectional_rnn_word_1/bw/attention_wrapper/bahdanau_attention/attention_v',\n",
       " 'bidirectional_rnn_word_1/bw/attention_wrapper/attention_layer/kernel',\n",
       " 'dense/kernel',\n",
       " 'dense/bias',\n",
       " 'Variable_2',\n",
       " 'bidirectional_rnn_word_1/fw/lstm_cell/kernel',\n",
       " 'bidirectional_rnn_word_1/fw/lstm_cell/bias',\n",
       " 'bidirectional_rnn_word_1/bw/lstm_cell/kernel',\n",
       " 'bidirectional_rnn_word_1/bw/lstm_cell/bias',\n",
       " 'dense_1/kernel',\n",
       " 'dense_1/bias',\n",
       " 'transitions',\n",
       " 'depends/transitions',\n",
       " 'logits',\n",
       " 'logits_depends']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saver = tf.train.Saver(tf.trainable_variables())\n",
    "saver.save(sess, 'luong-dependency/model.ckpt')\n",
    "\n",
    "strings = ','.join(\n",
    "    [\n",
    "        n.name\n",
    "        for n in tf.get_default_graph().as_graph_def().node\n",
    "        if ('Variable' in n.op\n",
    "        or 'Placeholder' in n.name\n",
    "        or 'logits' in n.name\n",
    "        or 'logits_depends' in n.name\n",
    "        or 'alphas' in n.name)\n",
    "        and 'Adam' not in n.name\n",
    "        and 'beta' not in n.name\n",
    "        and 'OptimizeLoss' not in n.name\n",
    "        and 'Global_Step' not in n.name\n",
    "        and 'Epoch_Step' not in n.name\n",
    "        and 'learning_rate' not in n.name\n",
    "    ]\n",
    ")\n",
    "strings.split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('luong-dependency.json','w') as fopen:\n",
    "    fopen.write(json.dumps({'idx2tag':idx2tag,'idx2word':idx2word,\n",
    "           'word2idx':word2idx,'tag2idx':tag2idx,'char2idx':char2idx}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_graph(model_dir, output_node_names):\n",
    "\n",
    "    if not tf.gfile.Exists(model_dir):\n",
    "        raise AssertionError(\n",
    "            \"Export directory doesn't exists. Please specify an export \"\n",
    "            'directory: %s' % model_dir\n",
    "        )\n",
    "\n",
    "    checkpoint = tf.train.get_checkpoint_state(model_dir)\n",
    "    input_checkpoint = checkpoint.model_checkpoint_path\n",
    "\n",
    "    absolute_model_dir = '/'.join(input_checkpoint.split('/')[:-1])\n",
    "    output_graph = absolute_model_dir + '/frozen_model.pb'\n",
    "    clear_devices = True\n",
    "    with tf.Session(graph = tf.Graph()) as sess:\n",
    "        saver = tf.train.import_meta_graph(\n",
    "            input_checkpoint + '.meta', clear_devices = clear_devices\n",
    "        )\n",
    "        saver.restore(sess, input_checkpoint)\n",
    "        output_graph_def = tf.graph_util.convert_variables_to_constants(\n",
    "            sess,\n",
    "            tf.get_default_graph().as_graph_def(),\n",
    "            output_node_names.split(','),\n",
    "        )\n",
    "        with tf.gfile.GFile(output_graph, 'wb') as f:\n",
    "            f.write(output_graph_def.SerializeToString())\n",
    "        print('%d ops in the final graph.' % len(output_graph_def.node))\n",
    "        \n",
    "def load_graph(frozen_graph_filename):\n",
    "    with tf.gfile.GFile(frozen_graph_filename, 'rb') as f:\n",
    "        graph_def = tf.GraphDef()\n",
    "        graph_def.ParseFromString(f.read())\n",
    "    with tf.Graph().as_default() as graph:\n",
    "        tf.import_graph_def(graph_def)\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from luong-dependency/model.ckpt\n",
      "INFO:tensorflow:Froze 45 variables.\n",
      "INFO:tensorflow:Converted 45 variables to const ops.\n",
      "2531 ops in the final graph.\n"
     ]
    }
   ],
   "source": [
    "freeze_graph('luong-dependency', strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = load_graph('luong-dependency/frozen_model.pb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  6 13  3]] [[2 2 3 4]]\n"
     ]
    }
   ],
   "source": [
    "word_ids = g.get_tensor_by_name('import/Placeholder:0')\n",
    "char_ids = g.get_tensor_by_name('import/Placeholder_1:0')\n",
    "tags_seq = g.get_tensor_by_name('import/logits:0')\n",
    "depends_seq = g.get_tensor_by_name('import/logits_depends:0')\n",
    "test_sess = tf.InteractiveSession(graph = g)\n",
    "seq, deps = test_sess.run([tags_seq, depends_seq],\n",
    "            feed_dict = {\n",
    "                word_ids: X_seq,\n",
    "                char_ids: X_char_seq,\n",
    "            })\n",
    "\n",
    "print(seq,deps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
