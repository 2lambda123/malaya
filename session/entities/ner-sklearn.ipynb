{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_string(string):\n",
    "    string = re.sub('[^A-Za-z0-9\\-\\/ ]+', ' ', string).split()\n",
    "    return [y.strip() for y in string]\n",
    "\n",
    "def to_title(string):\n",
    "    string = re.sub('[^A-Za-z0-9\\-\\/ ]+', ' ', string)\n",
    "    return re.sub(r'[ ]+', ' ', string).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_raw(filename):\n",
    "    with open(filename, 'r') as fopen:\n",
    "        entities = fopen.read()\n",
    "    soup = BeautifulSoup(entities, 'html.parser')\n",
    "    inside_tag = ''\n",
    "    texts, labels = [], []\n",
    "    for sentence in soup.prettify().split('\\n'):\n",
    "        if len(inside_tag):\n",
    "            splitted = process_string(sentence)\n",
    "            texts += splitted\n",
    "            labels += [inside_tag] * len(splitted)\n",
    "            inside_tag = ''\n",
    "        else:\n",
    "            if not sentence.find('</'):\n",
    "                pass\n",
    "            elif not sentence.find('<'):\n",
    "                inside_tag = sentence.split('>')[0][1:]\n",
    "            else:\n",
    "                splitted = process_string(sentence)\n",
    "                texts += splitted\n",
    "                labels += ['OTHER'] * len(splitted)\n",
    "    assert (len(texts)==len(labels)), \"length texts and labels are not same\"\n",
    "    print('len texts and labels: ', len(texts))\n",
    "    return texts,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len texts and labels:  34012\n",
      "len texts and labels:  9249\n"
     ]
    }
   ],
   "source": [
    "train_texts, train_labels = parse_raw('data_train.txt')\n",
    "test_texts, test_labels = parse_raw('data_test.txt')\n",
    "train_texts += test_texts\n",
    "train_labels += test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['OTHER', 'location', 'organization', 'person', 'quantity', 'time'],\n",
       "       dtype='<U12'), array([35613,  1536,  1592,  2358,  1336,   826]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(train_labels,return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('entities-bm-normalize-v3.txt','r') as fopen:\n",
    "    entities_bm = fopen.read().split('\\n')[:-1]\n",
    "entities_bm = [i.split() for i in entities_bm]\n",
    "entities_bm = [[i[0],'TIME' if i[0] in 'jam' else i[1]] for i in entities_bm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_by = {'organizaiton':'organization','orgnization':'organization',\n",
    "             'othoer': 'OTHER'}\n",
    "\n",
    "with open('NER-part1.txt','r') as fopen:\n",
    "    nexts = fopen.read().split('\\n')[:-1]\n",
    "nexts = [i.split() for i in nexts]\n",
    "for i in nexts:\n",
    "    if len(i) == 2:\n",
    "        label = i[1].lower()\n",
    "        if 'other' in label:\n",
    "            label = label.upper()\n",
    "        if label in replace_by:\n",
    "            label = replace_by[label]\n",
    "        train_labels.append(label)\n",
    "        train_texts.append(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'KN'\n",
      "'KA'\n"
     ]
    }
   ],
   "source": [
    "replace_by = {'LOC':'location','PRN':'person','NORP':'organization','ORG':'organization','LAW':'law',\n",
    "             'EVENT':'event','FAC':'organization','TIME':'time','O':'OTHER','ART':'person','DOC':'law'}\n",
    "for i in entities_bm:\n",
    "    try:\n",
    "        string = process_string(i[0])\n",
    "        if len(string):\n",
    "            train_labels.append(replace_by[i[1]])\n",
    "            train_texts.append(process_string(i[0])[0])  \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        \n",
    "assert (len(train_texts)==len(train_labels)), \"length texts and labels are not same\"\n",
    "\n",
    "for i in range(len(train_texts)):\n",
    "    train_texts[i] = to_title(train_texts[i].lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 50\n",
    "def iter_seq(x):\n",
    "    return np.array([x[i: i+seq_len] for i in range(0, len(x)-seq_len, 1)])\n",
    "\n",
    "def to_train_seq(*args):\n",
    "    return [iter_seq(x) for x in args]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag.util import untag\n",
    "\n",
    "def features(sentence, index):\n",
    "    \"\"\" sentence: [w1, w2, ...], index: the index of the word \"\"\"\n",
    "    return {\n",
    "        'word': sentence[index],\n",
    "        'is_first': index == 0,\n",
    "        'is_last': index == len(sentence) - 1,\n",
    "        'prefix-1': sentence[index][0],\n",
    "        'prefix-2': sentence[index][:2],\n",
    "        'prefix-3': sentence[index][:3],\n",
    "        'suffix-1': sentence[index][-1],\n",
    "        'suffix-2': sentence[index][-2:],\n",
    "        'suffix-3': sentence[index][-3:],\n",
    "        'prev_word': '' if index == 0 else sentence[index - 1],\n",
    "        'prev_word-prefix-1': '' if index == 0 else sentence[index - 1][0],\n",
    "        'prev_word-prefix-2': '' if index == 0 else sentence[index - 1][:2],\n",
    "        'prev_word-prefix-3': '' if index == 0 else sentence[index - 1][:3],\n",
    "        'prev_word-suffix-1': '' if index == 0 else sentence[index - 1][-1],\n",
    "        'prev_word-suffix-2': '' if index == 0 else sentence[index - 1][-2:],\n",
    "        'prev_word-suffix-3': '' if index == 0 else sentence[index - 1][-3:],\n",
    "        'next_word-prefix-1': '' if index == len(sentence) - 1 else sentence[index + 1][0],\n",
    "        'next_word-prefix-2': '' if index == len(sentence) - 1 else sentence[index + 1][:2],\n",
    "        'next_word-prefix-3': '' if index == len(sentence) - 1 else sentence[index + 1][:3],\n",
    "        'next_word-suffix-1': '' if index == len(sentence) - 1 else sentence[index + 1][-1],\n",
    "        'next_word-suffix-2': '' if index == len(sentence) - 1 else sentence[index + 1][-2:],\n",
    "        'next_word-suffix-3': '' if index == len(sentence) - 1 else sentence[index + 1][-3:],\n",
    "        'next_word': '' if index == len(sentence) - 1 else sentence[index + 1],\n",
    "        'has_hyphen': '-' in sentence[index],\n",
    "        'is_numeric': sentence[index].isdigit(),\n",
    "    }\n",
    "\n",
    "def transform_to_dataset(tagged_sentences):\n",
    "    X, y = [], []\n",
    " \n",
    "    for tagged in tagged_sentences:\n",
    "        X.append([features(untag(tagged), index) for index in range(len(tagged))])\n",
    "        y.append([tag for _, tag in tagged])\n",
    " \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = list(map(lambda X: (X[0],X[1]), list(zip(train_texts,train_labels))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(61767, 50, 2)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_seq = to_train_seq(combined)[0]\n",
    "combined_seq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "X, Y = transform_to_dataset(combined_seq)\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "train_X, test_X, train_Y, test_Y = train_test_split(X, Y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn_crfsuite\n",
    "from sklearn_crfsuite import scorers\n",
    "from sklearn_crfsuite import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8min 38s, sys: 488 ms, total: 8min 38s\n",
      "Wall time: 8min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "crf = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs',\n",
    "    c1=0.1,\n",
    "    c2=0.1,\n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=True\n",
    ")\n",
    "crf.fit(train_X, train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['quantity', 'location', 'time', 'person', 'organization', 'event', 'law']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = list(crf.classes_)\n",
    "labels.remove('OTHER')\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9852734679520555"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = crf.predict(test_X)\n",
    "metrics.flat_f1_score(test_Y, y_pred,\n",
    "                      average='weighted', labels = labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    quantity      0.991     0.991     0.991     13891\n",
      "    location      0.987     0.989     0.988     20798\n",
      "        time      0.987     0.977     0.982     13264\n",
      "      person      0.993     0.987     0.990     43590\n",
      "organization      0.974     0.973     0.973     25426\n",
      "       event      0.995     0.983     0.989      2417\n",
      "         law      0.994     0.988     0.991      1686\n",
      "\n",
      " avg / total      0.987     0.983     0.985    121072\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.flat_classification_report(\n",
    "    test_Y, y_pred, labels=labels, digits=3\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top likely transitions:\n",
      "quantity -> quantity 4.731903\n",
      "location -> location 4.547566\n",
      "organization -> organization 4.322757\n",
      "OTHER  -> OTHER   4.267569\n",
      "event  -> event   3.796581\n",
      "law    -> law     3.234600\n",
      "person -> person  3.178005\n",
      "time   -> time    2.716374\n",
      "location -> OTHER   0.057188\n",
      "OTHER  -> location -0.033477\n",
      "\n",
      "Top unlikely transitions:\n",
      "event  -> person  -4.618084\n",
      "event  -> quantity -4.649371\n",
      "time   -> law     -4.748618\n",
      "organization -> event   -4.763703\n",
      "event  -> location -4.995439\n",
      "organization -> law     -5.343159\n",
      "person -> law     -6.000496\n",
      "time   -> quantity -6.551308\n",
      "organization -> time    -6.602369\n",
      "quantity -> time    -8.047114\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def print_transitions(trans_features):\n",
    "    for (label_from, label_to), weight in trans_features:\n",
    "        print(\"%-6s -> %-7s %0.6f\" % (label_from, label_to, weight))\n",
    "\n",
    "print(\"Top likely transitions:\")\n",
    "print_transitions(Counter(crf.transition_features_).most_common(10))\n",
    "\n",
    "print(\"\\nTop unlikely transitions:\")\n",
    "print_transitions(Counter(crf.transition_features_).most_common()[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top positive:\n",
      "15.295689 person   word:pengarah\n",
      "12.352726 location word:dibuat-buat\n",
      "11.206675 organization word:pas\n",
      "10.718764 person   word:solana\n",
      "10.579257 person   word:anggodo\n",
      "10.205311 location word:kenya\n",
      "10.178896 time     word:jumat\n",
      "10.138113 person   word:terpantas\n",
      "9.938075 OTHER    word:sudah\n",
      "9.896239 location word:pakistan\n",
      "9.884769 location word:sandakan\n",
      "9.582762 organization word:pdi-perjuangan\n",
      "9.488252 organization word:hamas\n",
      "9.469484 person   word:saan\n",
      "9.420459 time     next_word:dihargai\n",
      "9.386450 location word:berlin\n",
      "9.210472 organization next_word:partai-partai\n",
      "9.164932 organization word:mesa\n",
      "9.045899 person   next_word:silahkan\n",
      "9.008229 person   word:berkelulusan\n",
      "8.812708 person   word:dinaungi\n",
      "8.772850 location word:rusia\n",
      "8.771755 organization word:suruhanjaya\n",
      "8.768771 location word:polandia\n",
      "8.729911 event    word:jakarta-palembang\n",
      "8.662130 person   word:johan\n",
      "8.523424 organization word:interpol\n",
      "8.409965 location word:iran\n",
      "8.399465 person   word:dipertuan\n",
      "8.385403 organization word:nasa\n",
      "\n",
      "Top negative:\n",
      "-4.864046 time     prev_word-prefix-2:in\n",
      "-4.975404 OTHER    prev_word:perbendaharaan\n",
      "-5.023681 OTHER    prev_word:sekitar\n",
      "-5.045967 organization is_numeric\n",
      "-5.047438 location next_word:indonesia\n",
      "-5.074802 organization next_word:merupakan\n",
      "-5.086573 OTHER    suffix-3:uni\n",
      "-5.279379 person   suffix-3:ada\n",
      "-5.311407 person   prefix-3:bal\n",
      "-5.313260 person   next_word:meskipun\n",
      "-5.322481 organization prefix-2:fo\n",
      "-5.349961 time     next_word-prefix-3:apa\n",
      "-5.392299 person   next_word:salah\n",
      "-5.435074 OTHER    prev_word:anugerah\n",
      "-5.533284 OTHER    prefix-3:bei\n",
      "-5.548355 organization suffix-2:ni\n",
      "-5.632340 location suffix-3:ian\n",
      "-5.755737 OTHER    word:pengasuhnya\n",
      "-5.841475 organization suffix-3:ada\n",
      "-6.089130 law      next_word-prefix-3:ked\n",
      "-6.265843 OTHER    word:memintanya\n",
      "-6.318719 OTHER    prefix-3:pah\n",
      "-6.365330 time     next_word-suffix-3:nin\n",
      "-6.443976 person   is_numeric\n",
      "-6.508225 event    suffix-1:u\n",
      "-6.535034 OTHER    prefix-3:wir\n",
      "-7.109250 OTHER    prefix-3:di-\n",
      "-7.176552 OTHER    word:ramadan\n",
      "-7.470627 organization suffix-3:ari\n",
      "-7.846867 time     next_word-prefix-1:n\n"
     ]
    }
   ],
   "source": [
    "def print_state_features(state_features):\n",
    "    for (attr, label), weight in state_features:\n",
    "        print(\"%0.6f %-8s %s\" % (weight, label, attr))\n",
    "\n",
    "print(\"Top positive:\")\n",
    "print_state_features(Counter(crf.state_features_).most_common(30))\n",
    "\n",
    "print(\"\\nTop negative:\")\n",
    "print_state_features(Counter(crf.state_features_).most_common()[-30:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = 'KUALA LUMPUR: Sempena sambutan Aidilfitri minggu depan, Perdana Menteri Tun Dr Mahathir Mohamad dan Menteri Pengangkutan Anthony Loke Siew Fook menitipkan pesanan khas kepada orang ramai yang mahu pulang ke kampung halaman masing-masing. Dalam video pendek terbitan Jabatan Keselamatan Jalan Raya (JKJR) itu, Dr Mahathir menasihati mereka supaya berhenti berehat dan tidur sebentar  sekiranya mengantuk ketika memandu.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'kuala lumpur sempena sambutan aidilfitri minggu depan perdana menteri tun dr mahathir mohamad dan menteri pengangkutan anthony loke siew fook menitipkan pesanan khas kepada orang ramai yang mahu pulang ke kampung halaman masing-masing dalam video pendek terbitan jabatan keselamatan jalan raya jkjr itu dr mahathir menasihati mereka supaya berhenti berehat dan tidur sebentar sekiranya mengantuk ketika memandu'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_str = to_title(string.lower())\n",
    "processed_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kuala location\n",
      "lumpur location\n",
      "sempena OTHER\n",
      "sambutan event\n",
      "aidilfitri event\n",
      "minggu OTHER\n",
      "depan OTHER\n",
      "perdana person\n",
      "menteri person\n",
      "tun person\n",
      "dr person\n",
      "mahathir person\n",
      "mohamad person\n",
      "dan OTHER\n",
      "menteri OTHER\n",
      "pengangkutan OTHER\n",
      "anthony person\n",
      "loke person\n",
      "siew person\n",
      "fook person\n",
      "menitipkan OTHER\n",
      "pesanan OTHER\n",
      "khas OTHER\n",
      "kepada OTHER\n",
      "orang OTHER\n",
      "ramai OTHER\n",
      "yang OTHER\n",
      "mahu OTHER\n",
      "pulang OTHER\n",
      "ke OTHER\n",
      "kampung location\n",
      "halaman location\n",
      "masing-masing OTHER\n",
      "dalam OTHER\n",
      "video OTHER\n",
      "pendek OTHER\n",
      "terbitan OTHER\n",
      "jabatan organization\n",
      "keselamatan organization\n",
      "jalan organization\n",
      "raya organization\n",
      "jkjr organization\n",
      "itu OTHER\n",
      "dr person\n",
      "mahathir person\n",
      "menasihati OTHER\n",
      "mereka OTHER\n",
      "supaya OTHER\n",
      "berhenti OTHER\n",
      "berehat OTHER\n",
      "dan OTHER\n",
      "tidur OTHER\n",
      "sebentar OTHER\n",
      "sekiranya OTHER\n",
      "mengantuk OTHER\n",
      "ketika OTHER\n",
      "memandu OTHER\n",
      "CPU times: user 0 ns, sys: 4 ms, total: 4 ms\n",
      "Wall time: 2.77 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "splitted = processed_str.split()\n",
    "test = [features(splitted, index) for index in range(len(splitted))]\n",
    "for no, i in enumerate(crf.predict_single(test)):\n",
    "    print(splitted[no], i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('crf-entities.pkl','wb') as fopen:\n",
    "    pickle.dump(crf,fopen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('crf-entities.pkl', 'rb') as fopen:\n",
    "    test_crf = pickle.load(fopen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kuala location\n",
      "lumpur location\n",
      "sempena OTHER\n",
      "sambutan event\n",
      "aidilfitri event\n",
      "minggu OTHER\n",
      "depan OTHER\n",
      "perdana person\n",
      "menteri person\n",
      "tun person\n",
      "dr person\n",
      "mahathir person\n",
      "mohamad person\n",
      "dan OTHER\n",
      "menteri OTHER\n",
      "pengangkutan OTHER\n",
      "anthony person\n",
      "loke person\n",
      "siew person\n",
      "fook person\n",
      "menitipkan OTHER\n",
      "pesanan OTHER\n",
      "khas OTHER\n",
      "kepada OTHER\n",
      "orang OTHER\n",
      "ramai OTHER\n",
      "yang OTHER\n",
      "mahu OTHER\n",
      "pulang OTHER\n",
      "ke OTHER\n",
      "kampung location\n",
      "halaman location\n",
      "masing-masing OTHER\n",
      "dalam OTHER\n",
      "video OTHER\n",
      "pendek OTHER\n",
      "terbitan OTHER\n",
      "jabatan organization\n",
      "keselamatan organization\n",
      "jalan organization\n",
      "raya organization\n",
      "jkjr organization\n",
      "itu OTHER\n",
      "dr person\n",
      "mahathir person\n",
      "menasihati OTHER\n",
      "mereka OTHER\n",
      "supaya OTHER\n",
      "berhenti OTHER\n",
      "berehat OTHER\n",
      "dan OTHER\n",
      "tidur OTHER\n",
      "sebentar OTHER\n",
      "sekiranya OTHER\n",
      "mengantuk OTHER\n",
      "ketika OTHER\n",
      "memandu OTHER\n"
     ]
    }
   ],
   "source": [
    "for no, i in enumerate(test_crf.predict_single(test)):\n",
    "    print(splitted[no], i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
