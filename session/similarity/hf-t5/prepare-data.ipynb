{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6200042656152798"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "import json\n",
    "import random\n",
    "\n",
    "random.random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "392702it [00:04, 89806.14it/s]\n"
     ]
    }
   ],
   "source": [
    "with open('train.json', 'w') as fopen_jsonl:\n",
    "    with open('translated-mnli-train.jsonl') as fopen:\n",
    "        for l in tqdm(fopen):\n",
    "            data = json.loads(l)\n",
    "            label = data['gold_label']\n",
    "            if label == '-':\n",
    "                continue\n",
    "            sent1 = data['sentence1'].strip()\n",
    "            sent2 = data['sentence2'].strip()\n",
    "            \n",
    "            sent1_ms = data['translate'][0].strip()\n",
    "            sent2_ms = data['translate'][1].strip()\n",
    "            \n",
    "            left = f'ayat1: {sent1} ayat2: {sent2}'\n",
    "            d = {\"src\": left, \"label\": label}\n",
    "            fopen_jsonl.write(f'{json.dumps(d)}\\n')\n",
    "            \n",
    "            left = f'ayat1: {sent1} ayat2: {sent2_ms}'\n",
    "            d = {\"src\": left, \"label\": label}\n",
    "            fopen_jsonl.write(f'{json.dumps(d)}\\n')\n",
    "            \n",
    "            left = f'ayat1: {sent1_ms} ayat2: {sent2_ms}'\n",
    "            d = {\"src\": left, \"label\": label}\n",
    "            fopen_jsonl.write(f'{json.dumps(d)}\\n')\n",
    "            \n",
    "            left = f'ayat1: {sent1_ms} ayat2: {sent2}'\n",
    "            d = {\"src\": left, \"label\": label}\n",
    "            fopen_jsonl.write(f'{json.dumps(d)}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!shuf train.json > shuffled-train.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10000it [00:00, 83295.51it/s]\n"
     ]
    }
   ],
   "source": [
    "with open('test.json', 'w') as fopen_jsonl:\n",
    "    with open('translated-mnli-dev_matched.jsonl') as fopen:\n",
    "        for l in tqdm(fopen):\n",
    "            data = json.loads(l)\n",
    "            label = data['gold_label']\n",
    "            if label == '-':\n",
    "                continue\n",
    "            sent1 = data['sentence1'].strip()\n",
    "            sent2 = data['sentence2'].strip()\n",
    "            \n",
    "            sent1_ms = data['translate'][0].strip()\n",
    "            sent2_ms = data['translate'][1].strip()\n",
    "            \n",
    "            left = f'ayat1: {sent1} ayat2: {sent2}'\n",
    "            d = {\"src\": left, \"label\": label}\n",
    "            fopen_jsonl.write(f'{json.dumps(d)}\\n')\n",
    "            \n",
    "            left = f'ayat1: {sent1} ayat2: {sent2_ms}'\n",
    "            d = {\"src\": left, \"label\": label}\n",
    "            fopen_jsonl.write(f'{json.dumps(d)}\\n')\n",
    "            \n",
    "            left = f'ayat1: {sent1_ms} ayat2: {sent2_ms}'\n",
    "            d = {\"src\": left, \"label\": label}\n",
    "            fopen_jsonl.write(f'{json.dumps(d)}\\n')\n",
    "            \n",
    "            left = f'ayat1: {sent1_ms} ayat2: {sent2}'\n",
    "            d = {\"src\": left, \"label\": label}\n",
    "            fopen_jsonl.write(f'{json.dumps(d)}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "!shuf test.json > shuffled-test.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n 1000 shuffled-test.json > test-1k.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = {\"train\": 'train.json', \"validation\": 'train.json'}\n",
    "raw_datasets = load_dataset(\n",
    "    \"json\",\n",
    "    data_files=data_files,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = raw_datasets[\"train\"].unique(\"label\")\n",
    "label_list.sort()  # Let's sort it for determinism\n",
    "num_labels = len(label_list)\n",
    "label_to_id = {v: i for i, v in enumerate(label_list)}\n",
    "label_to_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Model, T5Tokenizer, T5PreTrainedModel, T5Config, AutoConfig, AutoModelForSequenceClassification\n",
    "from transformers.models.bart.modeling_bart import shift_tokens_right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(\"mesolitica/t5-super-tiny-bahasa-cased\")\n",
    "config = T5Config.from_pretrained(\"mesolitica/t5-super-tiny-bahasa-cased\",\n",
    "                                  num_labels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    inputs = [ex for ex in examples['src']]\n",
    "\n",
    "    result = tokenizer(inputs, max_length=256, padding='longest', truncation=True)\n",
    "    # Map labels to IDs (not necessary for GLUE tasks)\n",
    "    if label_to_id is not None and \"label\" in examples:\n",
    "        result[\"label\"] = [(label_to_id[ex] if ex != -1 else -1) for ex in examples['label']]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets = raw_datasets.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
