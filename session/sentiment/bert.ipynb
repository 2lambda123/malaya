{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bert_model as modeling\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import collections\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from unidecode import unidecode\n",
    "from tqdm import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules_normalizer = {\n",
    "    'experience': 'pengalaman',\n",
    "    'bagasi': 'bagasi',\n",
    "    'kg': 'kampung',\n",
    "    'kilo': 'kilogram',\n",
    "    'g': 'gram',\n",
    "    'grm': 'gram',\n",
    "    'k': 'okay',\n",
    "    'abgkat': 'abang dekat',\n",
    "    'abis': 'habis',\n",
    "    'ade': 'ada',\n",
    "    'adoi': 'aduh',\n",
    "    'adoii': 'aduhh',\n",
    "    'aerodarat': 'kapal darat',\n",
    "    'agkt': 'angkat',\n",
    "    'ahh': 'ah',\n",
    "    'ailior': 'air liur',\n",
    "    'airasia': 'air asia x',\n",
    "    'airasiax': 'penerbangan',\n",
    "    'airline': 'penerbangan',\n",
    "    'airlines': 'penerbangan',\n",
    "    'airport': 'lapangan terbang',\n",
    "    'airpot': 'lapangan terbang',\n",
    "    'aje': 'sahaja',\n",
    "    'ajelah': 'sahajalah',\n",
    "    'ajer': 'sahaja',\n",
    "    'ak': 'aku',\n",
    "    'aq': 'aku',\n",
    "    'all': 'semua',\n",
    "    'ambik': 'ambil',\n",
    "    'amek': 'ambil',\n",
    "    'amer': 'amir',\n",
    "    'amik': 'ambil',\n",
    "    'ana': 'saya',\n",
    "    'angkt': 'angkat',\n",
    "    'anual': 'tahunan',\n",
    "    'apapun': 'apa pun',\n",
    "    'ape': 'apa',\n",
    "    'arab': 'arab',\n",
    "    'area': 'kawasan',\n",
    "    'aritu': 'hari itu',\n",
    "    'ask': 'tanya',\n",
    "    'astro': 'astro',\n",
    "    'at': 'pada',\n",
    "    'attitude': 'sikap',\n",
    "    'babi': 'khinzir',\n",
    "    'back': 'belakang',\n",
    "    'bag': 'beg',\n",
    "    'bang': 'abang',\n",
    "    'bangla': 'bangladesh',\n",
    "    'banyk': 'banyak',\n",
    "    'bard': 'pujangga',\n",
    "    'bargasi': 'bagasi',\n",
    "    'bawak': 'bawa',\n",
    "    'bawanges': 'bawang',\n",
    "    'be': 'jadi',\n",
    "    'behave': 'berkelakuan baik',\n",
    "    'belagak': 'berlagak',\n",
    "    'berdisiplin': 'berdisplin',\n",
    "    'berenti': 'berhenti',\n",
    "    'beskal': 'basikal',\n",
    "    'bff': 'rakan karib',\n",
    "    'bg': 'bagi',\n",
    "    'bgi': 'bagi',\n",
    "    'biase': 'biasa',\n",
    "    'big': 'besar',\n",
    "    'bike': 'basikal',\n",
    "    'bile': 'bila',\n",
    "    'binawe': 'binatang',\n",
    "    'bini': 'isteri',\n",
    "    'bkn': 'bukan',\n",
    "    'bla': 'bila',\n",
    "    'blom': 'belum',\n",
    "    'bnyak': 'banyak',\n",
    "    'body': 'tubuh',\n",
    "    'bole': 'boleh',\n",
    "    'boss': 'bos',\n",
    "    'bowling': 'boling',\n",
    "    'bpe': 'berapa',\n",
    "    'brand': 'jenama',\n",
    "    'brg': 'barang',\n",
    "    'briefing': 'taklimat',\n",
    "    'brng': 'barang',\n",
    "    'bro': 'abang',\n",
    "    'bru': 'baru',\n",
    "    'bruntung': 'beruntung',\n",
    "    'bsikal': 'basikal',\n",
    "    'btnggjwb': 'bertanggungjawab',\n",
    "    'btul': 'betul',\n",
    "    'buatlh': 'buatlah',\n",
    "    'buh': 'letak',\n",
    "    'buka': 'buka',\n",
    "    'but': 'tetapi',\n",
    "    'bwk': 'bawa',\n",
    "    'by': 'dengan',\n",
    "    'byr': 'bayar',\n",
    "    'bz': 'sibuk',\n",
    "    'camera': 'kamera',\n",
    "    'camni': 'macam ini',\n",
    "    'cane': 'macam mana',\n",
    "    'cant': 'tak boleh',\n",
    "    'carakerja': 'cara kerja',\n",
    "    'care': 'jaga',\n",
    "    'cargo': 'kargo',\n",
    "    'cctv': 'kamera litar tertutup',\n",
    "    'celako': 'celaka',\n",
    "    'cer': 'cerita',\n",
    "    'cheap': 'murah',\n",
    "    'check': 'semak',\n",
    "    'ciput': 'sedikit',\n",
    "    'cite': 'cerita',\n",
    "    'citer': 'cerita',\n",
    "    'ckit': 'sikit',\n",
    "    'ckp': 'cakap',\n",
    "    'class': 'kelas',\n",
    "    'cm': 'macam',\n",
    "    'cmni': 'macam ini',\n",
    "    'cmpak': 'campak',\n",
    "    'committed': 'komited',\n",
    "    'company': 'syarikat',\n",
    "    'complain': 'aduan',\n",
    "    'corn': 'jagung',\n",
    "    'couldnt': 'tak boleh',\n",
    "    'cr': 'cari',\n",
    "    'crew': 'krew',\n",
    "    'cube': 'cuba',\n",
    "    'cuma': 'cuma',\n",
    "    'curinyaa': 'curinya',\n",
    "    'cust': 'pelanggan',\n",
    "    'customer': 'pelanggan',\n",
    "    'd': 'di',\n",
    "    'da': 'dah',\n",
    "    'dn': 'dan',\n",
    "    'dahh': 'dah',\n",
    "    'damaged': 'rosak',\n",
    "    'dapek': 'dapat',\n",
    "    'day': 'hari',\n",
    "    'dazrin': 'dazrin',\n",
    "    'dbalingnya': 'dibalingnya',\n",
    "    'de': 'ada',\n",
    "    'deep': 'dalam',\n",
    "    'deliberately': 'sengaja',\n",
    "    'depa': 'mereka',\n",
    "    'dessa': 'desa',\n",
    "    'dgn': 'dengan',\n",
    "    'dh': 'dah',\n",
    "    'didunia': 'di dunia',\n",
    "    'diorang': 'mereka',\n",
    "    'diorng': 'mereka',\n",
    "    'direct': 'secara terus',\n",
    "    'diving': 'junam',\n",
    "    'dkt': 'dekat',\n",
    "    'dlempar': 'dilempar',\n",
    "    'dlm': 'dalam',\n",
    "    'dlt': 'padam',\n",
    "    'dlu': 'dulu',\n",
    "    'done': 'siap',\n",
    "    'dont': 'jangan',\n",
    "    'dorg': 'mereka',\n",
    "    'dpermudhkn': 'dipermudahkan',\n",
    "    'dpt': 'dapat',\n",
    "    'dr': 'dari',\n",
    "    'dri': 'dari',\n",
    "    'dsb': 'dan sebagainya',\n",
    "    'dy': 'dia',\n",
    "    'educate': 'mendidik',\n",
    "    'ensure': 'memastikan',\n",
    "    'everything': 'semua',\n",
    "    'ewahh': 'wah',\n",
    "    'expect': 'sangka',\n",
    "    'fb': 'facebook',\n",
    "    'fired': 'pecat',\n",
    "    'first': 'pertama',\n",
    "    'fkr': 'fikir',\n",
    "    'flight': 'kapal terbang',\n",
    "    'for': 'untuk',\n",
    "    'free': 'percuma',\n",
    "    'friend': 'kawan',\n",
    "    'fyi': 'untuk pengetahuan anda',\n",
    "    'gantila': 'gantilah',\n",
    "    'gantirugi': 'ganti rugi',\n",
    "    'gentlemen': 'lelaki budiman',\n",
    "    'gerenti': 'jaminan',\n",
    "    'gile': 'gila',\n",
    "    'gk': 'juga',\n",
    "    'gnti': 'ganti',\n",
    "    'go': 'pergi',\n",
    "    'gomen': 'kerajaan',\n",
    "    'goment': 'kerajaan',\n",
    "    'good': 'baik',\n",
    "    'ground': 'tanah',\n",
    "    'guarno': 'macam mana',\n",
    "    'hampa': 'mereka',\n",
    "    'hampeh': 'teruk',\n",
    "    'hanat': 'jahanam',\n",
    "    'handle': 'kawal',\n",
    "    'handling': 'kawalan',\n",
    "    'hanta': 'hantar',\n",
    "    'haritu': 'hari itu',\n",
    "    'hate': 'benci',\n",
    "    'have': 'ada',\n",
    "    'hawau': 'celaka',\n",
    "    'henpon': 'telefon',\n",
    "    'heran': 'hairan',\n",
    "    'him': 'dia',\n",
    "    'his': 'dia',\n",
    "    'hmpa': 'mereka',\n",
    "    'hntr': 'hantar',\n",
    "    'hotak': 'otak',\n",
    "    'hr': 'hari',\n",
    "    'i': 'saya',\n",
    "    'hrga': 'harga',\n",
    "    'hrp': 'harap',\n",
    "    'hu': 'sedih',\n",
    "    'humble': 'merendah diri',\n",
    "    'ibon': 'ikon',\n",
    "    'ichi': 'inci',\n",
    "    'idung': 'hidung',\n",
    "    'if': 'jika',\n",
    "    'ig': 'instagram',\n",
    "    'iklas': 'ikhlas',\n",
    "    'improve': 'menambah baik',\n",
    "    'in': 'masuk',\n",
    "    'isn t': 'tidak',\n",
    "    'isyaallah': 'insyallah',\n",
    "    'ja': 'sahaja',\n",
    "    'japan': 'jepun',\n",
    "    'jd': 'jadi',\n",
    "    'je': 'saja',\n",
    "    'jee': 'saja',\n",
    "    'jek': 'saja',\n",
    "    'jepun': 'jepun',\n",
    "    'jer': 'saja',\n",
    "    'jerr': 'saja',\n",
    "    'jez': 'saja',\n",
    "    'jg': 'juga',\n",
    "    'jgk': 'juga',\n",
    "    'jgn': 'jangan',\n",
    "    'jgnla': 'janganlah',\n",
    "    'jibake': 'celaka',\n",
    "    'jjur': 'jujur',\n",
    "    'job': 'kerja',\n",
    "    'jobscope': 'skop kerja',\n",
    "    'jogja': 'jogjakarta',\n",
    "    'jpam': 'jpam',\n",
    "    'jth': 'jatuh',\n",
    "    'jugak': 'juga',\n",
    "    'ka': 'ke',\n",
    "    'kalo': 'kalau',\n",
    "    'kalu': 'kalau',\n",
    "    'kang': 'nanti',\n",
    "    'kantoi': 'temberang',\n",
    "    'kasi': 'beri',\n",
    "    'kat': 'dekat',\n",
    "    'kbye': 'ok bye',\n",
    "    'kearah': 'ke arah',\n",
    "    'kecik': 'kecil',\n",
    "    'keja': 'kerja',\n",
    "    'keje': 'kerja',\n",
    "    'kejo': 'kerja',\n",
    "    'keksongan': 'kekosongan',\n",
    "    'kemana': 'ke mana',\n",
    "    'kene': 'kena',\n",
    "    'kenekan': 'kenakan',\n",
    "    'kesah': 'kisah',\n",
    "    'ketempat': 'ke tempat',\n",
    "    'kije': 'kerja',\n",
    "    'kijo': 'kerja',\n",
    "    'kiss': 'cium',\n",
    "    'kite': 'kita',\n",
    "    'kito': 'kita',\n",
    "    'kje': 'kerja',\n",
    "    'kjr': 'kerja',\n",
    "    'kk': 'okay',\n",
    "    'kmi': 'kami',\n",
    "    'kt': 'kat',\n",
    "    'tlg': 'tolong',\n",
    "    'kl': 'kuala lumpur',\n",
    "    'klai': 'kalau',\n",
    "    'klau': 'kalau',\n",
    "    'klia': 'klia',\n",
    "    'klo': 'kalau',\n",
    "    'klu': 'kalau',\n",
    "    'kn': 'kan',\n",
    "    'knapa': 'kenapa',\n",
    "    'kne': 'kena',\n",
    "    'ko': 'kau',\n",
    "    'kompom': 'sah',\n",
    "    'korang': 'kamu semua',\n",
    "    'korea': 'korea',\n",
    "    'korg': 'kamu semua',\n",
    "    'kot': 'mungkin',\n",
    "    'krja': 'kerja',\n",
    "    'ksalahan': 'kesalahan',\n",
    "    'kta': 'kita',\n",
    "    'kuar': 'keluar',\n",
    "    'kut': 'mungkin',\n",
    "    'la': 'lah',\n",
    "    'laa': 'lah',\n",
    "    'lahabau': 'celaka',\n",
    "    'lahanat': 'celaka',\n",
    "    'lainda': 'lain dah',\n",
    "    'lak': 'pula',\n",
    "    'last': 'akhir',\n",
    "    'le': 'lah',\n",
    "    'leader': 'ketua',\n",
    "    'leave': 'pergi',\n",
    "    'ler': 'lah',\n",
    "    'less': 'kurang',\n",
    "    'letter': 'surat',\n",
    "    'lg': 'lagi',\n",
    "    'lgi': 'lagi',\n",
    "    'lngsong': 'langsung',\n",
    "    'lol': 'hehe',\n",
    "    'lorr': 'lah',\n",
    "    'low': 'rendah',\n",
    "    'lps': 'lepas',\n",
    "    'luggage': 'bagasi',\n",
    "    'lumbe': 'lumba',\n",
    "    'lyak': 'layak',\n",
    "    'maap': 'maaf',\n",
    "    'maapkan': 'maafkan',\n",
    "    'mahai': 'mahal',\n",
    "    'mampos': 'mampus',\n",
    "    'mart': 'kedai',\n",
    "    'mau': 'mahu',\n",
    "    'mcm': 'macam',\n",
    "    'mcmtu': 'macam itu',\n",
    "    'memerlukn': 'memerlukan',\n",
    "    'mengembirakan': 'menggembirakan',\n",
    "    'mengmbilnyer': 'mengambilnya',\n",
    "    'mengtasi': 'mengatasi',\n",
    "    'mg': 'memang',\n",
    "    'mihak': 'memihak',\n",
    "    'min': 'admin',\n",
    "    'mingu': 'minggu',\n",
    "    'mintak': 'minta',\n",
    "    'mjtuhkn': 'menjatuhkan',\n",
    "    'mkyong': 'mak yong',\n",
    "    'mlibatkn': 'melibatkan',\n",
    "    'mmg': 'memang',\n",
    "    'mmnjang': 'memanjang',\n",
    "    'mmpos': 'mampus',\n",
    "    'mn': 'mana',\n",
    "    'mna': 'mana',\n",
    "    'mntak': 'minta',\n",
    "    'mntk': 'minta',\n",
    "    'mnyusun': 'menyusun',\n",
    "    'mood': 'suasana',\n",
    "    'most': 'paling',\n",
    "    'mr': 'tuan',\n",
    "    'msa': 'masa',\n",
    "    'msia': 'malaysia',\n",
    "    'mst': 'mesti',\n",
    "    'mu': 'awak',\n",
    "    'much': 'banyak',\n",
    "    'muko': 'muka',\n",
    "    'mum': 'emak',\n",
    "    'n': 'dan',\n",
    "    'nah': 'nah',\n",
    "    'nanny': 'nenek',\n",
    "    'napo': 'kenapa',\n",
    "    'nati': 'nanti',\n",
    "    'ngan': 'dengan',\n",
    "    'ngn': 'dengan',\n",
    "    'ni': 'ini',\n",
    "    'nie': 'ini',\n",
    "    'nii': 'ini',\n",
    "    'nk': 'nak',\n",
    "    'nmpk': 'nampak',\n",
    "    'nye': 'nya',\n",
    "    'ofis': 'pejabat',\n",
    "    'ohh': 'oh',\n",
    "    'oii': 'hoi',\n",
    "    'one': 'satu',\n",
    "    'online': 'dalam talian',\n",
    "    'or': 'atau',\n",
    "    'org': 'orang',\n",
    "    'orng': 'orang',\n",
    "    'otek': 'otak',\n",
    "    'p': 'pergi',\n",
    "    'paid': 'dah bayar',\n",
    "    'palabana': 'kepala otak',\n",
    "    'pasni': 'lepas ini',\n",
    "    'passengers': 'penumpang',\n",
    "    'passengger': 'penumpang',\n",
    "    'pastu': 'lepas itu',\n",
    "    'pd': 'pada',\n",
    "    'pegi': 'pergi',\n",
    "    'pekerje': 'pekerja',\n",
    "    'pekrja': 'pekerja',\n",
    "    'perabih': 'perabis',\n",
    "    'perkerja': 'pekerja',\n",
    "    'pg': 'pergi',\n",
    "    'phuii': 'puih',\n",
    "    'pikir': 'fikir',\n",
    "    'pilot': 'juruterbang',\n",
    "    'pk': 'fikir',\n",
    "    'pkerja': 'pekerja',\n",
    "    'pkerjaan': 'pekerjaan',\n",
    "    'pki': 'pakai',\n",
    "    'please': 'tolong',\n",
    "    'pls': 'tolong',\n",
    "    'pn': 'pun',\n",
    "    'pnh': 'pernah',\n",
    "    'pnt': 'penat',\n",
    "    'pnya': 'punya',\n",
    "    'pon': 'pun',\n",
    "    'priority': 'keutamaan',\n",
    "    'properties': 'harta benda',\n",
    "    'ptugas': 'petugas',\n",
    "    'pub': 'kelab malam',\n",
    "    'pulak': 'pula',\n",
    "    'puye': 'punya',\n",
    "    'pwrcuma': 'percuma',\n",
    "    'pyahnya': 'payahnya',\n",
    "    'quality': 'kualiti',\n",
    "    'quit': 'keluar',\n",
    "    'ramly': 'ramly',\n",
    "    'rege': 'harga',\n",
    "    'reger': 'harga',\n",
    "    'report': 'laporan',\n",
    "    'resigned': 'meletakkan jawatan',\n",
    "    'respect': 'hormat',\n",
    "    'rizal': 'rizal',\n",
    "    'rosak': 'rosak',\n",
    "    'rosok': 'rosak',\n",
    "    'rse': 'rasa',\n",
    "    'sacked': 'buang',\n",
    "    'sado': 'tegap',\n",
    "    'salute': 'sanjung',\n",
    "    'sam': 'sama',\n",
    "    'same': 'sama',\n",
    "    'samp': 'sampah',\n",
    "    'sbb': 'sebab',\n",
    "    'sbgai': 'sebagai',\n",
    "    'sblm': 'sebelum',\n",
    "    'sblum': 'sebelum',\n",
    "    'sbnarnya': 'sebenarnya',\n",
    "    'sbum': 'sebelum',\n",
    "    'sdg': 'sedang',\n",
    "    'sebb': 'sebab',\n",
    "    'sebijik': 'sebiji',\n",
    "    'see': 'lihat',\n",
    "    'seen': 'dilihat',\n",
    "    'selangor': 'selangor',\n",
    "    'selfie': 'swafoto',\n",
    "    'sempoi': 'cantik',\n",
    "    'senaraihitam': 'senarai hitam',\n",
    "    'seorg': 'seorang',\n",
    "    'service': 'perkhidmatan',\n",
    "    'sgt': 'sangat',\n",
    "    'shared': 'kongsi',\n",
    "    'shirt': 'kemeja',\n",
    "    'shut': 'tutup',\n",
    "    'sib': 'nasib',\n",
    "    'skali': 'sekali',\n",
    "    'sket': 'sikit',\n",
    "    'sma': 'sama',\n",
    "    'smoga': 'semoga',\n",
    "    'smpoi': 'cantik',\n",
    "    'sndiri': 'sendiri',\n",
    "    'sndr': 'sendiri',\n",
    "    'sndri': 'sendiri',\n",
    "    'sne': 'sana',\n",
    "    'so': 'jadi',\n",
    "    'sop': 'tatacara pengendalian piawai',\n",
    "    'sorang': 'seorang',\n",
    "    'spoting': 'pembintikan',\n",
    "    'sronok': 'seronok',\n",
    "    'ssh': 'susah',\n",
    "    'staff': 'staf',\n",
    "    'standing': 'berdiri',\n",
    "    'start': 'mula',\n",
    "    'steady': 'mantap',\n",
    "    'stiap': 'setiap',\n",
    "    'stress': 'stres',\n",
    "    'student': 'pelajar',\n",
    "    'study': 'belajar',\n",
    "    'studycase': 'kajian kes',\n",
    "    'sure': 'pasti',\n",
    "    'sykt': 'syarikat',\n",
    "    'tah': 'entah',\n",
    "    'taik': 'tahi',\n",
    "    'takan': 'tak akan',\n",
    "    'takat': 'setakat',\n",
    "    'takde': 'tak ada',\n",
    "    'takkan': 'tak akan',\n",
    "    'taknak': 'tak nak',\n",
    "    'tang': 'tentang',\n",
    "    'tanggungjawab': 'bertanggungjawab',\n",
    "    'taraa': 'sementara',\n",
    "    'tau': 'tahu',\n",
    "    'tbabit': 'terbabit',\n",
    "    'team': 'pasukan',\n",
    "    'terbaekk': 'terbaik',\n",
    "    'teruknye': 'teruknya',\n",
    "    'tgk': 'tengok',\n",
    "    'that': 'itu',\n",
    "    'thinking': 'fikir',\n",
    "    'those': 'itu',\n",
    "    'time': 'masa',\n",
    "    'tk': 'tak',\n",
    "    'tnggongjwb': 'tanggungjawab',\n",
    "    'tngok': 'tengok',\n",
    "    'tngu': 'tunggu',\n",
    "    'to': 'kepada',\n",
    "    'tosak': 'rosak',\n",
    "    'tp': 'tapi',\n",
    "    'tpi': 'tapi',\n",
    "    'tpon': 'telefon',\n",
    "    'transfer': 'pindah',\n",
    "    'trgelak': 'tergelak',\n",
    "    'ts': 'tan sri',\n",
    "    'tstony': 'tan sri tony',\n",
    "    'tu': 'itu',\n",
    "    'tuh': 'itu',\n",
    "    'tula': 'itulah',\n",
    "    'umeno': 'umno',\n",
    "    'unfortunately': 'malangnya',\n",
    "    'unhappy': 'tidak gembira',\n",
    "    'up': 'naik',\n",
    "    'upkan': 'naikkan',\n",
    "    'ur': 'awak',\n",
    "    'utk': 'untuk',\n",
    "    'very': 'sangat',\n",
    "    'viral': 'tular',\n",
    "    'vote': 'undi',\n",
    "    'warning': 'amaran',\n",
    "    'warranty': 'waranti',\n",
    "    'wassap': 'whatsapp',\n",
    "    'wat': 'apa',\n",
    "    'weii': 'wei',\n",
    "    'well': 'maklumlah',\n",
    "    'win': 'menang',\n",
    "    'with': 'dengan',\n",
    "    'wt': 'buat',\n",
    "    'x': 'tak',\n",
    "    'tw': 'tahu',\n",
    "    'ye': 'ya',\n",
    "    'yee': 'ya',\n",
    "    'yg': 'yang',\n",
    "    'yng': 'yang',\n",
    "    'you': 'awak',\n",
    "    'your': 'awak',\n",
    "    'sakai': 'selekeh',\n",
    "    'rmb': 'billion ringgit',\n",
    "    'rmj': 'juta ringgit',\n",
    "    'rmk': 'ribu ringgit',\n",
    "    'rm': 'ringgit',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "permulaan = [\n",
    "    'bel',\n",
    "    'se',\n",
    "    'ter',\n",
    "    'men',\n",
    "    'meng',\n",
    "    'mem',\n",
    "    'memper',\n",
    "    'di',\n",
    "    'pe',\n",
    "    'me',\n",
    "    'ke',\n",
    "    'ber',\n",
    "    'pen',\n",
    "    'per',\n",
    "]\n",
    "\n",
    "hujung = ['kan', 'kah', 'lah', 'tah', 'nya', 'an', 'wan', 'wati', 'ita']\n",
    "\n",
    "def naive_stemmer(word):\n",
    "    assert isinstance(word, str), 'input must be a string'\n",
    "    hujung_result = [e for e in hujung if word.endswith(e)]\n",
    "    if len(hujung_result):\n",
    "        hujung_result = max(hujung_result, key = len)\n",
    "        if len(hujung_result):\n",
    "            word = word[: -len(hujung_result)]\n",
    "    permulaan_result = [e for e in permulaan if word.startswith(e)]\n",
    "    if len(permulaan_result):\n",
    "        permulaan_result = max(permulaan_result, key = len)\n",
    "        if len(permulaan_result):\n",
    "            word = word[len(permulaan_result) :]\n",
    "    return word\n",
    "\n",
    "def build_dataset(words, n_words):\n",
    "    count = [['GO', 0], ['PAD', 1], ['EOS', 2], ['UNK', 3]]\n",
    "    counter = collections.Counter(words).most_common(n_words)\n",
    "    count.extend(counter)\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    data = list()\n",
    "    unk_count = 0\n",
    "    for word in words:\n",
    "        index = dictionary.get(word, 3)\n",
    "        if index == 0:\n",
    "            unk_count += 1\n",
    "        data.append(index)\n",
    "    count[0][1] = unk_count\n",
    "    reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return data, count, dictionary, reversed_dictionary\n",
    "\n",
    "\n",
    "def classification_textcleaning(string):\n",
    "    string = re.sub(\n",
    "        'http\\S+|www.\\S+',\n",
    "        '',\n",
    "        ' '.join(\n",
    "            [i for i in string.split() if i.find('#') < 0 and i.find('@') < 0]\n",
    "        ),\n",
    "    )\n",
    "    string = unidecode(string).replace('.', ' . ').replace(',', ' , ')\n",
    "    string = re.sub('[^A-Za-z ]+', ' ', string)\n",
    "    string = re.sub(r'[ ]+', ' ', string.lower()).strip()\n",
    "    string = [rules_normalizer.get(w, w) for w in string.split()]\n",
    "    string = [naive_stemmer(word) for word in string]\n",
    "    return ' '.join([word for word in string if len(word) > 1])\n",
    "\n",
    "\n",
    "def str_idx(corpus, dic, maxlen, UNK = 3):\n",
    "    X = np.zeros((len(corpus), maxlen))\n",
    "    for i in range(len(corpus)):\n",
    "        for no, k in enumerate(corpus[i].split()[:maxlen][::-1]):\n",
    "            X[i, -1 - no] = dic.get(k, UNK)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'raja benar sangat benci rakyat minyak naik gala'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_textcleaning('kerajaan sebenarnya sangat bencikan rakyatnya, minyak naik dan segalanya')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('sentiment-data-v2.csv')\n",
    "Y = LabelEncoder().fit_transform(df.label)\n",
    "with open('polarity-negative-translated.txt','r') as fopen:\n",
    "    texts = fopen.read().split('\\n')\n",
    "labels = [0] * len(texts)\n",
    "\n",
    "with open('polarity-positive-translated.txt','r') as fopen:\n",
    "    positive_texts = fopen.read().split('\\n')\n",
    "labels += [1] * len(positive_texts)\n",
    "texts += positive_texts\n",
    "texts += df.iloc[:,1].tolist()\n",
    "labels += Y.tolist()\n",
    "\n",
    "assert len(labels) == len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('bm-amazon.json') as fopen:\n",
    "    amazon = json.load(fopen)\n",
    "    \n",
    "with open('bm-imdb.json') as fopen:\n",
    "    imdb = json.load(fopen)\n",
    "    \n",
    "with open('bm-yelp.json') as fopen:\n",
    "    yelp = json.load(fopen)\n",
    "    \n",
    "texts += amazon['negative']\n",
    "labels += [0] * len(amazon['negative'])\n",
    "texts += amazon['positive']\n",
    "labels += [1] * len(amazon['positive'])\n",
    "\n",
    "texts += imdb['negative']\n",
    "labels += [0] * len(imdb['negative'])\n",
    "texts += imdb['positive']\n",
    "labels += [1] * len(imdb['positive'])\n",
    "\n",
    "texts += yelp['negative']\n",
    "labels += [0] * len(yelp['negative'])\n",
    "texts += yelp['positive']\n",
    "labels += [1] * len(yelp['positive'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "for i in [i for i in os.listdir('negative') if 'Store' not in i]:\n",
    "    with open('negative/'+i) as fopen:\n",
    "        a = json.load(fopen)\n",
    "        texts += a\n",
    "        labels += [0] * len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "for i in [i for i in os.listdir('positive') if 'Store' not in i]:\n",
    "    with open('positive/'+i) as fopen:\n",
    "        a = json.load(fopen)\n",
    "        texts += a\n",
    "        labels += [1] * len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(texts)):\n",
    "    texts[i] = classification_textcleaning(texts[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab from size: 120097\n",
      "Most common words [('saya', 533028), ('yang', 204446), ('tidak', 164296), ('untuk', 129707), ('anda', 126091), ('hari', 88975)]\n",
      "Sample data [2667, 229, 363, 235, 235, 94, 1357, 5, 78, 678] ['ringkas', 'bodoh', 'bosan', 'kanak', 'kanak', 'lelaki', 'remaja', 'yang', 'begitu', 'muda']\n"
     ]
    }
   ],
   "source": [
    "concat = ' '.join(texts).split()\n",
    "vocabulary_size = len(list(set(concat)))\n",
    "data, count, dictionary, rev_dictionary = build_dataset(concat, vocabulary_size)\n",
    "print('vocab from size: %d'%(vocabulary_size))\n",
    "print('Most common words', count[4:10])\n",
    "print('Sample data', data[:10], [rev_dictionary[i] for i in data[:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_layer = 256\n",
    "num_layers = 2\n",
    "embedded_size = 256\n",
    "dimension_output = len(np.unique(labels))\n",
    "learning_rate = 5e-4\n",
    "maxlen = 80\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "bert_config = modeling.BertConfig(\n",
    "    vocab_size = len(dictionary),\n",
    "    hidden_size = size_layer,\n",
    "    num_hidden_layers = num_layers,\n",
    "    num_attention_heads = size_layer // 4,\n",
    "    intermediate_size = size_layer * 2,\n",
    ")\n",
    "\n",
    "input_ids = tf.placeholder(tf.int32, [None, maxlen], name = 'Placeholder_input_ids')\n",
    "input_mask = tf.placeholder(tf.int32, [None, maxlen], name = 'Placeholder_input_mask')\n",
    "segment_ids = tf.placeholder(tf.int32, [None, maxlen], name = 'Placeholder_segment_ids')\n",
    "label_ids = tf.placeholder(tf.int32, [None], name = 'Placeholder_label_ids')\n",
    "is_training = tf.placeholder(tf.bool, name = 'Placeholder_is_training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(\n",
    "    bert_config,\n",
    "    is_training,\n",
    "    input_ids,\n",
    "    input_mask,\n",
    "    segment_ids,\n",
    "    labels,\n",
    "    num_labels,\n",
    "    use_one_hot_embeddings,\n",
    "    reuse_flag = False,\n",
    "):\n",
    "    model = modeling.BertModel(\n",
    "        config = bert_config,\n",
    "        is_training = is_training,\n",
    "        input_ids = input_ids,\n",
    "        input_mask = input_mask,\n",
    "        token_type_ids = segment_ids,\n",
    "        use_one_hot_embeddings = use_one_hot_embeddings,\n",
    "    )\n",
    "\n",
    "    output_layer = model.get_pooled_output()\n",
    "    hidden_size = output_layer.shape[-1].value\n",
    "    with tf.variable_scope('weights', reuse = reuse_flag):\n",
    "        output_weights = tf.get_variable(\n",
    "            'output_weights',\n",
    "            [num_labels, hidden_size],\n",
    "            initializer = tf.truncated_normal_initializer(stddev = 0.02),\n",
    "        )\n",
    "        output_bias = tf.get_variable(\n",
    "            'output_bias', [num_labels], initializer = tf.zeros_initializer()\n",
    "        )\n",
    "\n",
    "    with tf.variable_scope('loss'):\n",
    "        def apply_dropout_last_layer(output_layer):\n",
    "            output_layer = tf.nn.dropout(output_layer, keep_prob = 0.9)\n",
    "            return output_layer\n",
    "\n",
    "        def not_apply_dropout(output_layer):\n",
    "            return output_layer\n",
    "\n",
    "        output_layer = tf.cond(\n",
    "            is_training,\n",
    "            lambda: apply_dropout_last_layer(output_layer),\n",
    "            lambda: not_apply_dropout(output_layer),\n",
    "        )\n",
    "        logits = tf.matmul(output_layer, output_weights, transpose_b = True)\n",
    "        print(\n",
    "            'output_layer:',\n",
    "            output_layer.shape,\n",
    "            ', output_weights:',\n",
    "            output_weights.shape,\n",
    "            ', logits:',\n",
    "            logits.shape,\n",
    "        )\n",
    "\n",
    "        logits = tf.nn.bias_add(logits, output_bias, name = 'logits')\n",
    "        probabilities = tf.nn.softmax(logits)\n",
    "        loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            labels = labels, logits = logits\n",
    "        )\n",
    "        loss = tf.reduce_mean(loss)\n",
    "        correct_pred = tf.equal(tf.argmax(logits, 1, output_type = tf.int32), labels)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "        return loss, logits, probabilities, model, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_layer: (?, 256) , output_weights: (2, 256) , logits: (?, 2)\n"
     ]
    }
   ],
   "source": [
    "use_one_hot_embeddings = False\n",
    "loss, logits, probabilities, model, accuracy = create_model(\n",
    "    bert_config,\n",
    "    is_training,\n",
    "    input_ids,\n",
    "    input_mask,\n",
    "    segment_ids,\n",
    "    label_ids,\n",
    "    dimension_output,\n",
    "    use_one_hot_embeddings,\n",
    ")\n",
    "global_step = tf.Variable(0, trainable = False, name = 'Global_Step')\n",
    "optimizer = tf.contrib.layers.optimize_loss(\n",
    "    loss,\n",
    "    global_step = global_step,\n",
    "    learning_rate = learning_rate,\n",
    "    optimizer = 'Adam'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "strings = ','.join(\n",
    "    [\n",
    "        n.name\n",
    "        for n in tf.get_default_graph().as_graph_def().node\n",
    "        if ('Variable' in n.op\n",
    "        or 'Placeholder' in n.name\n",
    "        or 'logits' in n.name\n",
    "        or 'alphas' in n.name)\n",
    "        and 'Adam' not in n.name\n",
    "        and 'beta' not in n.name\n",
    "        and 'OptimizeLoss' not in n.name\n",
    "        and 'Global_Step' not in n.name\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Placeholder_input_ids',\n",
       " 'Placeholder_input_mask',\n",
       " 'Placeholder_segment_ids',\n",
       " 'Placeholder_label_ids',\n",
       " 'Placeholder_is_training',\n",
       " 'bert/embeddings/word_embeddings',\n",
       " 'bert/embeddings/token_type_embeddings',\n",
       " 'bert/embeddings/position_embeddings',\n",
       " 'bert/embeddings/LayerNorm/gamma',\n",
       " 'bert/encoder/layer_0/attention/self/query/kernel',\n",
       " 'bert/encoder/layer_0/attention/self/query/bias',\n",
       " 'bert/encoder/layer_0/attention/self/key/kernel',\n",
       " 'bert/encoder/layer_0/attention/self/key/bias',\n",
       " 'bert/encoder/layer_0/attention/self/value/kernel',\n",
       " 'bert/encoder/layer_0/attention/self/value/bias',\n",
       " 'bert/encoder/layer_0/attention/output/dense/kernel',\n",
       " 'bert/encoder/layer_0/attention/output/dense/bias',\n",
       " 'bert/encoder/layer_0/attention/output/LayerNorm/gamma',\n",
       " 'bert/encoder/layer_0/intermediate/dense/kernel',\n",
       " 'bert/encoder/layer_0/intermediate/dense/bias',\n",
       " 'bert/encoder/layer_0/output/dense/kernel',\n",
       " 'bert/encoder/layer_0/output/dense/bias',\n",
       " 'bert/encoder/layer_0/output/LayerNorm/gamma',\n",
       " 'bert/encoder/layer_1/attention/self/query/kernel',\n",
       " 'bert/encoder/layer_1/attention/self/query/bias',\n",
       " 'bert/encoder/layer_1/attention/self/key/kernel',\n",
       " 'bert/encoder/layer_1/attention/self/key/bias',\n",
       " 'bert/encoder/layer_1/attention/self/value/kernel',\n",
       " 'bert/encoder/layer_1/attention/self/value/bias',\n",
       " 'bert/encoder/layer_1/attention/output/dense/kernel',\n",
       " 'bert/encoder/layer_1/attention/output/dense/bias',\n",
       " 'bert/encoder/layer_1/attention/output/LayerNorm/gamma',\n",
       " 'bert/encoder/layer_1/intermediate/dense/kernel',\n",
       " 'bert/encoder/layer_1/intermediate/dense/bias',\n",
       " 'bert/encoder/layer_1/output/dense/kernel',\n",
       " 'bert/encoder/layer_1/output/dense/bias',\n",
       " 'bert/encoder/layer_1/output/LayerNorm/gamma',\n",
       " 'bert/pooler/dense/kernel',\n",
       " 'bert/pooler/dense/bias',\n",
       " 'weights/output_weights',\n",
       " 'weights/output_bias',\n",
       " 'loss/logits']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strings.split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bert/model.ckpt'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saver = tf.train.Saver(tf.trainable_variables())\n",
    "saver.save(sess, 'bert/model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = str_idx(texts, dictionary, maxlen)\n",
    "train_X, test_X, train_Y, test_Y = train_test_split(\n",
    "    texts, labels, test_size = 0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 16876/16876 [1:36:14<00:00,  3.26it/s, accuracy=0.833, cost=0.398]\n",
      "test minibatch loop: 100%|██████████| 4219/4219 [08:02<00:00,  9.04it/s, accuracy=0.862, cost=0.35] \n",
      "train minibatch loop:   0%|          | 0/16876 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, pass acc: 0.000000, current acc: 0.784242\n",
      "time taken: 6256.754328489304\n",
      "epoch: 0, training loss: 0.478161, training acc: 0.770285, valid loss: 0.454835, valid acc: 0.784242\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 16876/16876 [1:35:42<00:00,  3.32it/s, accuracy=0.722, cost=0.427]\n",
      "test minibatch loop: 100%|██████████| 4219/4219 [07:54<00:00,  9.21it/s, accuracy=0.828, cost=0.421]\n",
      "train minibatch loop:   0%|          | 0/16876 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken: 6216.978980302811\n",
      "epoch: 1, training loss: 0.424994, training acc: 0.804825, valid loss: 0.466114, valid acc: 0.778064\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 16876/16876 [1:35:28<00:00,  3.34it/s, accuracy=0.944, cost=0.31] \n",
      "test minibatch loop: 100%|██████████| 4219/4219 [07:56<00:00,  8.86it/s, accuracy=0.828, cost=0.429]\n",
      "train minibatch loop:   0%|          | 0/16876 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken: 6204.905177354813\n",
      "epoch: 2, training loss: 0.396685, training acc: 0.821129, valid loss: 0.481994, valid acc: 0.774056\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 16876/16876 [1:35:19<00:00,  3.37it/s, accuracy=0.778, cost=0.328] \n",
      "test minibatch loop: 100%|██████████| 4219/4219 [07:59<00:00,  9.08it/s, accuracy=0.828, cost=0.363]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken: 6198.127304553986\n",
      "epoch: 3, training loss: 0.378270, training acc: 0.830598, valid loss: 0.501795, valid acc: 0.770249\n",
      "\n",
      "break epoch:4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "EARLY_STOPPING, CURRENT_CHECKPOINT, CURRENT_ACC, EPOCH = 3, 0, 0, 0\n",
    "\n",
    "while True:\n",
    "    lasttime = time.time()\n",
    "    if CURRENT_CHECKPOINT == EARLY_STOPPING:\n",
    "        print('break epoch:%d\\n' % (EPOCH))\n",
    "        break\n",
    "\n",
    "    train_acc, train_loss, test_acc, test_loss = 0, 0, 0, 0\n",
    "    pbar = tqdm(\n",
    "        range(0, len(train_X), batch_size), desc = 'train minibatch loop'\n",
    "    )\n",
    "    for i in pbar:\n",
    "        batch_x = str_idx(train_X[i : min(i + batch_size, len(train_X))], dictionary, maxlen)\n",
    "        batch_y = train_Y[i : min(i + batch_size, len(train_X))]\n",
    "        np_mask = np.ones((len(batch_x), maxlen), dtype = np.int32)\n",
    "        np_segment = np.ones((len(batch_x), maxlen), dtype = np.int32)\n",
    "        acc, cost, _ = sess.run(\n",
    "            [accuracy, loss, optimizer],\n",
    "            feed_dict = {\n",
    "                input_ids: batch_x,\n",
    "                label_ids: batch_y,\n",
    "                input_mask: np_mask,\n",
    "                segment_ids: np_segment,\n",
    "                is_training: True\n",
    "            },\n",
    "        )\n",
    "        assert not np.isnan(cost)\n",
    "        train_loss += cost\n",
    "        train_acc += acc\n",
    "        pbar.set_postfix(cost = cost, accuracy = acc)\n",
    "        \n",
    "    pbar = tqdm(range(0, len(test_X), batch_size), desc = 'test minibatch loop')\n",
    "    for i in pbar:\n",
    "        batch_x = str_idx(test_X[i : min(i + batch_size, len(test_X))], dictionary, maxlen)\n",
    "        batch_y = test_Y[i : min(i + batch_size, len(test_X))]\n",
    "        np_mask = np.ones((len(batch_x), maxlen), dtype = np.int32)\n",
    "        np_segment = np.ones((len(batch_x), maxlen), dtype = np.int32)\n",
    "        acc, cost = sess.run(\n",
    "            [accuracy, loss],\n",
    "            feed_dict = {\n",
    "                input_ids: batch_x,\n",
    "                label_ids: batch_y,\n",
    "                input_mask: np_mask,\n",
    "                segment_ids: np_segment,\n",
    "                is_training: False\n",
    "            },\n",
    "        )\n",
    "        test_loss += cost\n",
    "        test_acc += acc\n",
    "        pbar.set_postfix(cost = cost, accuracy = acc)\n",
    "\n",
    "    train_loss /= len(train_X) / batch_size\n",
    "    train_acc /= len(train_X) / batch_size\n",
    "    test_loss /= len(test_X) / batch_size\n",
    "    test_acc /= len(test_X) / batch_size\n",
    "\n",
    "    if test_acc > CURRENT_ACC:\n",
    "        print(\n",
    "            'epoch: %d, pass acc: %f, current acc: %f'\n",
    "            % (EPOCH, CURRENT_ACC, test_acc)\n",
    "        )\n",
    "        CURRENT_ACC = test_acc\n",
    "        CURRENT_CHECKPOINT = 0\n",
    "    else:\n",
    "        CURRENT_CHECKPOINT += 1\n",
    "\n",
    "    print('time taken:', time.time() - lasttime)\n",
    "    print(\n",
    "        'epoch: %d, training loss: %f, training acc: %f, valid loss: %f, valid acc: %f\\n'\n",
    "        % (EPOCH, train_loss, train_acc, test_loss, test_acc)\n",
    "    )\n",
    "    EPOCH += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "validation minibatch loop: 100%|██████████| 4219/4219 [07:55<00:00,  9.23it/s]\n"
     ]
    }
   ],
   "source": [
    "real_Y, predict_Y = [], []\n",
    "\n",
    "pbar = tqdm(\n",
    "    range(0, len(test_X), batch_size), desc = 'validation minibatch loop'\n",
    ")\n",
    "for i in pbar:\n",
    "    batch_x = str_idx(test_X[i : min(i + batch_size, len(test_X))], dictionary, maxlen)\n",
    "    batch_y = test_Y[i : min(i + batch_size, len(test_X))]\n",
    "    np_mask = np.ones((len(batch_x), maxlen), dtype = np.int32)\n",
    "    np_segment = np.ones((len(batch_x), maxlen), dtype = np.int32)\n",
    "    predict_Y += np.argmax(\n",
    "        sess.run(\n",
    "            logits,\n",
    "            feed_dict = {\n",
    "                input_ids: batch_x,\n",
    "                label_ids: batch_y,\n",
    "                input_mask: np_mask,\n",
    "                segment_ids: np_segment,\n",
    "                is_training: False,\n",
    "            },\n",
    "        ),\n",
    "        1,\n",
    "    ).tolist()\n",
    "    real_Y += batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.83      0.79     70558\n",
      "    positive       0.79      0.70      0.75     64447\n",
      "\n",
      "   micro avg       0.77      0.77      0.77    135005\n",
      "   macro avg       0.77      0.77      0.77    135005\n",
      "weighted avg       0.77      0.77      0.77    135005\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.classification_report(real_Y, predict_Y, target_names = ['negative','positive']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.9040442 , 0.09595579]], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_mask = np.ones((1, maxlen), dtype = np.int32)\n",
    "np_segment = np.ones((1, maxlen), dtype = np.int32)\n",
    "text = classification_textcleaning(\n",
    "    'kerajaan sebenarnya sangat bencikan rakyatnya, minyak naik dan segalanya'\n",
    ")\n",
    "new_vector = str_idx([text[0]], dictionary, maxlen)\n",
    "sess.run(\n",
    "    tf.nn.softmax(logits),\n",
    "    feed_dict = {\n",
    "        input_ids: new_vector,\n",
    "        input_mask: np_mask,\n",
    "        segment_ids: np_segment,\n",
    "        is_training: False,\n",
    "    },\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.9040442 , 0.09595579]], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_mask = np.ones((1, maxlen), dtype = np.int32)\n",
    "np_segment = np.ones((1, maxlen), dtype = np.int32)\n",
    "text = classification_textcleaning(\n",
    "    'kerajaan sebenarnya sangat sayangkan rakyatnya, tetapi sebenarnya benci'\n",
    ")\n",
    "new_vector = str_idx([text[0]], dictionary, maxlen)\n",
    "sess.run(\n",
    "    tf.nn.softmax(logits),\n",
    "    feed_dict = {\n",
    "        input_ids: new_vector,\n",
    "        input_mask: np_mask,\n",
    "        segment_ids: np_segment,\n",
    "        is_training: False,\n",
    "    },\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bert/model.ckpt'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saver.save(sess, 'bert/model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('bert-sentiment.json','w') as fopen:\n",
    "    fopen.write(json.dumps({'dictionary':dictionary,'reverse_dictionary':rev_dictionary}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_graph(model_dir, output_node_names):\n",
    "\n",
    "    if not tf.gfile.Exists(model_dir):\n",
    "        raise AssertionError(\n",
    "            \"Export directory doesn't exists. Please specify an export \"\n",
    "            'directory: %s' % model_dir\n",
    "        )\n",
    "\n",
    "    checkpoint = tf.train.get_checkpoint_state(model_dir)\n",
    "    input_checkpoint = checkpoint.model_checkpoint_path\n",
    "\n",
    "    absolute_model_dir = '/'.join(input_checkpoint.split('/')[:-1])\n",
    "    output_graph = absolute_model_dir + '/frozen_model.pb'\n",
    "    clear_devices = True\n",
    "    with tf.Session(graph = tf.Graph()) as sess:\n",
    "        saver = tf.train.import_meta_graph(\n",
    "            input_checkpoint + '.meta', clear_devices = clear_devices\n",
    "        )\n",
    "        saver.restore(sess, input_checkpoint)\n",
    "        output_graph_def = tf.graph_util.convert_variables_to_constants(\n",
    "            sess,\n",
    "            tf.get_default_graph().as_graph_def(),\n",
    "            output_node_names.split(','),\n",
    "        )\n",
    "        with tf.gfile.GFile(output_graph, 'wb') as f:\n",
    "            f.write(output_graph_def.SerializeToString())\n",
    "        print('%d ops in the final graph.' % len(output_graph_def.node))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from bert/model.ckpt\n",
      "INFO:tensorflow:Froze 41 variables.\n",
      "INFO:tensorflow:Converted 41 variables to const ops.\n",
      "375 ops in the final graph.\n"
     ]
    }
   ],
   "source": [
    "freeze_graph('bert', strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_graph(frozen_graph_filename):\n",
    "    with tf.gfile.GFile(frozen_graph_filename, 'rb') as f:\n",
    "        graph_def = tf.GraphDef()\n",
    "        graph_def.ParseFromString(f.read())\n",
    "    with tf.Graph().as_default() as graph:\n",
    "        tf.import_graph_def(graph_def)\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = load_graph('bert/frozen_model.pb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['import/Placeholder_input_ids',\n",
       " 'import/Placeholder_input_mask',\n",
       " 'import/Placeholder_segment_ids',\n",
       " 'import/Placeholder_label_ids',\n",
       " 'import/Placeholder_is_training',\n",
       " 'import/bert/embeddings/ExpandDims/dim',\n",
       " 'import/bert/embeddings/ExpandDims',\n",
       " 'import/bert/embeddings/word_embeddings',\n",
       " 'import/bert/embeddings/word_embeddings/read',\n",
       " 'import/bert/embeddings/embedding_lookup/axis',\n",
       " 'import/bert/embeddings/embedding_lookup',\n",
       " 'import/bert/embeddings/embedding_lookup/Identity',\n",
       " 'import/bert/embeddings/Shape',\n",
       " 'import/bert/embeddings/strided_slice/stack',\n",
       " 'import/bert/embeddings/strided_slice/stack_1',\n",
       " 'import/bert/embeddings/strided_slice/stack_2',\n",
       " 'import/bert/embeddings/strided_slice',\n",
       " 'import/bert/embeddings/Reshape/shape/1',\n",
       " 'import/bert/embeddings/Reshape/shape/2',\n",
       " 'import/bert/embeddings/Reshape/shape',\n",
       " 'import/bert/embeddings/Reshape',\n",
       " 'import/bert/embeddings/Shape_1',\n",
       " 'import/bert/embeddings/strided_slice_1/stack',\n",
       " 'import/bert/embeddings/strided_slice_1/stack_1',\n",
       " 'import/bert/embeddings/strided_slice_1/stack_2',\n",
       " 'import/bert/embeddings/strided_slice_1',\n",
       " 'import/bert/embeddings/token_type_embeddings',\n",
       " 'import/bert/embeddings/token_type_embeddings/read',\n",
       " 'import/bert/embeddings/Reshape_1/shape',\n",
       " 'import/bert/embeddings/Reshape_1',\n",
       " 'import/bert/embeddings/one_hot/on_value',\n",
       " 'import/bert/embeddings/one_hot/off_value',\n",
       " 'import/bert/embeddings/one_hot/depth',\n",
       " 'import/bert/embeddings/one_hot',\n",
       " 'import/bert/embeddings/MatMul',\n",
       " 'import/bert/embeddings/Reshape_2/shape/1',\n",
       " 'import/bert/embeddings/Reshape_2/shape/2',\n",
       " 'import/bert/embeddings/Reshape_2/shape',\n",
       " 'import/bert/embeddings/Reshape_2',\n",
       " 'import/bert/embeddings/add',\n",
       " 'import/bert/embeddings/position_embeddings',\n",
       " 'import/bert/embeddings/position_embeddings/read',\n",
       " 'import/bert/embeddings/Slice/begin',\n",
       " 'import/bert/embeddings/Slice/size',\n",
       " 'import/bert/embeddings/Slice',\n",
       " 'import/bert/embeddings/Reshape_3/shape',\n",
       " 'import/bert/embeddings/Reshape_3',\n",
       " 'import/bert/embeddings/add_1',\n",
       " 'import/bert/embeddings/LayerNorm/beta',\n",
       " 'import/bert/embeddings/LayerNorm/beta/read',\n",
       " 'import/bert/embeddings/LayerNorm/gamma',\n",
       " 'import/bert/embeddings/LayerNorm/gamma/read',\n",
       " 'import/bert/embeddings/LayerNorm/moments/mean/reduction_indices',\n",
       " 'import/bert/embeddings/LayerNorm/moments/mean',\n",
       " 'import/bert/embeddings/LayerNorm/moments/StopGradient',\n",
       " 'import/bert/embeddings/LayerNorm/moments/SquaredDifference',\n",
       " 'import/bert/embeddings/LayerNorm/moments/variance/reduction_indices',\n",
       " 'import/bert/embeddings/LayerNorm/moments/variance',\n",
       " 'import/bert/embeddings/LayerNorm/batchnorm/add/y',\n",
       " 'import/bert/embeddings/LayerNorm/batchnorm/add',\n",
       " 'import/bert/embeddings/LayerNorm/batchnorm/Rsqrt',\n",
       " 'import/bert/embeddings/LayerNorm/batchnorm/mul',\n",
       " 'import/bert/embeddings/LayerNorm/batchnorm/mul_1',\n",
       " 'import/bert/embeddings/LayerNorm/batchnorm/mul_2',\n",
       " 'import/bert/embeddings/LayerNorm/batchnorm/sub',\n",
       " 'import/bert/embeddings/LayerNorm/batchnorm/add_1',\n",
       " 'import/bert/encoder/Shape',\n",
       " 'import/bert/encoder/strided_slice/stack',\n",
       " 'import/bert/encoder/strided_slice/stack_1',\n",
       " 'import/bert/encoder/strided_slice/stack_2',\n",
       " 'import/bert/encoder/strided_slice',\n",
       " 'import/bert/encoder/Reshape/shape/1',\n",
       " 'import/bert/encoder/Reshape/shape/2',\n",
       " 'import/bert/encoder/Reshape/shape',\n",
       " 'import/bert/encoder/Reshape',\n",
       " 'import/bert/encoder/Cast',\n",
       " 'import/bert/encoder/ones/packed/1',\n",
       " 'import/bert/encoder/ones/packed/2',\n",
       " 'import/bert/encoder/ones/packed',\n",
       " 'import/bert/encoder/ones/Const',\n",
       " 'import/bert/encoder/ones',\n",
       " 'import/bert/encoder/mul',\n",
       " 'import/bert/encoder/Shape_2',\n",
       " 'import/bert/encoder/strided_slice_2/stack',\n",
       " 'import/bert/encoder/strided_slice_2/stack_1',\n",
       " 'import/bert/encoder/strided_slice_2/stack_2',\n",
       " 'import/bert/encoder/strided_slice_2',\n",
       " 'import/bert/encoder/Reshape_1/shape',\n",
       " 'import/bert/encoder/Reshape_1',\n",
       " 'import/bert/encoder/layer_0/attention/self/query/kernel',\n",
       " 'import/bert/encoder/layer_0/attention/self/query/kernel/read',\n",
       " 'import/bert/encoder/layer_0/attention/self/query/bias',\n",
       " 'import/bert/encoder/layer_0/attention/self/query/bias/read',\n",
       " 'import/bert/encoder/layer_0/attention/self/query/MatMul',\n",
       " 'import/bert/encoder/layer_0/attention/self/query/BiasAdd',\n",
       " 'import/bert/encoder/layer_0/attention/self/key/kernel',\n",
       " 'import/bert/encoder/layer_0/attention/self/key/kernel/read',\n",
       " 'import/bert/encoder/layer_0/attention/self/key/bias',\n",
       " 'import/bert/encoder/layer_0/attention/self/key/bias/read',\n",
       " 'import/bert/encoder/layer_0/attention/self/key/MatMul',\n",
       " 'import/bert/encoder/layer_0/attention/self/key/BiasAdd',\n",
       " 'import/bert/encoder/layer_0/attention/self/value/kernel',\n",
       " 'import/bert/encoder/layer_0/attention/self/value/kernel/read',\n",
       " 'import/bert/encoder/layer_0/attention/self/value/bias',\n",
       " 'import/bert/encoder/layer_0/attention/self/value/bias/read',\n",
       " 'import/bert/encoder/layer_0/attention/self/value/MatMul',\n",
       " 'import/bert/encoder/layer_0/attention/self/value/BiasAdd',\n",
       " 'import/bert/encoder/layer_0/attention/self/Reshape/shape/1',\n",
       " 'import/bert/encoder/layer_0/attention/self/Reshape/shape/2',\n",
       " 'import/bert/encoder/layer_0/attention/self/Reshape/shape/3',\n",
       " 'import/bert/encoder/layer_0/attention/self/Reshape/shape',\n",
       " 'import/bert/encoder/layer_0/attention/self/Reshape',\n",
       " 'import/bert/encoder/layer_0/attention/self/transpose/perm',\n",
       " 'import/bert/encoder/layer_0/attention/self/transpose',\n",
       " 'import/bert/encoder/layer_0/attention/self/Reshape_1/shape/1',\n",
       " 'import/bert/encoder/layer_0/attention/self/Reshape_1/shape/2',\n",
       " 'import/bert/encoder/layer_0/attention/self/Reshape_1/shape/3',\n",
       " 'import/bert/encoder/layer_0/attention/self/Reshape_1/shape',\n",
       " 'import/bert/encoder/layer_0/attention/self/Reshape_1',\n",
       " 'import/bert/encoder/layer_0/attention/self/transpose_1/perm',\n",
       " 'import/bert/encoder/layer_0/attention/self/transpose_1',\n",
       " 'import/bert/encoder/layer_0/attention/self/MatMul',\n",
       " 'import/bert/encoder/layer_0/attention/self/Mul/y',\n",
       " 'import/bert/encoder/layer_0/attention/self/Mul',\n",
       " 'import/bert/encoder/layer_0/attention/self/ExpandDims/dim',\n",
       " 'import/bert/encoder/layer_0/attention/self/ExpandDims',\n",
       " 'import/bert/encoder/layer_0/attention/self/sub/x',\n",
       " 'import/bert/encoder/layer_0/attention/self/sub',\n",
       " 'import/bert/encoder/layer_0/attention/self/mul_1/y',\n",
       " 'import/bert/encoder/layer_0/attention/self/mul_1',\n",
       " 'import/bert/encoder/layer_0/attention/self/add',\n",
       " 'import/bert/encoder/layer_0/attention/self/Softmax',\n",
       " 'import/bert/encoder/layer_0/attention/self/Reshape_2/shape/1',\n",
       " 'import/bert/encoder/layer_0/attention/self/Reshape_2/shape/2',\n",
       " 'import/bert/encoder/layer_0/attention/self/Reshape_2/shape/3',\n",
       " 'import/bert/encoder/layer_0/attention/self/Reshape_2/shape',\n",
       " 'import/bert/encoder/layer_0/attention/self/Reshape_2',\n",
       " 'import/bert/encoder/layer_0/attention/self/transpose_2/perm',\n",
       " 'import/bert/encoder/layer_0/attention/self/transpose_2',\n",
       " 'import/bert/encoder/layer_0/attention/self/MatMul_1',\n",
       " 'import/bert/encoder/layer_0/attention/self/transpose_3/perm',\n",
       " 'import/bert/encoder/layer_0/attention/self/transpose_3',\n",
       " 'import/bert/encoder/layer_0/attention/self/mul_2/y',\n",
       " 'import/bert/encoder/layer_0/attention/self/mul_2',\n",
       " 'import/bert/encoder/layer_0/attention/self/Reshape_3/shape/1',\n",
       " 'import/bert/encoder/layer_0/attention/self/Reshape_3/shape',\n",
       " 'import/bert/encoder/layer_0/attention/self/Reshape_3',\n",
       " 'import/bert/encoder/layer_0/attention/output/dense/kernel',\n",
       " 'import/bert/encoder/layer_0/attention/output/dense/kernel/read',\n",
       " 'import/bert/encoder/layer_0/attention/output/dense/bias',\n",
       " 'import/bert/encoder/layer_0/attention/output/dense/bias/read',\n",
       " 'import/bert/encoder/layer_0/attention/output/dense/MatMul',\n",
       " 'import/bert/encoder/layer_0/attention/output/dense/BiasAdd',\n",
       " 'import/bert/encoder/layer_0/attention/output/add',\n",
       " 'import/bert/encoder/layer_0/attention/output/LayerNorm/beta',\n",
       " 'import/bert/encoder/layer_0/attention/output/LayerNorm/beta/read',\n",
       " 'import/bert/encoder/layer_0/attention/output/LayerNorm/gamma',\n",
       " 'import/bert/encoder/layer_0/attention/output/LayerNorm/gamma/read',\n",
       " 'import/bert/encoder/layer_0/attention/output/LayerNorm/moments/mean/reduction_indices',\n",
       " 'import/bert/encoder/layer_0/attention/output/LayerNorm/moments/mean',\n",
       " 'import/bert/encoder/layer_0/attention/output/LayerNorm/moments/StopGradient',\n",
       " 'import/bert/encoder/layer_0/attention/output/LayerNorm/moments/SquaredDifference',\n",
       " 'import/bert/encoder/layer_0/attention/output/LayerNorm/moments/variance/reduction_indices',\n",
       " 'import/bert/encoder/layer_0/attention/output/LayerNorm/moments/variance',\n",
       " 'import/bert/encoder/layer_0/attention/output/LayerNorm/batchnorm/add/y',\n",
       " 'import/bert/encoder/layer_0/attention/output/LayerNorm/batchnorm/add',\n",
       " 'import/bert/encoder/layer_0/attention/output/LayerNorm/batchnorm/Rsqrt',\n",
       " 'import/bert/encoder/layer_0/attention/output/LayerNorm/batchnorm/mul',\n",
       " 'import/bert/encoder/layer_0/attention/output/LayerNorm/batchnorm/mul_1',\n",
       " 'import/bert/encoder/layer_0/attention/output/LayerNorm/batchnorm/mul_2',\n",
       " 'import/bert/encoder/layer_0/attention/output/LayerNorm/batchnorm/sub',\n",
       " 'import/bert/encoder/layer_0/attention/output/LayerNorm/batchnorm/add_1',\n",
       " 'import/bert/encoder/layer_0/intermediate/dense/kernel',\n",
       " 'import/bert/encoder/layer_0/intermediate/dense/kernel/read',\n",
       " 'import/bert/encoder/layer_0/intermediate/dense/bias',\n",
       " 'import/bert/encoder/layer_0/intermediate/dense/bias/read',\n",
       " 'import/bert/encoder/layer_0/intermediate/dense/MatMul',\n",
       " 'import/bert/encoder/layer_0/intermediate/dense/BiasAdd',\n",
       " 'import/bert/encoder/layer_0/intermediate/dense/Sqrt/x',\n",
       " 'import/bert/encoder/layer_0/intermediate/dense/Sqrt',\n",
       " 'import/bert/encoder/layer_0/intermediate/dense/truediv',\n",
       " 'import/bert/encoder/layer_0/intermediate/dense/Erf',\n",
       " 'import/bert/encoder/layer_0/intermediate/dense/add/x',\n",
       " 'import/bert/encoder/layer_0/intermediate/dense/add',\n",
       " 'import/bert/encoder/layer_0/intermediate/dense/mul/x',\n",
       " 'import/bert/encoder/layer_0/intermediate/dense/mul',\n",
       " 'import/bert/encoder/layer_0/intermediate/dense/mul_1',\n",
       " 'import/bert/encoder/layer_0/output/dense/kernel',\n",
       " 'import/bert/encoder/layer_0/output/dense/kernel/read',\n",
       " 'import/bert/encoder/layer_0/output/dense/bias',\n",
       " 'import/bert/encoder/layer_0/output/dense/bias/read',\n",
       " 'import/bert/encoder/layer_0/output/dense/MatMul',\n",
       " 'import/bert/encoder/layer_0/output/dense/BiasAdd',\n",
       " 'import/bert/encoder/layer_0/output/add',\n",
       " 'import/bert/encoder/layer_0/output/LayerNorm/beta',\n",
       " 'import/bert/encoder/layer_0/output/LayerNorm/beta/read',\n",
       " 'import/bert/encoder/layer_0/output/LayerNorm/gamma',\n",
       " 'import/bert/encoder/layer_0/output/LayerNorm/gamma/read',\n",
       " 'import/bert/encoder/layer_0/output/LayerNorm/moments/mean/reduction_indices',\n",
       " 'import/bert/encoder/layer_0/output/LayerNorm/moments/mean',\n",
       " 'import/bert/encoder/layer_0/output/LayerNorm/moments/StopGradient',\n",
       " 'import/bert/encoder/layer_0/output/LayerNorm/moments/SquaredDifference',\n",
       " 'import/bert/encoder/layer_0/output/LayerNorm/moments/variance/reduction_indices',\n",
       " 'import/bert/encoder/layer_0/output/LayerNorm/moments/variance',\n",
       " 'import/bert/encoder/layer_0/output/LayerNorm/batchnorm/add/y',\n",
       " 'import/bert/encoder/layer_0/output/LayerNorm/batchnorm/add',\n",
       " 'import/bert/encoder/layer_0/output/LayerNorm/batchnorm/Rsqrt',\n",
       " 'import/bert/encoder/layer_0/output/LayerNorm/batchnorm/mul',\n",
       " 'import/bert/encoder/layer_0/output/LayerNorm/batchnorm/mul_1',\n",
       " 'import/bert/encoder/layer_0/output/LayerNorm/batchnorm/mul_2',\n",
       " 'import/bert/encoder/layer_0/output/LayerNorm/batchnorm/sub',\n",
       " 'import/bert/encoder/layer_0/output/LayerNorm/batchnorm/add_1',\n",
       " 'import/bert/encoder/layer_1/attention/self/query/kernel',\n",
       " 'import/bert/encoder/layer_1/attention/self/query/kernel/read',\n",
       " 'import/bert/encoder/layer_1/attention/self/query/bias',\n",
       " 'import/bert/encoder/layer_1/attention/self/query/bias/read',\n",
       " 'import/bert/encoder/layer_1/attention/self/query/MatMul',\n",
       " 'import/bert/encoder/layer_1/attention/self/query/BiasAdd',\n",
       " 'import/bert/encoder/layer_1/attention/self/key/kernel',\n",
       " 'import/bert/encoder/layer_1/attention/self/key/kernel/read',\n",
       " 'import/bert/encoder/layer_1/attention/self/key/bias',\n",
       " 'import/bert/encoder/layer_1/attention/self/key/bias/read',\n",
       " 'import/bert/encoder/layer_1/attention/self/key/MatMul',\n",
       " 'import/bert/encoder/layer_1/attention/self/key/BiasAdd',\n",
       " 'import/bert/encoder/layer_1/attention/self/value/kernel',\n",
       " 'import/bert/encoder/layer_1/attention/self/value/kernel/read',\n",
       " 'import/bert/encoder/layer_1/attention/self/value/bias',\n",
       " 'import/bert/encoder/layer_1/attention/self/value/bias/read',\n",
       " 'import/bert/encoder/layer_1/attention/self/value/MatMul',\n",
       " 'import/bert/encoder/layer_1/attention/self/value/BiasAdd',\n",
       " 'import/bert/encoder/layer_1/attention/self/Reshape/shape/1',\n",
       " 'import/bert/encoder/layer_1/attention/self/Reshape/shape/2',\n",
       " 'import/bert/encoder/layer_1/attention/self/Reshape/shape/3',\n",
       " 'import/bert/encoder/layer_1/attention/self/Reshape/shape',\n",
       " 'import/bert/encoder/layer_1/attention/self/Reshape',\n",
       " 'import/bert/encoder/layer_1/attention/self/transpose/perm',\n",
       " 'import/bert/encoder/layer_1/attention/self/transpose',\n",
       " 'import/bert/encoder/layer_1/attention/self/Reshape_1/shape/1',\n",
       " 'import/bert/encoder/layer_1/attention/self/Reshape_1/shape/2',\n",
       " 'import/bert/encoder/layer_1/attention/self/Reshape_1/shape/3',\n",
       " 'import/bert/encoder/layer_1/attention/self/Reshape_1/shape',\n",
       " 'import/bert/encoder/layer_1/attention/self/Reshape_1',\n",
       " 'import/bert/encoder/layer_1/attention/self/transpose_1/perm',\n",
       " 'import/bert/encoder/layer_1/attention/self/transpose_1',\n",
       " 'import/bert/encoder/layer_1/attention/self/MatMul',\n",
       " 'import/bert/encoder/layer_1/attention/self/Mul/y',\n",
       " 'import/bert/encoder/layer_1/attention/self/Mul',\n",
       " 'import/bert/encoder/layer_1/attention/self/ExpandDims/dim',\n",
       " 'import/bert/encoder/layer_1/attention/self/ExpandDims',\n",
       " 'import/bert/encoder/layer_1/attention/self/sub/x',\n",
       " 'import/bert/encoder/layer_1/attention/self/sub',\n",
       " 'import/bert/encoder/layer_1/attention/self/mul_1/y',\n",
       " 'import/bert/encoder/layer_1/attention/self/mul_1',\n",
       " 'import/bert/encoder/layer_1/attention/self/add',\n",
       " 'import/bert/encoder/layer_1/attention/self/Softmax',\n",
       " 'import/bert/encoder/layer_1/attention/self/Reshape_2/shape/1',\n",
       " 'import/bert/encoder/layer_1/attention/self/Reshape_2/shape/2',\n",
       " 'import/bert/encoder/layer_1/attention/self/Reshape_2/shape/3',\n",
       " 'import/bert/encoder/layer_1/attention/self/Reshape_2/shape',\n",
       " 'import/bert/encoder/layer_1/attention/self/Reshape_2',\n",
       " 'import/bert/encoder/layer_1/attention/self/transpose_2/perm',\n",
       " 'import/bert/encoder/layer_1/attention/self/transpose_2',\n",
       " 'import/bert/encoder/layer_1/attention/self/MatMul_1',\n",
       " 'import/bert/encoder/layer_1/attention/self/transpose_3/perm',\n",
       " 'import/bert/encoder/layer_1/attention/self/transpose_3',\n",
       " 'import/bert/encoder/layer_1/attention/self/mul_2/y',\n",
       " 'import/bert/encoder/layer_1/attention/self/mul_2',\n",
       " 'import/bert/encoder/layer_1/attention/self/Reshape_3/shape/1',\n",
       " 'import/bert/encoder/layer_1/attention/self/Reshape_3/shape',\n",
       " 'import/bert/encoder/layer_1/attention/self/Reshape_3',\n",
       " 'import/bert/encoder/layer_1/attention/output/dense/kernel',\n",
       " 'import/bert/encoder/layer_1/attention/output/dense/kernel/read',\n",
       " 'import/bert/encoder/layer_1/attention/output/dense/bias',\n",
       " 'import/bert/encoder/layer_1/attention/output/dense/bias/read',\n",
       " 'import/bert/encoder/layer_1/attention/output/dense/MatMul',\n",
       " 'import/bert/encoder/layer_1/attention/output/dense/BiasAdd',\n",
       " 'import/bert/encoder/layer_1/attention/output/add',\n",
       " 'import/bert/encoder/layer_1/attention/output/LayerNorm/beta',\n",
       " 'import/bert/encoder/layer_1/attention/output/LayerNorm/beta/read',\n",
       " 'import/bert/encoder/layer_1/attention/output/LayerNorm/gamma',\n",
       " 'import/bert/encoder/layer_1/attention/output/LayerNorm/gamma/read',\n",
       " 'import/bert/encoder/layer_1/attention/output/LayerNorm/moments/mean/reduction_indices',\n",
       " 'import/bert/encoder/layer_1/attention/output/LayerNorm/moments/mean',\n",
       " 'import/bert/encoder/layer_1/attention/output/LayerNorm/moments/StopGradient',\n",
       " 'import/bert/encoder/layer_1/attention/output/LayerNorm/moments/SquaredDifference',\n",
       " 'import/bert/encoder/layer_1/attention/output/LayerNorm/moments/variance/reduction_indices',\n",
       " 'import/bert/encoder/layer_1/attention/output/LayerNorm/moments/variance',\n",
       " 'import/bert/encoder/layer_1/attention/output/LayerNorm/batchnorm/add/y',\n",
       " 'import/bert/encoder/layer_1/attention/output/LayerNorm/batchnorm/add',\n",
       " 'import/bert/encoder/layer_1/attention/output/LayerNorm/batchnorm/Rsqrt',\n",
       " 'import/bert/encoder/layer_1/attention/output/LayerNorm/batchnorm/mul',\n",
       " 'import/bert/encoder/layer_1/attention/output/LayerNorm/batchnorm/mul_1',\n",
       " 'import/bert/encoder/layer_1/attention/output/LayerNorm/batchnorm/mul_2',\n",
       " 'import/bert/encoder/layer_1/attention/output/LayerNorm/batchnorm/sub',\n",
       " 'import/bert/encoder/layer_1/attention/output/LayerNorm/batchnorm/add_1',\n",
       " 'import/bert/encoder/layer_1/intermediate/dense/kernel',\n",
       " 'import/bert/encoder/layer_1/intermediate/dense/kernel/read',\n",
       " 'import/bert/encoder/layer_1/intermediate/dense/bias',\n",
       " 'import/bert/encoder/layer_1/intermediate/dense/bias/read',\n",
       " 'import/bert/encoder/layer_1/intermediate/dense/MatMul',\n",
       " 'import/bert/encoder/layer_1/intermediate/dense/BiasAdd',\n",
       " 'import/bert/encoder/layer_1/intermediate/dense/Sqrt/x',\n",
       " 'import/bert/encoder/layer_1/intermediate/dense/Sqrt',\n",
       " 'import/bert/encoder/layer_1/intermediate/dense/truediv',\n",
       " 'import/bert/encoder/layer_1/intermediate/dense/Erf',\n",
       " 'import/bert/encoder/layer_1/intermediate/dense/add/x',\n",
       " 'import/bert/encoder/layer_1/intermediate/dense/add',\n",
       " 'import/bert/encoder/layer_1/intermediate/dense/mul/x',\n",
       " 'import/bert/encoder/layer_1/intermediate/dense/mul',\n",
       " 'import/bert/encoder/layer_1/intermediate/dense/mul_1',\n",
       " 'import/bert/encoder/layer_1/output/dense/kernel',\n",
       " 'import/bert/encoder/layer_1/output/dense/kernel/read',\n",
       " 'import/bert/encoder/layer_1/output/dense/bias',\n",
       " 'import/bert/encoder/layer_1/output/dense/bias/read',\n",
       " 'import/bert/encoder/layer_1/output/dense/MatMul',\n",
       " 'import/bert/encoder/layer_1/output/dense/BiasAdd',\n",
       " 'import/bert/encoder/layer_1/output/add',\n",
       " 'import/bert/encoder/layer_1/output/LayerNorm/beta',\n",
       " 'import/bert/encoder/layer_1/output/LayerNorm/beta/read',\n",
       " 'import/bert/encoder/layer_1/output/LayerNorm/gamma',\n",
       " 'import/bert/encoder/layer_1/output/LayerNorm/gamma/read',\n",
       " 'import/bert/encoder/layer_1/output/LayerNorm/moments/mean/reduction_indices',\n",
       " 'import/bert/encoder/layer_1/output/LayerNorm/moments/mean',\n",
       " 'import/bert/encoder/layer_1/output/LayerNorm/moments/StopGradient',\n",
       " 'import/bert/encoder/layer_1/output/LayerNorm/moments/SquaredDifference',\n",
       " 'import/bert/encoder/layer_1/output/LayerNorm/moments/variance/reduction_indices',\n",
       " 'import/bert/encoder/layer_1/output/LayerNorm/moments/variance',\n",
       " 'import/bert/encoder/layer_1/output/LayerNorm/batchnorm/add/y',\n",
       " 'import/bert/encoder/layer_1/output/LayerNorm/batchnorm/add',\n",
       " 'import/bert/encoder/layer_1/output/LayerNorm/batchnorm/Rsqrt',\n",
       " 'import/bert/encoder/layer_1/output/LayerNorm/batchnorm/mul',\n",
       " 'import/bert/encoder/layer_1/output/LayerNorm/batchnorm/mul_1',\n",
       " 'import/bert/encoder/layer_1/output/LayerNorm/batchnorm/mul_2',\n",
       " 'import/bert/encoder/layer_1/output/LayerNorm/batchnorm/sub',\n",
       " 'import/bert/encoder/layer_1/output/LayerNorm/batchnorm/add_1',\n",
       " 'import/bert/encoder/Reshape_3/shape/1',\n",
       " 'import/bert/encoder/Reshape_3/shape/2',\n",
       " 'import/bert/encoder/Reshape_3/shape',\n",
       " 'import/bert/encoder/Reshape_3',\n",
       " 'import/bert/pooler/strided_slice/stack',\n",
       " 'import/bert/pooler/strided_slice/stack_1',\n",
       " 'import/bert/pooler/strided_slice/stack_2',\n",
       " 'import/bert/pooler/strided_slice',\n",
       " 'import/bert/pooler/Squeeze',\n",
       " 'import/bert/pooler/dense/kernel',\n",
       " 'import/bert/pooler/dense/kernel/read',\n",
       " 'import/bert/pooler/dense/bias',\n",
       " 'import/bert/pooler/dense/bias/read',\n",
       " 'import/bert/pooler/dense/MatMul',\n",
       " 'import/bert/pooler/dense/BiasAdd',\n",
       " 'import/bert/pooler/dense/Tanh',\n",
       " 'import/weights/output_weights',\n",
       " 'import/weights/output_weights/read',\n",
       " 'import/weights/output_bias',\n",
       " 'import/weights/output_bias/read',\n",
       " 'import/loss/cond/Switch',\n",
       " 'import/loss/cond/switch_t',\n",
       " 'import/loss/cond/pred_id',\n",
       " 'import/loss/cond/dropout/keep_prob',\n",
       " 'import/loss/cond/dropout/Shape/Switch',\n",
       " 'import/loss/cond/dropout/Shape',\n",
       " 'import/loss/cond/dropout/random_uniform/min',\n",
       " 'import/loss/cond/dropout/random_uniform/max',\n",
       " 'import/loss/cond/dropout/random_uniform/RandomUniform',\n",
       " 'import/loss/cond/dropout/random_uniform/sub',\n",
       " 'import/loss/cond/dropout/random_uniform/mul',\n",
       " 'import/loss/cond/dropout/random_uniform',\n",
       " 'import/loss/cond/dropout/add',\n",
       " 'import/loss/cond/dropout/Floor',\n",
       " 'import/loss/cond/dropout/div',\n",
       " 'import/loss/cond/dropout/mul',\n",
       " 'import/loss/cond/Switch_1',\n",
       " 'import/loss/cond/Merge',\n",
       " 'import/loss/MatMul',\n",
       " 'import/loss/logits']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[n.name for n in g.as_graph_def().node]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.9040442 , 0.09595579]], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "placeholder_input_ids = g.get_tensor_by_name('import/Placeholder_input_ids:0')\n",
    "placeholder_input_mask = g.get_tensor_by_name('import/Placeholder_input_mask:0')\n",
    "placeholder_segment_ids = g.get_tensor_by_name('import/Placeholder_segment_ids:0')\n",
    "placeholder_is_training = g.get_tensor_by_name('import/Placeholder_is_training:0')\n",
    "loss_logits = g.get_tensor_by_name('import/loss/logits:0')\n",
    "test_sess = tf.InteractiveSession(graph = g)\n",
    "test_sess.run(\n",
    "    tf.nn.softmax(loss_logits),\n",
    "    feed_dict = {\n",
    "        placeholder_input_ids: new_vector,\n",
    "        placeholder_input_mask: np_mask,\n",
    "        placeholder_segment_ids: np_segment,\n",
    "        placeholder_is_training: False,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
