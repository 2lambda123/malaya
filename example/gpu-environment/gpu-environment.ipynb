{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One model always consumed one unit of gpu. For now we do not support distributed batch processing to multiple GPUs from one model. But we can initiate multiple models to multiple GPUs.\n",
    "\n",
    "model_emotion -> GPU0\n",
    "\n",
    "model_sentiment -> GPU1\n",
    "\n",
    "model_translation -> GPU2\n",
    "\n",
    "and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.94 s, sys: 2.35 s, total: 8.29 s\n",
      "Wall time: 4.01 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import malaya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jul  7 21:32:37 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 410.129      Driver Version: 410.129      CUDA Version: 10.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-DGXS...  On   | 00000000:07:00.0  On |                    0 |\n",
      "| N/A   55C    P0    42W / 300W |      0MiB / 32475MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-DGXS...  On   | 00000000:08:00.0 Off |                    0 |\n",
      "| N/A   65C    P0   250W / 300W |  31452MiB / 32478MiB |     89%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla V100-DGXS...  On   | 00000000:0E:00.0 Off |                    0 |\n",
      "| N/A   63C    P0   270W / 300W |  31452MiB / 32478MiB |     92%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla V100-DGXS...  On   | 00000000:0F:00.0 Off |                    0 |\n",
      "| N/A   63C    P0   252W / 300W |  31452MiB / 32478MiB |     77%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|    1     11646      C   python3                                    31431MiB |\n",
      "|    2     11646      C   python3                                    31431MiB |\n",
      "|    3     11646      C   python3                                    31431MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right now all the GPUs in resting mode, no computation happened."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU Rules\n",
    "\n",
    "1. By default, all models will initiate in first GPU, unless override `gpu` parameter in any load model API. Example as below.\n",
    "2. Malaya will not consumed all available GPU memory, but slowly grow based on batch size. This growth only towards positive (use more GPU memory) dynamically, but will not reduce GPU memory if feed small batch size.\n",
    "3. Use `malaya.clear_session` to clear session from unused models but this will not free GPU memory.\n",
    "4. By default Malaya will not set max cap for GPU memory, to put a cap, override `gpu_limit` parameter in any load model API. `gpu_limit` should 0 < `gpu_limit` < 1. If `gpu_limit = 0.3`, it means the model will not use more than 30% of GPU memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "anger_text = 'babi la company ni, aku dah la penat datang dari jauh'\n",
    "fear_text = 'takut doh tengok cerita hantu tadi'\n",
    "happy_text = 'bestnya dapat tidur harini, tak payah pergi kerja'\n",
    "love_text = 'aku sayang sgt dia dah doh'\n",
    "sadness_text = 'kecewa tengok kerajaan baru ni, janji ape pun tak dapat'\n",
    "surprise_text = 'sakit jantung aku, terkejut dengan cerita hantu tadi'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/husein/malaya/Malaya/malaya/function/__init__.py:61: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/husein/malaya/Malaya/malaya/function/__init__.py:62: The name tf.GraphDef is deprecated. Please use tf.compat.v1.GraphDef instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/husein/malaya/Malaya/malaya/function/__init__.py:50: The name tf.GPUOptions is deprecated. Please use tf.compat.v1.GPUOptions instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/husein/malaya/Malaya/malaya/function/__init__.py:51: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/husein/malaya/Malaya/malaya/function/__init__.py:53: The name tf.InteractiveSession is deprecated. Please use tf.compat.v1.InteractiveSession instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = malaya.emotion.transformer(model = 'bert', gpu = '0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.94 s, sys: 541 ms, total: 2.48 s\n",
      "Wall time: 2.52 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'anger': 0.9998965,\n",
       "  'fear': 1.7692768e-05,\n",
       "  'happy': 1.8747674e-05,\n",
       "  'love': 1.656881e-05,\n",
       "  'sadness': 3.130815e-05,\n",
       "  'surprise': 1.9183277e-05},\n",
       " {'anger': 7.4469484e-05,\n",
       "  'fear': 0.99977416,\n",
       "  'happy': 6.824215e-05,\n",
       "  'love': 2.773282e-05,\n",
       "  'sadness': 1.9767067e-05,\n",
       "  'surprise': 3.5663204e-05},\n",
       " {'anger': 0.99963737,\n",
       "  'fear': 3.931449e-05,\n",
       "  'happy': 0.0001562279,\n",
       "  'love': 3.3580774e-05,\n",
       "  'sadness': 0.00011328616,\n",
       "  'surprise': 2.0134145e-05},\n",
       " {'anger': 3.1319763e-05,\n",
       "  'fear': 1.7286226e-05,\n",
       "  'happy': 2.9899325e-05,\n",
       "  'love': 0.99987257,\n",
       "  'sadness': 2.7867774e-05,\n",
       "  'surprise': 2.096328e-05},\n",
       " {'anger': 8.965934e-05,\n",
       "  'fear': 1.8196944e-05,\n",
       "  'happy': 2.9275663e-05,\n",
       "  'love': 1.7211949e-05,\n",
       "  'sadness': 0.9998247,\n",
       "  'surprise': 2.0944033e-05},\n",
       " {'anger': 4.132152e-05,\n",
       "  'fear': 6.202527e-05,\n",
       "  'happy': 3.1012056e-05,\n",
       "  'love': 5.3896296e-05,\n",
       "  'sadness': 6.202101e-05,\n",
       "  'surprise': 0.9997497}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model.predict_proba(\n",
    "    [anger_text, fear_text, happy_text, love_text, sadness_text, surprise_text]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jul  7 21:32:57 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 410.129      Driver Version: 410.129      CUDA Version: 10.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-DGXS...  On   | 00000000:07:00.0  On |                    0 |\n",
      "| N/A   56C    P0    58W / 300W |   1099MiB / 32475MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-DGXS...  On   | 00000000:08:00.0 Off |                    0 |\n",
      "| N/A   64C    P0   219W / 300W |  31452MiB / 32478MiB |     99%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla V100-DGXS...  On   | 00000000:0E:00.0 Off |                    0 |\n",
      "| N/A   62C    P0   248W / 300W |  31452MiB / 32478MiB |     99%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla V100-DGXS...  On   | 00000000:0F:00.0 Off |                    0 |\n",
      "| N/A   62C    P0   236W / 300W |  31452MiB / 32478MiB |     76%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|    0      2536      C   /usr/bin/python3                            1087MiB |\n",
      "|    1     11646      C   python3                                    31431MiB |\n",
      "|    2     11646      C   python3                                    31431MiB |\n",
      "|    3     11646      C   python3                                    31431MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "malaya.clear_session(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
