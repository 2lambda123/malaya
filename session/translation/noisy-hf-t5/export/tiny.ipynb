{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['finetune-t5-tiny-noisy-bahasa-cased/checkpoint-2790000',\n",
       " 'finetune-t5-tiny-noisy-bahasa-cased/checkpoint-2800000',\n",
       " 'finetune-t5-tiny-noisy-bahasa-cased/checkpoint-2810000',\n",
       " 'finetune-t5-tiny-noisy-bahasa-cased/checkpoint-2820000',\n",
       " 'finetune-t5-tiny-noisy-bahasa-cased/checkpoint-2830000',\n",
       " 'finetune-t5-tiny-noisy-bahasa-cased/checkpoint-2840000',\n",
       " 'finetune-t5-tiny-noisy-bahasa-cased/checkpoint-2850000',\n",
       " 'finetune-t5-tiny-noisy-bahasa-cased/checkpoint-2860000',\n",
       " 'finetune-t5-tiny-noisy-bahasa-cased/checkpoint-2870000',\n",
       " 'finetune-t5-tiny-noisy-bahasa-cased/checkpoint-2880000']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from glob import glob\n",
    "\n",
    "checkpoints = sorted(glob('finetune-t5-tiny-noisy-bahasa-cased/checkpoint-*'))\n",
    "checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained('mesolitica/t5-small-standard-bahasa-cased')\n",
    "model = T5ForConditionalGeneration.from_pretrained(checkpoints[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> Hi guys! I noticed yesterday and today many of our cookies are available. So today I want to share some of our first batch post mortem:</s>\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer.encode('terjemah Melayu ke Inggeris: Hi guys! I noticed semalam & harini dah ramai yang dapat cookies ni kan. So harini i nak share some post mortem of our first batch:', return_tensors = 'pt')\n",
    "outputs = model.generate(input_ids, max_length = 100)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> Hai kawan! Saya perhatikan semalam & harini dah ramai yang dapat cookies ni kan. Jadi harini saya nak kongsi post mortem batch pertama kami:</s>\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer.encode('terjemah Inggeris ke Melayu: Hi guys! I noticed semalam & harini dah ramai yang dapat cookies ni kan. So harini i nak share some post mortem of our first batch:', return_tensors = 'pt')\n",
    "outputs = model.generate(input_ids, max_length = 100)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> I don't understand</s>\n",
      "<pad> 8th in the KK market, he was good at choosing a place.</s>\n",
      "<pad> So it's illegal.</s>\n",
      "<pad> Where do you want to go?</s>\n",
      "<pad> It's like taking half a day</s>\n",
      "<pad> Imagine PH and win pru-14. The past is a bit of a back door. Ismail Sabri's last-last rise. That's why I don't give a fk about politics anymore. I swear it's up.</s>\n"
     ]
    }
   ],
   "source": [
    "strings = [\n",
    "    'ak tak paham la',\n",
    "    'jam 8 di pasar KK memang org ramai üòÇ, pandai dia pilih tmpt.',\n",
    "    'Jadi haram jadahüòÄüòÉü§≠',\n",
    "    'nak gi mana tuu',\n",
    "    'Macam nak ambil half day',\n",
    "    \"Bayangkan PH dan menang pru-14. Pastu macam-macam pintu belakang ada. Last-last Ismail Sabri naik. That's why I don't give a fk about politics anymore. Sumpah dah fk up dah.\",\n",
    "]\n",
    "for s in strings:\n",
    "    input_ids = tokenizer.encode(f'terjemah Melayu ke Inggeris: {s}', return_tensors = 'pt')\n",
    "    outputs = model.generate(input_ids, max_length = 100)\n",
    "    print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> u ni, cakap betul lah</s>\n",
      "<pad> baru sahaja menghadiri majlis perkahwinan sepupu saya. pelik jugak dia buat majlis biasa2 je sebab gaya hidupnya kelihatan mewah. kemudian saya tahu mereka akan berbulan madu 3 minggu. keputusan pintar <unk> </s>\n",
      "<pad> Saya setelah melihat video ini: mm dapnya burger benjo extra mayo</s>\n",
      "<pad> Hai kawan! Saya perhatikan semalam & harini dah ramai yang dapat cookies ni kan. Jadi harini saya nak kongsi post mortem batch pertama kami:</s>\n"
     ]
    }
   ],
   "source": [
    "strings = [\n",
    "    'u ni, talk properly lah',\n",
    "    \"just attended my cousin's wedding. pelik jugak dia buat majlis biasa2 je sebab her lifestyle looks lavish. then i found out they're going on a 3 weeks honeymoon. smart decision üëç\",\n",
    "    'Me after seeing this video: mm dapnya burger benjo extra mayo',\n",
    "    'Hi guys! I noticed semalam & harini dah ramai yang dapat cookies ni kan. So harini i nak share some post mortem of our first batch:',\n",
    "]\n",
    "for s in strings:\n",
    "    input_ids = tokenizer.encode(f'terjemah Inggeris ke Melayu: {s}', return_tensors = 'pt')\n",
    "    outputs = model.generate(input_ids, max_length = 100)\n",
    "    print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6cd404bd5e04405a00d9cf954586690",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file pytorch_model.bin:   0%|          | 4.00k/133M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "remote: Scanning LFS files for validity, may be slow...        \n",
      "remote: LFS file scan complete.        \n",
      "To https://huggingface.co/mesolitica/finetune-noisy-translation-t5-tiny-bahasa-cased\n",
      "   3d32735..3be598e  main -> main\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://huggingface.co/mesolitica/finetune-noisy-translation-t5-tiny-bahasa-cased/commit/3be598e914c794d8ef31fede774bba6ca8e55a5d'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.push_to_hub('finetune-noisy-translation-t5-tiny-bahasa-cased', organization='mesolitica')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.push_to_hub('finetune-noisy-translation-t5-tiny-bahasa-cased', organization='mesolitica')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[main 99b33f9] add tensorboard\n",
      " 1 file changed, 2 insertions(+), 2 deletions(-)\n",
      "Uploading LFS objects: 100% (1/1), 2.6 MB | 342 KB/s, done.                     \n",
      "Enumerating objects: 9, done.\n",
      "Counting objects: 100% (9/9), done.\n",
      "Delta compression using up to 16 threads\n",
      "Compressing objects: 100% (5/5), done.\n",
      "Writing objects: 100% (5/5), 488 bytes | 488.00 KiB/s, done.\n",
      "Total 5 (delta 3), reused 0 (delta 0)\n",
      "remote: Scanning LFS files for validity, may be slow...\u001b[K\n",
      "remote: LFS file scan complete.\u001b[K\n",
      "To https://huggingface.co/mesolitica/finetune-noisy-translation-t5-tiny-bahasa-cased\n",
      "   3be598e..99b33f9  main -> main\n"
     ]
    }
   ],
   "source": [
    "!cp -r finetune-t5-tiny-noisy-bahasa-cased/runs finetune-noisy-translation-t5-tiny-bahasa-cased\n",
    "!cd finetune-noisy-translation-t5-tiny-bahasa-cased && git add . && git commit -m 'add tensorboard' && git push"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sacrebleu.metrics import BLEU, CHRF, TER\n",
    "\n",
    "bleu = BLEU()\n",
    "chrf = CHRF(word_order = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6854"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from unidecode import unidecode\n",
    "import json\n",
    "\n",
    "with open('test-noisy-shuffled.json') as fopen:\n",
    "    test = fopen.read().split('\\n')\n",
    "    test = [json.loads(t) for t in test if len(t)]\n",
    "    \n",
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6854/6854 [17:29<00:00,  6.53it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "results_en_ms, filtered_right_en_ms = [], []\n",
    "results_ms_en, filtered_right_ms_en = [], []\n",
    "for i in tqdm(range(len(test))):\n",
    "    t = test[i]['translation']\n",
    "    p = t['prefix']\n",
    "    s = t['src']\n",
    "    tgt = t['tgt']\n",
    "    \n",
    "    input_ids = [{'input_ids': tokenizer.encode(f'{p}{s}', return_tensors = 'pt')[0]}]\n",
    "    padded = tokenizer.pad(input_ids, padding = 'longest')\n",
    "    outputs = model.generate(**padded, max_length = 1000)[0]\n",
    "    o = tokenizer.decode(outputs, skip_special_tokens=True)\n",
    "    if len(o):\n",
    "        if 'Inggeris ke Melayu' in p:\n",
    "            results_en_ms.append(o)\n",
    "            filtered_right_en_ms.append(tgt)\n",
    "        else:\n",
    "            results_ms_en.append(o)\n",
    "            filtered_right_ms_en.append(tgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2937, 3917)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(results_en_ms), len(results_ms_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'name': 'BLEU',\n",
       "  'score': 41.03641425544081,\n",
       "  '_mean': -1.0,\n",
       "  '_ci': -1.0,\n",
       "  '_verbose': '72.9/49.2/34.8/25.0 (BP = 0.977 ratio = 0.977 hyp_len = 63005 ref_len = 64473)',\n",
       "  'bp': 0.976969604853212,\n",
       "  'counts': [45920, 29534, 19878, 13530],\n",
       "  'totals': [63005, 60068, 57131, 54194],\n",
       "  'sys_len': 63005,\n",
       "  'ref_len': 64473,\n",
       "  'precisions': [72.88310451551465,\n",
       "   49.16761004195246,\n",
       "   34.793719696837094,\n",
       "   24.96586337970993],\n",
       "  'prec_str': '72.9/49.2/34.8/25.0',\n",
       "  'ratio': 0.9772307787756115},\n",
       " chrF2++ = 65.58)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "refs = [filtered_right_en_ms]\n",
    "sys = results_en_ms\n",
    "r = bleu.corpus_score(sys, refs)\n",
    "r.__dict__, chrf.corpus_score(sys, refs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'name': 'BLEU',\n",
       "  'score': 39.72513374635353,\n",
       "  '_mean': -1.0,\n",
       "  '_ci': -1.0,\n",
       "  '_verbose': '69.8/46.2/32.8/23.6 (BP = 0.999 ratio = 0.999 hyp_len = 92913 ref_len = 92985)',\n",
       "  'bp': 0.9992253816996589,\n",
       "  'counts': [64860, 41099, 27914, 19169],\n",
       "  'totals': [92913, 88996, 85079, 81162],\n",
       "  'sys_len': 92913,\n",
       "  'ref_len': 92985,\n",
       "  'precisions': [69.80723903006037,\n",
       "   46.18072722369545,\n",
       "   32.80950645870309,\n",
       "   23.61819570735073],\n",
       "  'prec_str': '69.8/46.2/32.8/23.6',\n",
       "  'ratio': 0.9992256815615422},\n",
       " chrF2++ = 63.16)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "refs = [filtered_right_ms_en]\n",
    "sys = results_ms_en\n",
    "r = bleu.corpus_score(sys, refs)\n",
    "r.__dict__, chrf.corpus_score(sys, refs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
