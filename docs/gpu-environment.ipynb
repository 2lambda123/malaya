{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "This tutorial is available as an IPython notebook at [Malaya/example/gpu-environment](https://github.com/huseinzol05/Malaya/tree/master/example/gpu-environment).\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep learning Malaya trained on CUDA 10.0 and Tensorflow 1.15, supposedly any new version of CUDA and Tensorflow able to support Tensorflow >= 1.13 features.\n",
    "\n",
    "To prevent any CPU and GPU version conflict, should try to uninstall all Malaya version first,\n",
    "\n",
    "```bash\n",
    "pip uninstall malaya malaya-gpu\n",
    "```\n",
    "\n",
    "After that simply install gpu version,\n",
    "\n",
    "```bash\n",
    "pip install malaya-gpu\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU Version Benefit\n",
    "\n",
    "1. Limit GPU memory.\n",
    "2. N Models to N gpus.\n",
    "3. Automatically try to use cugraph for any networkx functions.\n",
    "\n",
    "**We will add more GPU benefits in the future**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.14 s, sys: 1.96 s, total: 7.1 s\n",
      "Wall time: 4.42 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import malaya"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Malaya is GPU\n",
    "\n",
    "Make sure install `malaya-gpu` to utilize full gpu functions in Malaya,\n",
    "\n",
    "```bash\n",
    "\n",
    "pip install malaya-gpu\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "malaya.check_malaya_gpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List available GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/device:GPU:0', '/device:GPU:1', '/device:GPU:2', '/device:GPU:3']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "malaya.__gpu__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Jul 12 19:25:50 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 410.129      Driver Version: 410.129      CUDA Version: 10.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-DGXS...  On   | 00000000:07:00.0  On |                    0 |\n",
      "| N/A   44C    P0    39W / 300W |      0MiB / 32475MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-DGXS...  On   | 00000000:08:00.0 Off |                    0 |\n",
      "| N/A   45C    P0    39W / 300W |      0MiB / 32478MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla V100-DGXS...  On   | 00000000:0E:00.0 Off |                    0 |\n",
      "| N/A   44C    P0    38W / 300W |      0MiB / 32478MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla V100-DGXS...  On   | 00000000:0F:00.0 Off |                    0 |\n",
      "| N/A   45C    P0    43W / 300W |      0MiB / 32478MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right now all the GPUs in resting mode, no computation happened."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limit GPU memory\n",
    "\n",
    "By default Malaya will not set max cap for GPU memory, to put a cap, override `gpu_limit` parameter in any load model API. `gpu_limit` should 0 < `gpu_limit` < 1. If `gpu_limit = 0.3`, it means the model will not use more than 30% of GPU memory.\n",
    "\n",
    "```python\n",
    "\n",
    "malaya.sentiment.transformer(gpu_limit = 0.3)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N Models to N gpus\n",
    "\n",
    "To allocate a model to another GPU, use `gpu` parameter, default is 0.\n",
    "\n",
    "```python\n",
    "\n",
    "model_sentiment = malaya.sentiment.transformer(model = 'bert', gpu_limit = 0.5, gpu = 0)\n",
    "model_subjectivity = malaya.subjectivity.transformer(model = 'bert', gpu_limit = 0.5, gpu = 1)\n",
    "model_emotion = malaya.emotion.transformer(model = 'bert', gpu_limit = 0.5, gpu = 2)\n",
    "model_translation = malaya.translation.ms_en.transformer(gpu_limit = 0.5, gpu = 3)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU Rules\n",
    "\n",
    "1. Malaya will not consumed all available GPU memory, but slowly grow based on batch size. This growth only towards positive (use more GPU memory) dynamically, but will not reduce GPU memory if feed small batch size.\n",
    "2. Use `malaya.clear_session` to clear session from unused models but this will not free GPU memory.\n",
    "3. Even if you installed Malaya CPU version, it will always to load the models in GPU 0 first, if failed, it will load it in CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "anger_text = 'babi la company ni, aku dah la penat datang dari jauh'\n",
    "fear_text = 'takut doh tengok cerita hantu tadi'\n",
    "happy_text = 'bestnya dapat tidur harini, tak payah pergi kerja'\n",
    "love_text = 'aku sayang sgt dia dah doh'\n",
    "sadness_text = 'kecewa tengok kerajaan baru ni, janji ape pun tak dapat'\n",
    "surprise_text = 'sakit jantung aku, terkejut dengan cerita hantu tadi'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/husein/malaya/Malaya/malaya/function/__init__.py:73: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/husein/malaya/Malaya/malaya/function/__init__.py:75: The name tf.GraphDef is deprecated. Please use tf.compat.v1.GraphDef instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/husein/malaya/Malaya/malaya/function/__init__.py:50: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/husein/malaya/Malaya/malaya/function/__init__.py:65: The name tf.InteractiveSession is deprecated. Please use tf.compat.v1.InteractiveSession instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_sentiment = malaya.sentiment.transformer(model = 'bert', gpu_limit = 0.5, gpu = 0)\n",
    "model_subjectivity = malaya.subjectivity.transformer(model = 'bert', gpu_limit = 0.5, gpu = 1)\n",
    "model_emotion = malaya.emotion.transformer(model = 'bert', gpu_limit = 0.5, gpu = 2)\n",
    "model_translation = malaya.translation.ms_en.transformer(gpu_limit = 0.5, gpu = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.61 s, sys: 2.71 s, total: 11.3 s\n",
      "Wall time: 10.8 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Mahathir made a hasty decision']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model_sentiment.predict_proba(\n",
    "    [anger_text, fear_text, happy_text, love_text, sadness_text, surprise_text]\n",
    ")\n",
    "model_subjectivity.predict_proba(\n",
    "    [anger_text, fear_text, happy_text, love_text, sadness_text, surprise_text]\n",
    ")\n",
    "model_emotion.predict_proba(\n",
    "    [anger_text, fear_text, happy_text, love_text, sadness_text, surprise_text]\n",
    ")\n",
    "model_translation.translate(['Mahathir buat keputusan terburu-buru'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Jul 12 19:26:18 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 410.129      Driver Version: 410.129      CUDA Version: 10.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-DGXS...  On   | 00000000:07:00.0  On |                    0 |\n",
      "| N/A   45C    P0    54W / 300W |   1101MiB / 32475MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-DGXS...  On   | 00000000:08:00.0 Off |                    0 |\n",
      "| N/A   46C    P0    52W / 300W |   1100MiB / 32478MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla V100-DGXS...  On   | 00000000:0E:00.0 Off |                    0 |\n",
      "| N/A   45C    P0    52W / 300W |   1100MiB / 32478MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla V100-DGXS...  On   | 00000000:0F:00.0 Off |                    0 |\n",
      "| N/A   45C    P0    53W / 300W |   1100MiB / 32478MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|    0     12786      C   /usr/bin/python3                            1089MiB |\n",
      "|    1     12786      C   /usr/bin/python3                            1089MiB |\n",
      "|    2     12786      C   /usr/bin/python3                            1089MiB |\n",
      "|    3     12786      C   /usr/bin/python3                            1089MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
