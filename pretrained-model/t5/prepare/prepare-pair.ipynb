{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from t5.data import preprocessors as prep\n",
    "import functools\n",
    "import t5\n",
    "import gin\n",
    "import sentencepiece as spm\n",
    "from glob import glob\n",
    "import os\n",
    "\n",
    "gin.parse_config_file('pretrained_models_base_operative_config.gin')\n",
    "vocab = 'sp10m.cased.t5.model'\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.Load(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\n",
    "    '../../pure-text/dumping-parliament.txt',\n",
    "    '../../pure-text/filtered-dumping-wiki.txt',\n",
    "    '../../pure-text/filtered-dumping-cleaned-common-crawl.txt',\n",
    "    '../../pure-text/filtered-dumping-academia.txt',\n",
    "    '../../news/cleaned-news.txt'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def cleaning(string):\n",
    "    string = string.replace('\\n', ' ').replace('\\t', ' ')\n",
    "    string = re.sub(r'[ ]+', ' ', string).strip()\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(files[-2]) as fopen:\n",
    "    data = fopen.read().split('\\n')\n",
    "results, result = [], []\n",
    "for i in data:\n",
    "    if not len(i) and len(result):\n",
    "        results.append(' '.join(result))\n",
    "        result = []\n",
    "    else:\n",
    "        if len(i):\n",
    "            result.append(i)\n",
    "if len(result):\n",
    "    results.append(' '.join(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../pure-text/dumping-parliament.txt 69702\n",
      "../../pure-text/filtered-dumping-wiki.txt 363578\n",
      "../../pure-text/filtered-dumping-cleaned-common-crawl.txt 8915464\n",
      "../../pure-text/filtered-dumping-academia.txt 963765\n",
      "../../news/cleaned-news.txt 173012\n"
     ]
    }
   ],
   "source": [
    "for file in files:\n",
    "    with open(file) as fopen:\n",
    "        data = fopen.read().split('\\n')\n",
    "    results, result = [], []\n",
    "    for i in data:\n",
    "        if not len(i) and len(result):\n",
    "            results.append(' '.join(result))\n",
    "            result = []\n",
    "        else:\n",
    "            if len(i):\n",
    "                result.append(i)\n",
    "    if len(result):\n",
    "        results.append(' '.join(result))\n",
    "        \n",
    "    print(file, len(results))\n",
    "    s = os.path.split(file)[1]\n",
    "    filename = f'{s}-pair.tsv'\n",
    "    \n",
    "    with tf.io.gfile.GFile(filename, 'w') as outfile:\n",
    "        for i in range(len(results)):\n",
    "            outfile.write('%s\\t\\n' % (cleaning(results[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/husein/.local/lib/python3.6/site-packages/t5/models/mesh_transformer.py:210: UserWarning: get_sentencepiece_model_path is deprecated. Please pass the mixture or task vocabulary directly to the Mesh TensorFlow Transformer instead.\n",
      "  \"get_sentencepiece_model_path is deprecated. Please pass the mixture or \"\n"
     ]
    }
   ],
   "source": [
    "def pair_dataset(split, shuffle_files = False):\n",
    "    del shuffle_files\n",
    "    ds = tf.data.TextLineDataset(\n",
    "        glob('*pair.tsv')\n",
    "    )\n",
    "\n",
    "    ds = ds.map(\n",
    "        functools.partial(\n",
    "            tf.io.decode_csv,\n",
    "            record_defaults = ['', ''],\n",
    "            field_delim = '\\t',\n",
    "            use_quote_delim = False,\n",
    "        ),\n",
    "        num_parallel_calls = tf.data.experimental.AUTOTUNE,\n",
    "    )\n",
    "    ds = ds.map(lambda *ex: dict(zip(['text'], ex)))\n",
    "    return ds\n",
    "\n",
    "\n",
    "t5.data.TaskRegistry.remove('pair_dataset')\n",
    "t5.data.TaskRegistry.add(\n",
    "    'pair_dataset',\n",
    "    dataset_fn = pair_dataset,\n",
    "    splits = ['train'],\n",
    "    text_preprocessor = [prep.next_sentence_prediction],\n",
    "    sentencepiece_model_path = vocab,\n",
    "    metric_fns = [t5.evaluation.metrics.accuracy],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <function neighboring_pairs.<locals>.<lambda> at 0x7f478bae3b70> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <function neighboring_pairs.<locals>.<lambda> at 0x7f478bae3b70>, which Python reported as:\n",
      "      lambda x: x[text_key], num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
      "\n",
      "If this is a lambda function, the error may be avoided by creating the lambda in a standalone statement.\n",
      "WARNING: Entity <function neighboring_pairs.<locals>.<lambda> at 0x7f478bae3b70> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <function neighboring_pairs.<locals>.<lambda> at 0x7f478bae3b70>, which Python reported as:\n",
      "      lambda x: x[text_key], num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
      "\n",
      "If this is a lambda function, the error may be avoided by creating the lambda in a standalone statement.\n"
     ]
    }
   ],
   "source": [
    "nq_task = t5.data.TaskRegistry.get(\"pair_dataset\")\n",
    "ds = nq_task.get_dataset(split='qa.tsv', sequence_length={\"inputs\": 1024, \"targets\": 1024})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = tfds.as_numpy(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'inputs_plaintext': b'nsp: Dr Mahathir, yang juga Pengerusi Pakatan Harapan berkata, kajian semula terhadap M63 itu pasti akan menimbulkan semula keperluan penilaian semula terhadap beberapa amalan yang telah terlaksana sebelum ini. Ini merupakan kali pertama Perdana Menteri ke negeri ini selepas beliau dipilih semula mengetuai kerajaan persekutuan pada Pilihan Raya Umum yang lepas.',\n",
       " 'inputs': array([ 2532,  7175,    50,   122,   386,    14,    17,    42,   758,\n",
       "          919,   831,    60,    14,   577,   196,   109,   190,  2963,\n",
       "           24,   808,    38,  1834,   196,   646,  2907,   196,   109,\n",
       "          103,  1436,    17,    33, 21075,   137,    20,     3,   297,\n",
       "           34,   252,   131,   203,    82,    30,    86,    20,    98,\n",
       "           71,  1345,   196,  4479,    72,  2835,    23,   866,   489,\n",
       "         1339,    17,  1155,     3,     1]),\n",
       " 'targets_plaintext': b'next',\n",
       " 'targets': array([7426,    1])}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
