{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ac5edf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://f000.backblazeb2.com/file/malaya-model/pretrained/gpt2-345m.tar.gz\n",
    "# !tar -zxf gpt2-345m.tar.gz\n",
    "# !rm gpt2-345m.tar.gz\n",
    "# !wget https://huggingface.co/huseinzol05/bpe/resolve/main/gpt2-encoder.json\n",
    "# !wget https://huggingface.co/huseinzol05/bpe/resolve/main/gpt2-vocab.bpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7da08489",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ''\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5cc97940",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "x = {\n",
    "    \"n_vocab\": 50257,\n",
    "    \"n_ctx\": 1024,\n",
    "    \"n_embd\": 1024,\n",
    "    \"n_head\": 16,\n",
    "    \"n_layer\": 24\n",
    "}\n",
    "with open('config.json', 'w') as fopen:\n",
    "    json.dump(x, fopen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af3872de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint\t frozen_model.pb.quantized\t model.ckpt.index\r\n",
      "frozen_model.pb  model.ckpt.data-00000-of-00001  model.ckpt.meta\r\n"
     ]
    }
   ],
   "source": [
    "!ls gpt2-345m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f23492bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir gpt2-345m-pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd0bd9c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting TensorFlow checkpoint from /home/husein/malaya/gpt2-345m/model.ckpt\n",
      "Loading TF weight model/h0/attn/c_attn/b with shape [3072]\n",
      "Loading TF weight model/h0/attn/c_attn/w with shape [1, 1024, 3072]\n",
      "Loading TF weight model/h0/attn/c_proj/b with shape [1024]\n",
      "Loading TF weight model/h0/attn/c_proj/w with shape [1, 1024, 1024]\n",
      "Loading TF weight model/h0/ln_1/b with shape [1024]\n",
      "Loading TF weight model/h0/ln_1/g with shape [1024]\n",
      "Loading TF weight model/h0/ln_2/b with shape [1024]\n",
      "Loading TF weight model/h0/ln_2/g with shape [1024]\n",
      "Loading TF weight model/h0/mlp/c_fc/b with shape [4096]\n",
      "Loading TF weight model/h0/mlp/c_fc/w with shape [1, 1024, 4096]\n",
      "Loading TF weight model/h0/mlp/c_proj/b with shape [1024]\n",
      "Loading TF weight model/h0/mlp/c_proj/w with shape [1, 4096, 1024]\n",
      "Loading TF weight model/h1/attn/c_attn/b with shape [3072]\n",
      "Loading TF weight model/h1/attn/c_attn/w with shape [1, 1024, 3072]\n",
      "Loading TF weight model/h1/attn/c_proj/b with shape [1024]\n",
      "Loading TF weight model/h1/attn/c_proj/w with shape [1, 1024, 1024]\n",
      "Loading TF weight model/h1/ln_1/b with shape [1024]\n",
      "Loading TF weight model/h1/ln_1/g with shape [1024]\n",
      "Loading TF weight model/h1/ln_2/b with shape [1024]\n",
      "Loading TF weight model/h1/ln_2/g with shape [1024]\n",
      "Loading TF weight model/h1/mlp/c_fc/b with shape [4096]\n",
      "Loading TF weight model/h1/mlp/c_fc/w with shape [1, 1024, 4096]\n",
      "Loading TF weight model/h1/mlp/c_proj/b with shape [1024]\n",
      "Loading TF weight model/h1/mlp/c_proj/w with shape [1, 4096, 1024]\n",
      "Loading TF weight model/h10/attn/c_attn/b with shape [3072]\n",
      "Loading TF weight model/h10/attn/c_attn/w with shape [1, 1024, 3072]\n",
      "Loading TF weight model/h10/attn/c_proj/b with shape [1024]\n",
      "Loading TF weight model/h10/attn/c_proj/w with shape [1, 1024, 1024]\n",
      "Loading TF weight model/h10/ln_1/b with shape [1024]\n",
      "Loading TF weight model/h10/ln_1/g with shape [1024]\n",
      "Loading TF weight model/h10/ln_2/b with shape [1024]\n",
      "Loading TF weight model/h10/ln_2/g with shape [1024]\n",
      "Loading TF weight model/h10/mlp/c_fc/b with shape [4096]\n",
      "Loading TF weight model/h10/mlp/c_fc/w with shape [1, 1024, 4096]\n",
      "Loading TF weight model/h10/mlp/c_proj/b with shape [1024]\n",
      "Loading TF weight model/h10/mlp/c_proj/w with shape [1, 4096, 1024]\n",
      "Loading TF weight model/h11/attn/c_attn/b with shape [3072]\n",
      "Loading TF weight model/h11/attn/c_attn/w with shape [1, 1024, 3072]\n",
      "Loading TF weight model/h11/attn/c_proj/b with shape [1024]\n",
      "Loading TF weight model/h11/attn/c_proj/w with shape [1, 1024, 1024]\n",
      "Loading TF weight model/h11/ln_1/b with shape [1024]\n",
      "Loading TF weight model/h11/ln_1/g with shape [1024]\n",
      "Loading TF weight model/h11/ln_2/b with shape [1024]\n",
      "Loading TF weight model/h11/ln_2/g with shape [1024]\n",
      "Loading TF weight model/h11/mlp/c_fc/b with shape [4096]\n",
      "Loading TF weight model/h11/mlp/c_fc/w with shape [1, 1024, 4096]\n",
      "Loading TF weight model/h11/mlp/c_proj/b with shape [1024]\n",
      "Loading TF weight model/h11/mlp/c_proj/w with shape [1, 4096, 1024]\n",
      "Loading TF weight model/h12/attn/c_attn/b with shape [3072]\n",
      "Loading TF weight model/h12/attn/c_attn/w with shape [1, 1024, 3072]\n",
      "Loading TF weight model/h12/attn/c_proj/b with shape [1024]\n",
      "Loading TF weight model/h12/attn/c_proj/w with shape [1, 1024, 1024]\n",
      "Loading TF weight model/h12/ln_1/b with shape [1024]\n",
      "Loading TF weight model/h12/ln_1/g with shape [1024]\n",
      "Loading TF weight model/h12/ln_2/b with shape [1024]\n",
      "Loading TF weight model/h12/ln_2/g with shape [1024]\n",
      "Loading TF weight model/h12/mlp/c_fc/b with shape [4096]\n",
      "Loading TF weight model/h12/mlp/c_fc/w with shape [1, 1024, 4096]\n",
      "Loading TF weight model/h12/mlp/c_proj/b with shape [1024]\n",
      "Loading TF weight model/h12/mlp/c_proj/w with shape [1, 4096, 1024]\n",
      "Loading TF weight model/h13/attn/c_attn/b with shape [3072]\n",
      "Loading TF weight model/h13/attn/c_attn/w with shape [1, 1024, 3072]\n",
      "Loading TF weight model/h13/attn/c_proj/b with shape [1024]\n",
      "Loading TF weight model/h13/attn/c_proj/w with shape [1, 1024, 1024]\n",
      "Loading TF weight model/h13/ln_1/b with shape [1024]\n",
      "Loading TF weight model/h13/ln_1/g with shape [1024]\n",
      "Loading TF weight model/h13/ln_2/b with shape [1024]\n",
      "Loading TF weight model/h13/ln_2/g with shape [1024]\n",
      "Loading TF weight model/h13/mlp/c_fc/b with shape [4096]\n",
      "Loading TF weight model/h13/mlp/c_fc/w with shape [1, 1024, 4096]\n",
      "Loading TF weight model/h13/mlp/c_proj/b with shape [1024]\n",
      "Loading TF weight model/h13/mlp/c_proj/w with shape [1, 4096, 1024]\n",
      "Loading TF weight model/h14/attn/c_attn/b with shape [3072]\n",
      "Loading TF weight model/h14/attn/c_attn/w with shape [1, 1024, 3072]\n",
      "Loading TF weight model/h14/attn/c_proj/b with shape [1024]\n",
      "Loading TF weight model/h14/attn/c_proj/w with shape [1, 1024, 1024]\n",
      "Loading TF weight model/h14/ln_1/b with shape [1024]\n",
      "Loading TF weight model/h14/ln_1/g with shape [1024]\n",
      "Loading TF weight model/h14/ln_2/b with shape [1024]\n",
      "Loading TF weight model/h14/ln_2/g with shape [1024]\n",
      "Loading TF weight model/h14/mlp/c_fc/b with shape [4096]\n",
      "Loading TF weight model/h14/mlp/c_fc/w with shape [1, 1024, 4096]\n",
      "Loading TF weight model/h14/mlp/c_proj/b with shape [1024]\n",
      "Loading TF weight model/h14/mlp/c_proj/w with shape [1, 4096, 1024]\n",
      "Loading TF weight model/h15/attn/c_attn/b with shape [3072]\n",
      "Loading TF weight model/h15/attn/c_attn/w with shape [1, 1024, 3072]\n",
      "Loading TF weight model/h15/attn/c_proj/b with shape [1024]\n",
      "Loading TF weight model/h15/attn/c_proj/w with shape [1, 1024, 1024]\n",
      "Loading TF weight model/h15/ln_1/b with shape [1024]\n",
      "Loading TF weight model/h15/ln_1/g with shape [1024]\n",
      "Loading TF weight model/h15/ln_2/b with shape [1024]\n",
      "Loading TF weight model/h15/ln_2/g with shape [1024]\n",
      "Loading TF weight model/h15/mlp/c_fc/b with shape [4096]\n",
      "Loading TF weight model/h15/mlp/c_fc/w with shape [1, 1024, 4096]\n",
      "Loading TF weight model/h15/mlp/c_proj/b with shape [1024]\n",
      "Loading TF weight model/h15/mlp/c_proj/w with shape [1, 4096, 1024]\n",
      "Loading TF weight model/h16/attn/c_attn/b with shape [3072]\n",
      "Loading TF weight model/h16/attn/c_attn/w with shape [1, 1024, 3072]\n",
      "Loading TF weight model/h16/attn/c_proj/b with shape [1024]\n",
      "Loading TF weight model/h16/attn/c_proj/w with shape [1, 1024, 1024]\n",
      "Loading TF weight model/h16/ln_1/b with shape [1024]\n",
      "Loading TF weight model/h16/ln_1/g with shape [1024]\n",
      "Loading TF weight model/h16/ln_2/b with shape [1024]\n",
      "Loading TF weight model/h16/ln_2/g with shape [1024]\n",
      "Loading TF weight model/h16/mlp/c_fc/b with shape [4096]\n",
      "Loading TF weight model/h16/mlp/c_fc/w with shape [1, 1024, 4096]\n",
      "Loading TF weight model/h16/mlp/c_proj/b with shape [1024]\n",
      "Loading TF weight model/h16/mlp/c_proj/w with shape [1, 4096, 1024]\n",
      "Loading TF weight model/h17/attn/c_attn/b with shape [3072]\n",
      "Loading TF weight model/h17/attn/c_attn/w with shape [1, 1024, 3072]\n",
      "Loading TF weight model/h17/attn/c_proj/b with shape [1024]\n",
      "Loading TF weight model/h17/attn/c_proj/w with shape [1, 1024, 1024]\n",
      "Loading TF weight model/h17/ln_1/b with shape [1024]\n",
      "Loading TF weight model/h17/ln_1/g with shape [1024]\n",
      "Loading TF weight model/h17/ln_2/b with shape [1024]\n",
      "Loading TF weight model/h17/ln_2/g with shape [1024]\n",
      "Loading TF weight model/h17/mlp/c_fc/b with shape [4096]\n",
      "Loading TF weight model/h17/mlp/c_fc/w with shape [1, 1024, 4096]\n",
      "Loading TF weight model/h17/mlp/c_proj/b with shape [1024]\n",
      "Loading TF weight model/h17/mlp/c_proj/w with shape [1, 4096, 1024]\n",
      "Loading TF weight model/h18/attn/c_attn/b with shape [3072]\n",
      "Loading TF weight model/h18/attn/c_attn/w with shape [1, 1024, 3072]\n",
      "Loading TF weight model/h18/attn/c_proj/b with shape [1024]\n",
      "Loading TF weight model/h18/attn/c_proj/w with shape [1, 1024, 1024]\n",
      "Loading TF weight model/h18/ln_1/b with shape [1024]\n",
      "Loading TF weight model/h18/ln_1/g with shape [1024]\n",
      "Loading TF weight model/h18/ln_2/b with shape [1024]\n",
      "Loading TF weight model/h18/ln_2/g with shape [1024]\n",
      "Loading TF weight model/h18/mlp/c_fc/b with shape [4096]\n",
      "Loading TF weight model/h18/mlp/c_fc/w with shape [1, 1024, 4096]\n",
      "Loading TF weight model/h18/mlp/c_proj/b with shape [1024]\n",
      "Loading TF weight model/h18/mlp/c_proj/w with shape [1, 4096, 1024]\n",
      "Loading TF weight model/h19/attn/c_attn/b with shape [3072]\n",
      "Loading TF weight model/h19/attn/c_attn/w with shape [1, 1024, 3072]\n",
      "Loading TF weight model/h19/attn/c_proj/b with shape [1024]\n",
      "Loading TF weight model/h19/attn/c_proj/w with shape [1, 1024, 1024]\n",
      "Loading TF weight model/h19/ln_1/b with shape [1024]\n",
      "Loading TF weight model/h19/ln_1/g with shape [1024]\n",
      "Loading TF weight model/h19/ln_2/b with shape [1024]\n",
      "Loading TF weight model/h19/ln_2/g with shape [1024]\n",
      "Loading TF weight model/h19/mlp/c_fc/b with shape [4096]\n",
      "Loading TF weight model/h19/mlp/c_fc/w with shape [1, 1024, 4096]\n",
      "Loading TF weight model/h19/mlp/c_proj/b with shape [1024]\n",
      "Loading TF weight model/h19/mlp/c_proj/w with shape [1, 4096, 1024]\n",
      "Loading TF weight model/h2/attn/c_attn/b with shape [3072]\n",
      "Loading TF weight model/h2/attn/c_attn/w with shape [1, 1024, 3072]\n",
      "Loading TF weight model/h2/attn/c_proj/b with shape [1024]\n",
      "Loading TF weight model/h2/attn/c_proj/w with shape [1, 1024, 1024]\n",
      "Loading TF weight model/h2/ln_1/b with shape [1024]\n",
      "Loading TF weight model/h2/ln_1/g with shape [1024]\n",
      "Loading TF weight model/h2/ln_2/b with shape [1024]\n",
      "Loading TF weight model/h2/ln_2/g with shape [1024]\n",
      "Loading TF weight model/h2/mlp/c_fc/b with shape [4096]\n",
      "Loading TF weight model/h2/mlp/c_fc/w with shape [1, 1024, 4096]\n",
      "Loading TF weight model/h2/mlp/c_proj/b with shape [1024]\n",
      "Loading TF weight model/h2/mlp/c_proj/w with shape [1, 4096, 1024]\n",
      "Loading TF weight model/h20/attn/c_attn/b with shape [3072]\n",
      "Loading TF weight model/h20/attn/c_attn/w with shape [1, 1024, 3072]\n",
      "Loading TF weight model/h20/attn/c_proj/b with shape [1024]\n",
      "Loading TF weight model/h20/attn/c_proj/w with shape [1, 1024, 1024]\n",
      "Loading TF weight model/h20/ln_1/b with shape [1024]\n",
      "Loading TF weight model/h20/ln_1/g with shape [1024]\n",
      "Loading TF weight model/h20/ln_2/b with shape [1024]\n",
      "Loading TF weight model/h20/ln_2/g with shape [1024]\n",
      "Loading TF weight model/h20/mlp/c_fc/b with shape [4096]\n",
      "Loading TF weight model/h20/mlp/c_fc/w with shape [1, 1024, 4096]\n",
      "Loading TF weight model/h20/mlp/c_proj/b with shape [1024]\n",
      "Loading TF weight model/h20/mlp/c_proj/w with shape [1, 4096, 1024]\n",
      "Loading TF weight model/h21/attn/c_attn/b with shape [3072]\n",
      "Loading TF weight model/h21/attn/c_attn/w with shape [1, 1024, 3072]\n",
      "Loading TF weight model/h21/attn/c_proj/b with shape [1024]\n",
      "Loading TF weight model/h21/attn/c_proj/w with shape [1, 1024, 1024]\n",
      "Loading TF weight model/h21/ln_1/b with shape [1024]\n",
      "Loading TF weight model/h21/ln_1/g with shape [1024]\n",
      "Loading TF weight model/h21/ln_2/b with shape [1024]\n",
      "Loading TF weight model/h21/ln_2/g with shape [1024]\n",
      "Loading TF weight model/h21/mlp/c_fc/b with shape [4096]\n",
      "Loading TF weight model/h21/mlp/c_fc/w with shape [1, 1024, 4096]\n",
      "Loading TF weight model/h21/mlp/c_proj/b with shape [1024]\n",
      "Loading TF weight model/h21/mlp/c_proj/w with shape [1, 4096, 1024]\n",
      "Loading TF weight model/h22/attn/c_attn/b with shape [3072]\n",
      "Loading TF weight model/h22/attn/c_attn/w with shape [1, 1024, 3072]\n",
      "Loading TF weight model/h22/attn/c_proj/b with shape [1024]\n",
      "Loading TF weight model/h22/attn/c_proj/w with shape [1, 1024, 1024]\n",
      "Loading TF weight model/h22/ln_1/b with shape [1024]\n",
      "Loading TF weight model/h22/ln_1/g with shape [1024]\n",
      "Loading TF weight model/h22/ln_2/b with shape [1024]\n",
      "Loading TF weight model/h22/ln_2/g with shape [1024]\n",
      "Loading TF weight model/h22/mlp/c_fc/b with shape [4096]\n",
      "Loading TF weight model/h22/mlp/c_fc/w with shape [1, 1024, 4096]\n",
      "Loading TF weight model/h22/mlp/c_proj/b with shape [1024]\n",
      "Loading TF weight model/h22/mlp/c_proj/w with shape [1, 4096, 1024]\n",
      "Loading TF weight model/h23/attn/c_attn/b with shape [3072]\n",
      "Loading TF weight model/h23/attn/c_attn/w with shape [1, 1024, 3072]\n",
      "Loading TF weight model/h23/attn/c_proj/b with shape [1024]\n",
      "Loading TF weight model/h23/attn/c_proj/w with shape [1, 1024, 1024]\n",
      "Loading TF weight model/h23/ln_1/b with shape [1024]\n",
      "Loading TF weight model/h23/ln_1/g with shape [1024]\n",
      "Loading TF weight model/h23/ln_2/b with shape [1024]\n",
      "Loading TF weight model/h23/ln_2/g with shape [1024]\n",
      "Loading TF weight model/h23/mlp/c_fc/b with shape [4096]\n",
      "Loading TF weight model/h23/mlp/c_fc/w with shape [1, 1024, 4096]\n",
      "Loading TF weight model/h23/mlp/c_proj/b with shape [1024]\n",
      "Loading TF weight model/h23/mlp/c_proj/w with shape [1, 4096, 1024]\n",
      "Loading TF weight model/h3/attn/c_attn/b with shape [3072]\n",
      "Loading TF weight model/h3/attn/c_attn/w with shape [1, 1024, 3072]\n",
      "Loading TF weight model/h3/attn/c_proj/b with shape [1024]\n",
      "Loading TF weight model/h3/attn/c_proj/w with shape [1, 1024, 1024]\n",
      "Loading TF weight model/h3/ln_1/b with shape [1024]\n",
      "Loading TF weight model/h3/ln_1/g with shape [1024]\n",
      "Loading TF weight model/h3/ln_2/b with shape [1024]\n",
      "Loading TF weight model/h3/ln_2/g with shape [1024]\n",
      "Loading TF weight model/h3/mlp/c_fc/b with shape [4096]\n",
      "Loading TF weight model/h3/mlp/c_fc/w with shape [1, 1024, 4096]\n",
      "Loading TF weight model/h3/mlp/c_proj/b with shape [1024]\n",
      "Loading TF weight model/h3/mlp/c_proj/w with shape [1, 4096, 1024]\n",
      "Loading TF weight model/h4/attn/c_attn/b with shape [3072]\n",
      "Loading TF weight model/h4/attn/c_attn/w with shape [1, 1024, 3072]\n",
      "Loading TF weight model/h4/attn/c_proj/b with shape [1024]\n",
      "Loading TF weight model/h4/attn/c_proj/w with shape [1, 1024, 1024]\n",
      "Loading TF weight model/h4/ln_1/b with shape [1024]\n",
      "Loading TF weight model/h4/ln_1/g with shape [1024]\n",
      "Loading TF weight model/h4/ln_2/b with shape [1024]\n",
      "Loading TF weight model/h4/ln_2/g with shape [1024]\n",
      "Loading TF weight model/h4/mlp/c_fc/b with shape [4096]\n",
      "Loading TF weight model/h4/mlp/c_fc/w with shape [1, 1024, 4096]\n",
      "Loading TF weight model/h4/mlp/c_proj/b with shape [1024]\n",
      "Loading TF weight model/h4/mlp/c_proj/w with shape [1, 4096, 1024]\n",
      "Loading TF weight model/h5/attn/c_attn/b with shape [3072]\n",
      "Loading TF weight model/h5/attn/c_attn/w with shape [1, 1024, 3072]\n",
      "Loading TF weight model/h5/attn/c_proj/b with shape [1024]\n",
      "Loading TF weight model/h5/attn/c_proj/w with shape [1, 1024, 1024]\n",
      "Loading TF weight model/h5/ln_1/b with shape [1024]\n",
      "Loading TF weight model/h5/ln_1/g with shape [1024]\n",
      "Loading TF weight model/h5/ln_2/b with shape [1024]\n",
      "Loading TF weight model/h5/ln_2/g with shape [1024]\n",
      "Loading TF weight model/h5/mlp/c_fc/b with shape [4096]\n",
      "Loading TF weight model/h5/mlp/c_fc/w with shape [1, 1024, 4096]\n",
      "Loading TF weight model/h5/mlp/c_proj/b with shape [1024]\n",
      "Loading TF weight model/h5/mlp/c_proj/w with shape [1, 4096, 1024]\n",
      "Loading TF weight model/h6/attn/c_attn/b with shape [3072]\n",
      "Loading TF weight model/h6/attn/c_attn/w with shape [1, 1024, 3072]\n",
      "Loading TF weight model/h6/attn/c_proj/b with shape [1024]\n",
      "Loading TF weight model/h6/attn/c_proj/w with shape [1, 1024, 1024]\n",
      "Loading TF weight model/h6/ln_1/b with shape [1024]\n",
      "Loading TF weight model/h6/ln_1/g with shape [1024]\n",
      "Loading TF weight model/h6/ln_2/b with shape [1024]\n",
      "Loading TF weight model/h6/ln_2/g with shape [1024]\n",
      "Loading TF weight model/h6/mlp/c_fc/b with shape [4096]\n",
      "Loading TF weight model/h6/mlp/c_fc/w with shape [1, 1024, 4096]\n",
      "Loading TF weight model/h6/mlp/c_proj/b with shape [1024]\n",
      "Loading TF weight model/h6/mlp/c_proj/w with shape [1, 4096, 1024]\n",
      "Loading TF weight model/h7/attn/c_attn/b with shape [3072]\n",
      "Loading TF weight model/h7/attn/c_attn/w with shape [1, 1024, 3072]\n",
      "Loading TF weight model/h7/attn/c_proj/b with shape [1024]\n",
      "Loading TF weight model/h7/attn/c_proj/w with shape [1, 1024, 1024]\n",
      "Loading TF weight model/h7/ln_1/b with shape [1024]\n",
      "Loading TF weight model/h7/ln_1/g with shape [1024]\n",
      "Loading TF weight model/h7/ln_2/b with shape [1024]\n",
      "Loading TF weight model/h7/ln_2/g with shape [1024]\n",
      "Loading TF weight model/h7/mlp/c_fc/b with shape [4096]\n",
      "Loading TF weight model/h7/mlp/c_fc/w with shape [1, 1024, 4096]\n",
      "Loading TF weight model/h7/mlp/c_proj/b with shape [1024]\n",
      "Loading TF weight model/h7/mlp/c_proj/w with shape [1, 4096, 1024]\n",
      "Loading TF weight model/h8/attn/c_attn/b with shape [3072]\n",
      "Loading TF weight model/h8/attn/c_attn/w with shape [1, 1024, 3072]\n",
      "Loading TF weight model/h8/attn/c_proj/b with shape [1024]\n",
      "Loading TF weight model/h8/attn/c_proj/w with shape [1, 1024, 1024]\n",
      "Loading TF weight model/h8/ln_1/b with shape [1024]\n",
      "Loading TF weight model/h8/ln_1/g with shape [1024]\n",
      "Loading TF weight model/h8/ln_2/b with shape [1024]\n",
      "Loading TF weight model/h8/ln_2/g with shape [1024]\n",
      "Loading TF weight model/h8/mlp/c_fc/b with shape [4096]\n",
      "Loading TF weight model/h8/mlp/c_fc/w with shape [1, 1024, 4096]\n",
      "Loading TF weight model/h8/mlp/c_proj/b with shape [1024]\n",
      "Loading TF weight model/h8/mlp/c_proj/w with shape [1, 4096, 1024]\n",
      "Loading TF weight model/h9/attn/c_attn/b with shape [3072]\n",
      "Loading TF weight model/h9/attn/c_attn/w with shape [1, 1024, 3072]\n",
      "Loading TF weight model/h9/attn/c_proj/b with shape [1024]\n",
      "Loading TF weight model/h9/attn/c_proj/w with shape [1, 1024, 1024]\n",
      "Loading TF weight model/h9/ln_1/b with shape [1024]\n",
      "Loading TF weight model/h9/ln_1/g with shape [1024]\n",
      "Loading TF weight model/h9/ln_2/b with shape [1024]\n",
      "Loading TF weight model/h9/ln_2/g with shape [1024]\n",
      "Loading TF weight model/h9/mlp/c_fc/b with shape [4096]\n",
      "Loading TF weight model/h9/mlp/c_fc/w with shape [1, 1024, 4096]\n",
      "Loading TF weight model/h9/mlp/c_proj/b with shape [1024]\n",
      "Loading TF weight model/h9/mlp/c_proj/w with shape [1, 4096, 1024]\n",
      "Loading TF weight model/ln_f/b with shape [1024]\n",
      "Loading TF weight model/ln_f/g with shape [1024]\n",
      "Loading TF weight model/wpe with shape [1024, 1024]\n",
      "Loading TF weight model/wte with shape [50257, 1024]\n",
      "Initialize PyTorch weight ['h0', 'attn', 'c_attn', 'b']\r\n",
      "Initialize PyTorch weight ['h0', 'attn', 'c_attn', 'w']\r\n",
      "Initialize PyTorch weight ['h0', 'attn', 'c_proj', 'b']\r\n",
      "Initialize PyTorch weight ['h0', 'attn', 'c_proj', 'w']\r\n",
      "Initialize PyTorch weight ['h0', 'ln_1', 'b']\r\n",
      "Initialize PyTorch weight ['h0', 'ln_1', 'g']\r\n",
      "Initialize PyTorch weight ['h0', 'ln_2', 'b']\r\n",
      "Initialize PyTorch weight ['h0', 'ln_2', 'g']\r\n",
      "Initialize PyTorch weight ['h0', 'mlp', 'c_fc', 'b']\r\n",
      "Initialize PyTorch weight ['h0', 'mlp', 'c_fc', 'w']\r\n",
      "Initialize PyTorch weight ['h0', 'mlp', 'c_proj', 'b']\r\n",
      "Initialize PyTorch weight ['h0', 'mlp', 'c_proj', 'w']\r\n",
      "Initialize PyTorch weight ['h1', 'attn', 'c_attn', 'b']\r\n",
      "Initialize PyTorch weight ['h1', 'attn', 'c_attn', 'w']\r\n",
      "Initialize PyTorch weight ['h1', 'attn', 'c_proj', 'b']\r\n",
      "Initialize PyTorch weight ['h1', 'attn', 'c_proj', 'w']\r\n",
      "Initialize PyTorch weight ['h1', 'ln_1', 'b']\r\n",
      "Initialize PyTorch weight ['h1', 'ln_1', 'g']\r\n",
      "Initialize PyTorch weight ['h1', 'ln_2', 'b']\r\n",
      "Initialize PyTorch weight ['h1', 'ln_2', 'g']\r\n",
      "Initialize PyTorch weight ['h1', 'mlp', 'c_fc', 'b']\r\n",
      "Initialize PyTorch weight ['h1', 'mlp', 'c_fc', 'w']\r\n",
      "Initialize PyTorch weight ['h1', 'mlp', 'c_proj', 'b']\r\n",
      "Initialize PyTorch weight ['h1', 'mlp', 'c_proj', 'w']\r\n",
      "Initialize PyTorch weight ['h10', 'attn', 'c_attn', 'b']\r\n",
      "Initialize PyTorch weight ['h10', 'attn', 'c_attn', 'w']\r\n",
      "Initialize PyTorch weight ['h10', 'attn', 'c_proj', 'b']\r\n",
      "Initialize PyTorch weight ['h10', 'attn', 'c_proj', 'w']\r\n",
      "Initialize PyTorch weight ['h10', 'ln_1', 'b']\r\n",
      "Initialize PyTorch weight ['h10', 'ln_1', 'g']\r\n",
      "Initialize PyTorch weight ['h10', 'ln_2', 'b']\r\n",
      "Initialize PyTorch weight ['h10', 'ln_2', 'g']\r\n",
      "Initialize PyTorch weight ['h10', 'mlp', 'c_fc', 'b']\r\n",
      "Initialize PyTorch weight ['h10', 'mlp', 'c_fc', 'w']\r\n",
      "Initialize PyTorch weight ['h10', 'mlp', 'c_proj', 'b']\r\n",
      "Initialize PyTorch weight ['h10', 'mlp', 'c_proj', 'w']\r\n",
      "Initialize PyTorch weight ['h11', 'attn', 'c_attn', 'b']\r\n",
      "Initialize PyTorch weight ['h11', 'attn', 'c_attn', 'w']\r\n",
      "Initialize PyTorch weight ['h11', 'attn', 'c_proj', 'b']\r\n",
      "Initialize PyTorch weight ['h11', 'attn', 'c_proj', 'w']\r\n",
      "Initialize PyTorch weight ['h11', 'ln_1', 'b']\r\n",
      "Initialize PyTorch weight ['h11', 'ln_1', 'g']\r\n",
      "Initialize PyTorch weight ['h11', 'ln_2', 'b']\r\n",
      "Initialize PyTorch weight ['h11', 'ln_2', 'g']\r\n",
      "Initialize PyTorch weight ['h11', 'mlp', 'c_fc', 'b']\r\n",
      "Initialize PyTorch weight ['h11', 'mlp', 'c_fc', 'w']\r\n",
      "Initialize PyTorch weight ['h11', 'mlp', 'c_proj', 'b']\r\n",
      "Initialize PyTorch weight ['h11', 'mlp', 'c_proj', 'w']\r\n",
      "Initialize PyTorch weight ['h12', 'attn', 'c_attn', 'b']\r\n",
      "Initialize PyTorch weight ['h12', 'attn', 'c_attn', 'w']\r\n",
      "Initialize PyTorch weight ['h12', 'attn', 'c_proj', 'b']\r\n",
      "Initialize PyTorch weight ['h12', 'attn', 'c_proj', 'w']\r\n",
      "Initialize PyTorch weight ['h12', 'ln_1', 'b']\r\n",
      "Initialize PyTorch weight ['h12', 'ln_1', 'g']\r\n",
      "Initialize PyTorch weight ['h12', 'ln_2', 'b']\r\n",
      "Initialize PyTorch weight ['h12', 'ln_2', 'g']\r\n",
      "Initialize PyTorch weight ['h12', 'mlp', 'c_fc', 'b']\r\n",
      "Initialize PyTorch weight ['h12', 'mlp', 'c_fc', 'w']\r\n",
      "Initialize PyTorch weight ['h12', 'mlp', 'c_proj', 'b']\r\n",
      "Initialize PyTorch weight ['h12', 'mlp', 'c_proj', 'w']\r\n",
      "Initialize PyTorch weight ['h13', 'attn', 'c_attn', 'b']\r\n",
      "Initialize PyTorch weight ['h13', 'attn', 'c_attn', 'w']\r\n",
      "Initialize PyTorch weight ['h13', 'attn', 'c_proj', 'b']\r\n",
      "Initialize PyTorch weight ['h13', 'attn', 'c_proj', 'w']\r\n",
      "Initialize PyTorch weight ['h13', 'ln_1', 'b']\r\n",
      "Initialize PyTorch weight ['h13', 'ln_1', 'g']\r\n",
      "Initialize PyTorch weight ['h13', 'ln_2', 'b']\r\n",
      "Initialize PyTorch weight ['h13', 'ln_2', 'g']\r\n",
      "Initialize PyTorch weight ['h13', 'mlp', 'c_fc', 'b']\r\n",
      "Initialize PyTorch weight ['h13', 'mlp', 'c_fc', 'w']\r\n",
      "Initialize PyTorch weight ['h13', 'mlp', 'c_proj', 'b']\r\n",
      "Initialize PyTorch weight ['h13', 'mlp', 'c_proj', 'w']\r\n",
      "Initialize PyTorch weight ['h14', 'attn', 'c_attn', 'b']\r\n",
      "Initialize PyTorch weight ['h14', 'attn', 'c_attn', 'w']\r\n",
      "Initialize PyTorch weight ['h14', 'attn', 'c_proj', 'b']\r\n",
      "Initialize PyTorch weight ['h14', 'attn', 'c_proj', 'w']\r\n",
      "Initialize PyTorch weight ['h14', 'ln_1', 'b']\r\n",
      "Initialize PyTorch weight ['h14', 'ln_1', 'g']\r\n",
      "Initialize PyTorch weight ['h14', 'ln_2', 'b']\r\n",
      "Initialize PyTorch weight ['h14', 'ln_2', 'g']\r\n",
      "Initialize PyTorch weight ['h14', 'mlp', 'c_fc', 'b']\r\n",
      "Initialize PyTorch weight ['h14', 'mlp', 'c_fc', 'w']\r\n",
      "Initialize PyTorch weight ['h14', 'mlp', 'c_proj', 'b']\r\n",
      "Initialize PyTorch weight ['h14', 'mlp', 'c_proj', 'w']\r\n",
      "Initialize PyTorch weight ['h15', 'attn', 'c_attn', 'b']\r\n",
      "Initialize PyTorch weight ['h15', 'attn', 'c_attn', 'w']\r\n",
      "Initialize PyTorch weight ['h15', 'attn', 'c_proj', 'b']\r\n",
      "Initialize PyTorch weight ['h15', 'attn', 'c_proj', 'w']\r\n",
      "Initialize PyTorch weight ['h15', 'ln_1', 'b']\r\n",
      "Initialize PyTorch weight ['h15', 'ln_1', 'g']\r\n",
      "Initialize PyTorch weight ['h15', 'ln_2', 'b']\r\n",
      "Initialize PyTorch weight ['h15', 'ln_2', 'g']\r\n",
      "Initialize PyTorch weight ['h15', 'mlp', 'c_fc', 'b']\r\n",
      "Initialize PyTorch weight ['h15', 'mlp', 'c_fc', 'w']\r\n",
      "Initialize PyTorch weight ['h15', 'mlp', 'c_proj', 'b']\r\n",
      "Initialize PyTorch weight ['h15', 'mlp', 'c_proj', 'w']\r\n",
      "Initialize PyTorch weight ['h16', 'attn', 'c_attn', 'b']\r\n",
      "Initialize PyTorch weight ['h16', 'attn', 'c_attn', 'w']\r\n",
      "Initialize PyTorch weight ['h16', 'attn', 'c_proj', 'b']\r\n",
      "Initialize PyTorch weight ['h16', 'attn', 'c_proj', 'w']\r\n",
      "Initialize PyTorch weight ['h16', 'ln_1', 'b']\r\n",
      "Initialize PyTorch weight ['h16', 'ln_1', 'g']\r\n",
      "Initialize PyTorch weight ['h16', 'ln_2', 'b']\r\n",
      "Initialize PyTorch weight ['h16', 'ln_2', 'g']\r\n",
      "Initialize PyTorch weight ['h16', 'mlp', 'c_fc', 'b']\r\n",
      "Initialize PyTorch weight ['h16', 'mlp', 'c_fc', 'w']\r\n",
      "Initialize PyTorch weight ['h16', 'mlp', 'c_proj', 'b']\r\n",
      "Initialize PyTorch weight ['h16', 'mlp', 'c_proj', 'w']\r\n",
      "Initialize PyTorch weight ['h17', 'attn', 'c_attn', 'b']\r\n",
      "Initialize PyTorch weight ['h17', 'attn', 'c_attn', 'w']\r\n",
      "Initialize PyTorch weight ['h17', 'attn', 'c_proj', 'b']\r\n",
      "Initialize PyTorch weight ['h17', 'attn', 'c_proj', 'w']\r\n",
      "Initialize PyTorch weight ['h17', 'ln_1', 'b']\r\n",
      "Initialize PyTorch weight ['h17', 'ln_1', 'g']\r\n",
      "Initialize PyTorch weight ['h17', 'ln_2', 'b']\r\n",
      "Initialize PyTorch weight ['h17', 'ln_2', 'g']\r\n",
      "Initialize PyTorch weight ['h17', 'mlp', 'c_fc', 'b']\r\n",
      "Initialize PyTorch weight ['h17', 'mlp', 'c_fc', 'w']\r\n",
      "Initialize PyTorch weight ['h17', 'mlp', 'c_proj', 'b']\r\n",
      "Initialize PyTorch weight ['h17', 'mlp', 'c_proj', 'w']\r\n",
      "Initialize PyTorch weight ['h18', 'attn', 'c_attn', 'b']\r\n",
      "Initialize PyTorch weight ['h18', 'attn', 'c_attn', 'w']\r\n",
      "Initialize PyTorch weight ['h18', 'attn', 'c_proj', 'b']\r\n",
      "Initialize PyTorch weight ['h18', 'attn', 'c_proj', 'w']\r\n",
      "Initialize PyTorch weight ['h18', 'ln_1', 'b']\r\n",
      "Initialize PyTorch weight ['h18', 'ln_1', 'g']\r\n",
      "Initialize PyTorch weight ['h18', 'ln_2', 'b']\r\n",
      "Initialize PyTorch weight ['h18', 'ln_2', 'g']\r\n",
      "Initialize PyTorch weight ['h18', 'mlp', 'c_fc', 'b']\r\n",
      "Initialize PyTorch weight ['h18', 'mlp', 'c_fc', 'w']\r\n",
      "Initialize PyTorch weight ['h18', 'mlp', 'c_proj', 'b']\r\n",
      "Initialize PyTorch weight ['h18', 'mlp', 'c_proj', 'w']\r\n",
      "Initialize PyTorch weight ['h19', 'attn', 'c_attn', 'b']\r\n",
      "Initialize PyTorch weight ['h19', 'attn', 'c_attn', 'w']\r\n",
      "Initialize PyTorch weight ['h19', 'attn', 'c_proj', 'b']\r\n",
      "Initialize PyTorch weight ['h19', 'attn', 'c_proj', 'w']\r\n",
      "Initialize PyTorch weight ['h19', 'ln_1', 'b']\r\n",
      "Initialize PyTorch weight ['h19', 'ln_1', 'g']\r\n",
      "Initialize PyTorch weight ['h19', 'ln_2', 'b']\r\n",
      "Initialize PyTorch weight ['h19', 'ln_2', 'g']\r\n",
      "Initialize PyTorch weight ['h19', 'mlp', 'c_fc', 'b']\r\n",
      "Initialize PyTorch weight ['h19', 'mlp', 'c_fc', 'w']\r\n",
      "Initialize PyTorch weight ['h19', 'mlp', 'c_proj', 'b']\r\n",
      "Initialize PyTorch weight ['h19', 'mlp', 'c_proj', 'w']\r\n",
      "Initialize PyTorch weight ['h2', 'attn', 'c_attn', 'b']\r\n",
      "Initialize PyTorch weight ['h2', 'attn', 'c_attn', 'w']\r\n",
      "Initialize PyTorch weight ['h2', 'attn', 'c_proj', 'b']\r\n",
      "Initialize PyTorch weight ['h2', 'attn', 'c_proj', 'w']\r\n",
      "Initialize PyTorch weight ['h2', 'ln_1', 'b']\r\n",
      "Initialize PyTorch weight ['h2', 'ln_1', 'g']\r\n",
      "Initialize PyTorch weight ['h2', 'ln_2', 'b']\r\n",
      "Initialize PyTorch weight ['h2', 'ln_2', 'g']\r\n",
      "Initialize PyTorch weight ['h2', 'mlp', 'c_fc', 'b']\r\n",
      "Initialize PyTorch weight ['h2', 'mlp', 'c_fc', 'w']\r\n",
      "Initialize PyTorch weight ['h2', 'mlp', 'c_proj', 'b']\r\n",
      "Initialize PyTorch weight ['h2', 'mlp', 'c_proj', 'w']\r\n",
      "Initialize PyTorch weight ['h20', 'attn', 'c_attn', 'b']\r\n",
      "Initialize PyTorch weight ['h20', 'attn', 'c_attn', 'w']\r\n",
      "Initialize PyTorch weight ['h20', 'attn', 'c_proj', 'b']\r\n",
      "Initialize PyTorch weight ['h20', 'attn', 'c_proj', 'w']\r\n",
      "Initialize PyTorch weight ['h20', 'ln_1', 'b']\r\n",
      "Initialize PyTorch weight ['h20', 'ln_1', 'g']\r\n",
      "Initialize PyTorch weight ['h20', 'ln_2', 'b']\r\n",
      "Initialize PyTorch weight ['h20', 'ln_2', 'g']\r\n",
      "Initialize PyTorch weight ['h20', 'mlp', 'c_fc', 'b']\r\n",
      "Initialize PyTorch weight ['h20', 'mlp', 'c_fc', 'w']\r\n",
      "Initialize PyTorch weight ['h20', 'mlp', 'c_proj', 'b']\r\n",
      "Initialize PyTorch weight ['h20', 'mlp', 'c_proj', 'w']\r\n",
      "Initialize PyTorch weight ['h21', 'attn', 'c_attn', 'b']\r\n",
      "Initialize PyTorch weight ['h21', 'attn', 'c_attn', 'w']\r\n",
      "Initialize PyTorch weight ['h21', 'attn', 'c_proj', 'b']\r\n",
      "Initialize PyTorch weight ['h21', 'attn', 'c_proj', 'w']\r\n",
      "Initialize PyTorch weight ['h21', 'ln_1', 'b']\r\n",
      "Initialize PyTorch weight ['h21', 'ln_1', 'g']\r\n",
      "Initialize PyTorch weight ['h21', 'ln_2', 'b']\r\n",
      "Initialize PyTorch weight ['h21', 'ln_2', 'g']\r\n",
      "Initialize PyTorch weight ['h21', 'mlp', 'c_fc', 'b']\r\n",
      "Initialize PyTorch weight ['h21', 'mlp', 'c_fc', 'w']\r\n",
      "Initialize PyTorch weight ['h21', 'mlp', 'c_proj', 'b']\r\n",
      "Initialize PyTorch weight ['h21', 'mlp', 'c_proj', 'w']\r\n",
      "Initialize PyTorch weight ['h22', 'attn', 'c_attn', 'b']\r\n",
      "Initialize PyTorch weight ['h22', 'attn', 'c_attn', 'w']\r\n",
      "Initialize PyTorch weight ['h22', 'attn', 'c_proj', 'b']\r\n",
      "Initialize PyTorch weight ['h22', 'attn', 'c_proj', 'w']\r\n",
      "Initialize PyTorch weight ['h22', 'ln_1', 'b']\r\n",
      "Initialize PyTorch weight ['h22', 'ln_1', 'g']\r\n",
      "Initialize PyTorch weight ['h22', 'ln_2', 'b']\r\n",
      "Initialize PyTorch weight ['h22', 'ln_2', 'g']\r\n",
      "Initialize PyTorch weight ['h22', 'mlp', 'c_fc', 'b']\r\n",
      "Initialize PyTorch weight ['h22', 'mlp', 'c_fc', 'w']\r\n",
      "Initialize PyTorch weight ['h22', 'mlp', 'c_proj', 'b']\r\n",
      "Initialize PyTorch weight ['h22', 'mlp', 'c_proj', 'w']\r\n",
      "Initialize PyTorch weight ['h23', 'attn', 'c_attn', 'b']\r\n",
      "Initialize PyTorch weight ['h23', 'attn', 'c_attn', 'w']\r\n",
      "Initialize PyTorch weight ['h23', 'attn', 'c_proj', 'b']\r\n",
      "Initialize PyTorch weight ['h23', 'attn', 'c_proj', 'w']\r\n",
      "Initialize PyTorch weight ['h23', 'ln_1', 'b']\r\n",
      "Initialize PyTorch weight ['h23', 'ln_1', 'g']\r\n",
      "Initialize PyTorch weight ['h23', 'ln_2', 'b']\r\n",
      "Initialize PyTorch weight ['h23', 'ln_2', 'g']\r\n",
      "Initialize PyTorch weight ['h23', 'mlp', 'c_fc', 'b']\r\n",
      "Initialize PyTorch weight ['h23', 'mlp', 'c_fc', 'w']\r\n",
      "Initialize PyTorch weight ['h23', 'mlp', 'c_proj', 'b']\r\n",
      "Initialize PyTorch weight ['h23', 'mlp', 'c_proj', 'w']\r\n",
      "Initialize PyTorch weight ['h3', 'attn', 'c_attn', 'b']\r\n",
      "Initialize PyTorch weight ['h3', 'attn', 'c_attn', 'w']\r\n",
      "Initialize PyTorch weight ['h3', 'attn', 'c_proj', 'b']\r\n",
      "Initialize PyTorch weight ['h3', 'attn', 'c_proj', 'w']\r\n",
      "Initialize PyTorch weight ['h3', 'ln_1', 'b']\r\n",
      "Initialize PyTorch weight ['h3', 'ln_1', 'g']\r\n",
      "Initialize PyTorch weight ['h3', 'ln_2', 'b']\r\n",
      "Initialize PyTorch weight ['h3', 'ln_2', 'g']\r\n",
      "Initialize PyTorch weight ['h3', 'mlp', 'c_fc', 'b']\r\n",
      "Initialize PyTorch weight ['h3', 'mlp', 'c_fc', 'w']\r\n",
      "Initialize PyTorch weight ['h3', 'mlp', 'c_proj', 'b']\r\n",
      "Initialize PyTorch weight ['h3', 'mlp', 'c_proj', 'w']\r\n",
      "Initialize PyTorch weight ['h4', 'attn', 'c_attn', 'b']\r\n",
      "Initialize PyTorch weight ['h4', 'attn', 'c_attn', 'w']\r\n",
      "Initialize PyTorch weight ['h4', 'attn', 'c_proj', 'b']\r\n",
      "Initialize PyTorch weight ['h4', 'attn', 'c_proj', 'w']\r\n",
      "Initialize PyTorch weight ['h4', 'ln_1', 'b']\r\n",
      "Initialize PyTorch weight ['h4', 'ln_1', 'g']\r\n",
      "Initialize PyTorch weight ['h4', 'ln_2', 'b']\r\n",
      "Initialize PyTorch weight ['h4', 'ln_2', 'g']\r\n",
      "Initialize PyTorch weight ['h4', 'mlp', 'c_fc', 'b']\r\n",
      "Initialize PyTorch weight ['h4', 'mlp', 'c_fc', 'w']\r\n",
      "Initialize PyTorch weight ['h4', 'mlp', 'c_proj', 'b']\r\n",
      "Initialize PyTorch weight ['h4', 'mlp', 'c_proj', 'w']\r\n",
      "Initialize PyTorch weight ['h5', 'attn', 'c_attn', 'b']\r\n",
      "Initialize PyTorch weight ['h5', 'attn', 'c_attn', 'w']\r\n",
      "Initialize PyTorch weight ['h5', 'attn', 'c_proj', 'b']\r\n",
      "Initialize PyTorch weight ['h5', 'attn', 'c_proj', 'w']\r\n",
      "Initialize PyTorch weight ['h5', 'ln_1', 'b']\r\n",
      "Initialize PyTorch weight ['h5', 'ln_1', 'g']\r\n",
      "Initialize PyTorch weight ['h5', 'ln_2', 'b']\r\n",
      "Initialize PyTorch weight ['h5', 'ln_2', 'g']\r\n",
      "Initialize PyTorch weight ['h5', 'mlp', 'c_fc', 'b']\r\n",
      "Initialize PyTorch weight ['h5', 'mlp', 'c_fc', 'w']\r\n",
      "Initialize PyTorch weight ['h5', 'mlp', 'c_proj', 'b']\r\n",
      "Initialize PyTorch weight ['h5', 'mlp', 'c_proj', 'w']\r\n",
      "Initialize PyTorch weight ['h6', 'attn', 'c_attn', 'b']\r\n",
      "Initialize PyTorch weight ['h6', 'attn', 'c_attn', 'w']\r\n",
      "Initialize PyTorch weight ['h6', 'attn', 'c_proj', 'b']\r\n",
      "Initialize PyTorch weight ['h6', 'attn', 'c_proj', 'w']\r\n",
      "Initialize PyTorch weight ['h6', 'ln_1', 'b']\r\n",
      "Initialize PyTorch weight ['h6', 'ln_1', 'g']\r\n",
      "Initialize PyTorch weight ['h6', 'ln_2', 'b']\r\n",
      "Initialize PyTorch weight ['h6', 'ln_2', 'g']\r\n",
      "Initialize PyTorch weight ['h6', 'mlp', 'c_fc', 'b']\r\n",
      "Initialize PyTorch weight ['h6', 'mlp', 'c_fc', 'w']\r\n",
      "Initialize PyTorch weight ['h6', 'mlp', 'c_proj', 'b']\r\n",
      "Initialize PyTorch weight ['h6', 'mlp', 'c_proj', 'w']\r\n",
      "Initialize PyTorch weight ['h7', 'attn', 'c_attn', 'b']\r\n",
      "Initialize PyTorch weight ['h7', 'attn', 'c_attn', 'w']\r\n",
      "Initialize PyTorch weight ['h7', 'attn', 'c_proj', 'b']\r\n",
      "Initialize PyTorch weight ['h7', 'attn', 'c_proj', 'w']\r\n",
      "Initialize PyTorch weight ['h7', 'ln_1', 'b']\r\n",
      "Initialize PyTorch weight ['h7', 'ln_1', 'g']\r\n",
      "Initialize PyTorch weight ['h7', 'ln_2', 'b']\r\n",
      "Initialize PyTorch weight ['h7', 'ln_2', 'g']\r\n",
      "Initialize PyTorch weight ['h7', 'mlp', 'c_fc', 'b']\r\n",
      "Initialize PyTorch weight ['h7', 'mlp', 'c_fc', 'w']\r\n",
      "Initialize PyTorch weight ['h7', 'mlp', 'c_proj', 'b']\r\n",
      "Initialize PyTorch weight ['h7', 'mlp', 'c_proj', 'w']\r\n",
      "Initialize PyTorch weight ['h8', 'attn', 'c_attn', 'b']\r\n",
      "Initialize PyTorch weight ['h8', 'attn', 'c_attn', 'w']\r\n",
      "Initialize PyTorch weight ['h8', 'attn', 'c_proj', 'b']\r\n",
      "Initialize PyTorch weight ['h8', 'attn', 'c_proj', 'w']\r\n",
      "Initialize PyTorch weight ['h8', 'ln_1', 'b']\r\n",
      "Initialize PyTorch weight ['h8', 'ln_1', 'g']\r\n",
      "Initialize PyTorch weight ['h8', 'ln_2', 'b']\r\n",
      "Initialize PyTorch weight ['h8', 'ln_2', 'g']\r\n",
      "Initialize PyTorch weight ['h8', 'mlp', 'c_fc', 'b']\r\n",
      "Initialize PyTorch weight ['h8', 'mlp', 'c_fc', 'w']\r\n",
      "Initialize PyTorch weight ['h8', 'mlp', 'c_proj', 'b']\r\n",
      "Initialize PyTorch weight ['h8', 'mlp', 'c_proj', 'w']\r\n",
      "Initialize PyTorch weight ['h9', 'attn', 'c_attn', 'b']\r\n",
      "Initialize PyTorch weight ['h9', 'attn', 'c_attn', 'w']\r\n",
      "Initialize PyTorch weight ['h9', 'attn', 'c_proj', 'b']\r\n",
      "Initialize PyTorch weight ['h9', 'attn', 'c_proj', 'w']\r\n",
      "Initialize PyTorch weight ['h9', 'ln_1', 'b']\r\n",
      "Initialize PyTorch weight ['h9', 'ln_1', 'g']\r\n",
      "Initialize PyTorch weight ['h9', 'ln_2', 'b']\r\n",
      "Initialize PyTorch weight ['h9', 'ln_2', 'g']\r\n",
      "Initialize PyTorch weight ['h9', 'mlp', 'c_fc', 'b']\r\n",
      "Initialize PyTorch weight ['h9', 'mlp', 'c_fc', 'w']\r\n",
      "Initialize PyTorch weight ['h9', 'mlp', 'c_proj', 'b']\r\n",
      "Initialize PyTorch weight ['h9', 'mlp', 'c_proj', 'w']\n",
      "Initialize PyTorch weight ['ln_f', 'b']\n",
      "Initialize PyTorch weight ['ln_f', 'g']\n",
      "Initialize PyTorch weight ['wpe']\n",
      "Initialize PyTorch weight ['wte']\n",
      "Save PyTorch model to gpt2-345m-pt/pytorch_model.bin\n",
      "Save configuration file to gpt2-345m-pt/config.json\n"
     ]
    }
   ],
   "source": [
    "!transformers-cli convert --model_type gpt2 \\\n",
    "  --tf_checkpoint gpt2-345m/model.ckpt \\\n",
    "  --pytorch_dump_output gpt2-345m-pt \\\n",
    "  --config config.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3250db88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Config, GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef360ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained('./gpt2-345m-pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5879703",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('gpt2-345m-pt/tokenizer_config.json',\n",
       " 'gpt2-345m-pt/special_tokens_map.json',\n",
       " 'gpt2-345m-pt/vocab.json',\n",
       " 'gpt2-345m-pt/merges.txt',\n",
       " 'gpt2-345m-pt/added_tokens.json')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer('gpt2-encoder.json', 'gpt2-vocab.bpe')\n",
    "tokenizer.save_pretrained('gpt2-345m-pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d93f42c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  77, 1228,  571,  374, 1031,  461,  512,  282,  993,  384,  273,  648]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('najib razak adalah seorang', return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "21863182",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "g = model.generate(tokenizer.encode('Najib Razak adalah seorang', return_tensors='pt'),\n",
    "                  do_sample=True, \n",
    "    max_length=100, \n",
    "    top_p=0.95, \n",
    "    top_k=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f5248076",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Najib Razak adalah seorang ahli perniagaan yang kaya dan menguntungkan.\\nApabila kesunyian menjadi nyata, saya membaca mengenai Perdana Menteri Tony Blair: 'Ada perang selanjutnya?\\nSaya terbang ke Washington untuk mesyuarat perniagaan tiga tahun di sana.\\n\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(g[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c3485e9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/husein/.local/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:38: FutureWarning: Deprecated positional argument(s) used in 'create_repo': pass token='gpt2-345m-bahasa-cased' as keyword args. From version 0.12 passing these as positional arguments will result in an error,\n",
      "  warnings.warn(\n",
      "/home/husein/.local/lib/python3.8/site-packages/huggingface_hub/hf_api.py:102: FutureWarning: `name` and `organization` input arguments are deprecated and will be removed in v0.10. Pass `repo_id` instead.\n",
      "  warnings.warn(\n",
      "/home/husein/.local/lib/python3.8/site-packages/huggingface_hub/hf_api.py:681: FutureWarning: `create_repo` now takes `token` as an optional positional argument. Be sure to adapt your code!\n",
      "  warnings.warn(\n",
      "Cloning https://huggingface.co/mesolitica/gpt2-345m-bahasa-cased into local empty directory.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30e986e0d19140d3b9c46680b7f0152a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file pytorch_model.bin:   0%|          | 32.0k/1.35G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "remote: Scanning LFS files for validity, may be slow...        \n",
      "remote: LFS file scan complete.        \n",
      "To https://huggingface.co/mesolitica/gpt2-345m-bahasa-cased\n",
      "   16b6b3c..1c24f8b  main -> main\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://huggingface.co/mesolitica/gpt2-345m-bahasa-cased/commit/1c24f8bfe0f8541df5794994e32abc5a2b66f3b8'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.push_to_hub('gpt2-345m-bahasa-cased', organization='mesolitica')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "88742d1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To https://huggingface.co/mesolitica/gpt2-345m-bahasa-cased\n",
      "   1c24f8b..ba649e0  main -> main\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://huggingface.co/mesolitica/gpt2-345m-bahasa-cased/commit/ba649e05582238c96273db4ccb6cd04c535a7c3c'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.push_to_hub('gpt2-345m-bahasa-cased', organization='mesolitica')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
