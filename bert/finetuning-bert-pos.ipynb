{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-06-17 11:16:24--  https://raw.githubusercontent.com/huseinzol05/Malaya-Dataset/master/part-of-speech/pos-data-v3.json\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.8.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.8.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3085086 (2.9M) [text/plain]\n",
      "Saving to: ‘pos-data-v3.json.1’\n",
      "\n",
      "pos-data-v3.json.1  100%[===================>]   2.94M  --.-KB/s    in 0.06s   \n",
      "\n",
      "2019-06-17 11:16:24 (47.7 MB/s) - ‘pos-data-v3.json.1’ saved [3085086/3085086]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/huseinzol05/Malaya-Dataset/master/part-of-speech/pos-data-v3.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pos-data-v3.json') as fopen:\n",
    "    data = json.load(fopen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_train = [i[0] for i in data]\n",
    "right_train = [i[-1] for i in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 50\n",
    "def iter_seq(x):\n",
    "    return np.array([x[i: i+seq_len] for i in range(0, len(x)-seq_len, 1)])\n",
    "\n",
    "def to_train_seq(*args):\n",
    "    return [iter_seq(x) for x in args]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_train, right_train = to_train_seq(left_train, right_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PAD': 0,\n",
       " 'ADJ': 1,\n",
       " 'ADP': 2,\n",
       " 'ADV': 3,\n",
       " 'AUX': 4,\n",
       " 'CCONJ': 5,\n",
       " 'DET': 6,\n",
       " 'NOUN': 7,\n",
       " 'NUM': 8,\n",
       " 'PART': 9,\n",
       " 'PRON': 10,\n",
       " 'PROPN': 11,\n",
       " 'SCONJ': 12,\n",
       " 'SYM': 13,\n",
       " 'VERB': 14,\n",
       " 'X': 15}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag2idx = {'PAD': 0}\n",
    "for no, u in enumerate(np.unique(right_train)):\n",
    "    tag2idx[u] = no + 1\n",
    "tag2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['Sampul', 'dari', 'dua', ..., 'dan', 'dirilis', 'pada'],\n",
       "       ['dari', 'dua', 'singel', ..., 'dirilis', 'pada', 'tanggal'],\n",
       "       ['dua', 'singel', 'pertama', ..., 'pada', 'tanggal', '21'],\n",
       "       ...,\n",
       "       ['di', 'kalender', 'Yahudi', ..., 'General', 'Superintendent',\n",
       "        'UPC'],\n",
       "       ['kalender', 'Yahudi', 'Cibodas', ..., 'Superintendent', 'UPC',\n",
       "        'di'],\n",
       "       ['Yahudi', 'Cibodas', 'Baru', ..., 'UPC', 'di', 'Amerika']],\n",
       "      dtype='<U25')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "left_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_INIT_CHKPNT = 'bert-emotion/model.ckpt'\n",
    "BERT_CONFIG = 'bert-bahasa/config.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import itertools\n",
    "from unidecode import unidecode\n",
    "import malaya\n",
    "\n",
    "_tokenizer = malaya.preprocessing._SocialTokenizer().tokenize\n",
    "rules_normalizer = malaya.texts._tatabahasa.rules_normalizer\n",
    "rejected = ['wkwk', 'http', 'https', 'lolol', 'hahaha']\n",
    "\n",
    "def is_number_regex(s):\n",
    "    if re.match(\"^\\d+?\\.\\d+?$\", s) is None:\n",
    "        return s.isdigit()\n",
    "    return True\n",
    "\n",
    "def detect_money(word):\n",
    "    if word[:2] == 'rm' and is_number_regex(word[2:]):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def preprocessing(string):\n",
    "    string = ''.join(''.join(s)[:2] for _, s in itertools.groupby(unidecode(string)))\n",
    "    tokenized = _tokenizer(string)\n",
    "    tokenized = [malaya.stem.naive(w) for w in tokenized]\n",
    "    tokenized = [w.lower() for w in tokenized if len(w) > 1]\n",
    "    tokenized = [w for w in tokenized if all([r not in w for r in rejected])]\n",
    "    tokenized = [rules_normalizer.get(w, w) for w in tokenized]\n",
    "    tokenized = ['<NUM>' if is_number_regex(w) else w for w in tokenized]\n",
    "    tokenized = ['<MONEY>' if detect_money(w) else w for w in tokenized]\n",
    "    return tokenized\n",
    "\n",
    "with open('dictionary.json') as fopen:\n",
    "    d = json.load(fopen)\n",
    "dictionary = d['dictionary']\n",
    "rev_dictionary = d['reverse_dictionary']\n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self, vocab, rev_dictionary):\n",
    "        self.vocab = vocab\n",
    "        self.inv_vocab = rev_dictionary\n",
    "    \n",
    "    def tokenize(self, string):\n",
    "        return preprocessing(string)\n",
    "    \n",
    "    def convert_tokens_to_ids(self, tokens):\n",
    "        return [self.vocab.get(t, 1) for t in tokens]\n",
    "    \n",
    "    def convert_ids_to_tokens(self, ids):\n",
    "        return [self.inv_vocab[i] for i in ids]\n",
    "    \n",
    "tokenizer = Tokenizer(dictionary, rev_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def XY(left_train, right_train):\n",
    "    X, Y = [], []\n",
    "    for i in tqdm(range(len(left_train))):\n",
    "        left = left_train[i]\n",
    "        right = right_train[i]\n",
    "        bert_tokens = ['[CLS]']\n",
    "        y = ['PAD']\n",
    "        for no, orig_token in enumerate(left):\n",
    "            t = tokenizer.tokenize(orig_token)\n",
    "            if len(t):\n",
    "                y.append(right[no])\n",
    "                bert_tokens.extend(t)\n",
    "                y.extend(['PAD'] * (len(t) - 1))\n",
    "        bert_tokens.append(\"[SEP]\")\n",
    "        y.append('PAD')\n",
    "        X.append(tokenizer.convert_tokens_to_ids(bert_tokens))\n",
    "        Y.append([tag2idx[i] for i in y])\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 103645/103645 [02:12<00:00, 783.64it/s]\n"
     ]
    }
   ],
   "source": [
    "train_X, train_Y = XY(left_train, right_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((103645, 55), (103645, 55))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "train_X = keras.preprocessing.sequence.pad_sequences(train_X, padding='post')\n",
    "train_Y = keras.preprocessing.sequence.pad_sequences(train_Y, padding='post')\n",
    "train_X.shape, train_Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bert\n",
    "from bert import run_classifier\n",
    "from bert import optimization\n",
    "from bert import tokenization\n",
    "from bert import modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 3\n",
    "batch_size = 16\n",
    "warmup_proportion = 0.1\n",
    "num_train_steps = int(len(train_X) / batch_size * epoch)\n",
    "num_warmup_steps = int(num_train_steps * warmup_proportion)\n",
    "bert_config = modeling.BertConfig.from_json_file(BERT_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(\n",
    "        self,\n",
    "        dimension_output,\n",
    "        learning_rate = 2e-5,\n",
    "    ):\n",
    "        self.X = tf.placeholder(tf.int32, [None, None])\n",
    "        self.Y = tf.placeholder(tf.int32, [None, None])\n",
    "        self.maxlen = tf.shape(self.X)[1]\n",
    "        self.lengths = tf.count_nonzero(self.X, 1)\n",
    "        \n",
    "        model = modeling.BertModel(\n",
    "            config=bert_config,\n",
    "            is_training=True,\n",
    "            input_ids=self.X,\n",
    "            use_one_hot_embeddings=False)\n",
    "        output_layer = model.get_sequence_output()\n",
    "        logits = tf.layers.dense(output_layer, dimension_output)\n",
    "        y_t = self.Y\n",
    "        log_likelihood, transition_params = tf.contrib.crf.crf_log_likelihood(\n",
    "            logits, y_t, self.lengths\n",
    "        )\n",
    "        self.cost = tf.reduce_mean(-log_likelihood)\n",
    "        self.optimizer = tf.train.AdamOptimizer(\n",
    "            learning_rate = learning_rate\n",
    "        ).minimize(self.cost)\n",
    "        mask = tf.sequence_mask(self.lengths, maxlen = self.maxlen)\n",
    "        self.tags_seq, tags_score = tf.contrib.crf.crf_decode(\n",
    "            logits, transition_params, self.lengths\n",
    "        )\n",
    "        self.tags_seq = tf.identity(self.tags_seq, name = 'logits')\n",
    "\n",
    "        y_t = tf.cast(y_t, tf.int32)\n",
    "        self.prediction = tf.boolean_mask(self.tags_seq, mask)\n",
    "        mask_label = tf.boolean_mask(y_t, mask)\n",
    "        correct_pred = tf.equal(self.prediction, mask_label)\n",
    "        correct_index = tf.cast(correct_pred, tf.float32)\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/jupyter/.local/lib/python3.6/site-packages/bert/modeling.py:358: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /home/jupyter/.local/lib/python3.6/site-packages/bert/modeling.py:671: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/crf/python/ops/crf.py:213: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn.py:626: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from bert-emotion/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "dimension_output = len(tag2idx)\n",
    "learning_rate = 2e-5\n",
    "\n",
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "model = Model(\n",
    "    dimension_output,\n",
    "    learning_rate\n",
    ")\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "var_lists = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope = 'bert')\n",
    "saver = tf.train.Saver(var_list = var_lists)\n",
    "saver.restore(sess, BERT_INIT_CHKPNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 6478/6478 [13:24<00:00,  8.19it/s, accuracy=0.905, cost=9.56]\n",
      "train minibatch loop:   0%|          | 1/6478 [00:00<12:42,  8.50it/s, accuracy=0.872, cost=15.2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken: 804.5755860805511\n",
      "epoch: 0, training loss: 16.775186, training acc: 0.874612\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 6478/6478 [13:24<00:00,  8.27it/s, accuracy=0.951, cost=5.92] \n",
      "train minibatch loop:   0%|          | 1/6478 [00:00<13:03,  8.26it/s, accuracy=0.911, cost=12.4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken: 804.7049980163574\n",
      "epoch: 1, training loss: 13.124162, training acc: 0.900369\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 6478/6478 [13:24<00:00,  8.22it/s, accuracy=0.977, cost=3.08] \n",
      "train minibatch loop:   0%|          | 1/6478 [00:00<13:19,  8.11it/s, accuracy=0.925, cost=11.3]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken: 804.7344207763672\n",
      "epoch: 2, training loss: 9.865208, training acc: 0.924173\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 6478/6478 [13:24<00:00,  8.23it/s, accuracy=0.975, cost=2.23] \n",
      "train minibatch loop:   0%|          | 1/6478 [00:00<12:58,  8.32it/s, accuracy=0.967, cost=5.3]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken: 804.8675396442413\n",
      "epoch: 3, training loss: 7.126463, training acc: 0.944937\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 6478/6478 [13:24<00:00,  8.24it/s, accuracy=0.973, cost=2.54] \n",
      "train minibatch loop:   0%|          | 1/6478 [00:00<12:52,  8.39it/s, accuracy=0.932, cost=7.06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken: 804.5777688026428\n",
      "epoch: 4, training loss: 5.068951, training acc: 0.961455\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 6478/6478 [13:24<00:00,  8.28it/s, accuracy=0.981, cost=1.96] \n",
      "train minibatch loop:   0%|          | 1/6478 [00:00<13:07,  8.23it/s, accuracy=0.962, cost=5.22]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken: 804.8435492515564\n",
      "epoch: 5, training loss: 3.750298, training acc: 0.971970\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 6478/6478 [13:24<00:00,  8.25it/s, accuracy=0.991, cost=1.13] \n",
      "train minibatch loop:   0%|          | 1/6478 [00:00<13:29,  8.00it/s, accuracy=0.986, cost=1.74]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken: 804.6170637607574\n",
      "epoch: 6, training loss: 2.928225, training acc: 0.978502\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 6478/6478 [13:24<00:00,  8.29it/s, accuracy=0.981, cost=1.68]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken: 804.8858578205109\n",
      "epoch: 7, training loss: 2.379846, training acc: 0.982927\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "for e in range(8):\n",
    "    lasttime = time.time()\n",
    "    train_acc, train_loss = 0, 0\n",
    "    pbar = tqdm(\n",
    "        range(0, len(train_X), batch_size), desc = 'train minibatch loop'\n",
    "    )\n",
    "    for i in pbar:\n",
    "        batch_x = train_X[i : min(i + batch_size, train_X.shape[0])]\n",
    "        batch_y = train_Y[i : min(i + batch_size, train_X.shape[0])]\n",
    "        acc, cost, _ = sess.run(\n",
    "            [model.accuracy, model.cost, model.optimizer],\n",
    "            feed_dict = {\n",
    "                model.X: batch_x,\n",
    "                model.Y: batch_y\n",
    "            },\n",
    "        )\n",
    "        assert not np.isnan(cost)\n",
    "        train_loss += cost\n",
    "        train_acc += acc\n",
    "        pbar.set_postfix(cost = cost, accuracy = acc)\n",
    "    \n",
    "    train_loss /= len(train_X) / batch_size\n",
    "    train_acc /= len(train_X) / batch_size\n",
    "\n",
    "    print('time taken:', time.time() - lasttime)\n",
    "    print(\n",
    "        'epoch: %d, training loss: %f, training acc: %f\\n'\n",
    "        % (e, train_loss, train_acc)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bert-pos/model.ckpt'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saver.save(sess, 'bert-pos/model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
