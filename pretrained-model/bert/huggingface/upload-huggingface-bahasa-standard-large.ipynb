{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "out = 'bert-large-bahasa-standard-cased'\n",
    "os.makedirs(out, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel, BertConfig, AutoTokenizer, AutoModelWithLMHead, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.1.0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('bert-large-bahasa-standard-cased/vocab.txt',\n",
       " 'bert-large-bahasa-standard-cased/special_tokens_map.json',\n",
       " 'bert-large-bahasa-standard-cased/added_tokens.json')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = BertTokenizer('BERT.wordpiece', do_lower_case = False)\n",
    "tokenizer.save_pretrained('bert-large-bahasa-standard-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('./bert-large-bahasa-standard-cased', do_lower_case = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building PyTorch model from configuration: BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "Converting TensorFlow checkpoint from /home/husein/bert-standard/bert-large/model.ckpt-700000\n",
      "Loading TF weight bert/embeddings/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/embeddings/LayerNorm/beta/adam_m with shape [1024]\n",
      "Loading TF weight bert/embeddings/LayerNorm/beta/adam_v with shape [1024]\n",
      "Loading TF weight bert/embeddings/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/embeddings/LayerNorm/gamma/adam_m with shape [1024]\n",
      "Loading TF weight bert/embeddings/LayerNorm/gamma/adam_v with shape [1024]\n",
      "Loading TF weight bert/embeddings/position_embeddings with shape [512, 1024]\n",
      "Loading TF weight bert/embeddings/position_embeddings/adam_m with shape [512, 1024]\n",
      "Loading TF weight bert/embeddings/position_embeddings/adam_v with shape [512, 1024]\n",
      "Loading TF weight bert/embeddings/token_type_embeddings with shape [2, 1024]\n",
      "Loading TF weight bert/embeddings/token_type_embeddings/adam_m with shape [2, 1024]\n",
      "Loading TF weight bert/embeddings/token_type_embeddings/adam_v with shape [2, 1024]\n",
      "Loading TF weight bert/embeddings/word_embeddings with shape [32000, 1024]\n",
      "Loading TF weight bert/embeddings/word_embeddings/adam_m with shape [32000, 1024]\n",
      "Loading TF weight bert/embeddings/word_embeddings/adam_v with shape [32000, 1024]\n",
      "Loading TF weight bert/encoder/layer_0/attention/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_0/attention/output/LayerNorm/beta/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_0/attention/output/LayerNorm/beta/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_0/attention/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_0/attention/output/LayerNorm/gamma/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_0/attention/output/LayerNorm/gamma/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_0/attention/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_0/attention/output/dense/bias/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_0/attention/output/dense/bias/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_0/attention/output/dense/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_0/attention/output/dense/kernel/adam_m with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_0/attention/output/dense/kernel/adam_v with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_0/attention/self/key/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_0/attention/self/key/bias/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_0/attention/self/key/bias/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_0/attention/self/key/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_0/attention/self/key/kernel/adam_m with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_0/attention/self/key/kernel/adam_v with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_0/attention/self/query/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_0/attention/self/query/bias/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_0/attention/self/query/bias/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_0/attention/self/query/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_0/attention/self/query/kernel/adam_m with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_0/attention/self/query/kernel/adam_v with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_0/attention/self/value/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_0/attention/self/value/bias/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_0/attention/self/value/bias/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_0/attention/self/value/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_0/attention/self/value/kernel/adam_m with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_0/attention/self/value/kernel/adam_v with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_0/intermediate/dense/bias with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_0/intermediate/dense/bias/adam_m with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_0/intermediate/dense/bias/adam_v with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_0/intermediate/dense/kernel with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_0/intermediate/dense/kernel/adam_m with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_0/intermediate/dense/kernel/adam_v with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_0/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_0/output/LayerNorm/beta/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_0/output/LayerNorm/beta/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_0/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_0/output/LayerNorm/gamma/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_0/output/LayerNorm/gamma/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_0/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_0/output/dense/bias/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_0/output/dense/bias/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_0/output/dense/kernel with shape [4096, 1024]\n",
      "Loading TF weight bert/encoder/layer_0/output/dense/kernel/adam_m with shape [4096, 1024]\n",
      "Loading TF weight bert/encoder/layer_0/output/dense/kernel/adam_v with shape [4096, 1024]\n",
      "Loading TF weight bert/encoder/layer_1/attention/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_1/attention/output/LayerNorm/beta/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_1/attention/output/LayerNorm/beta/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_1/attention/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_1/attention/output/LayerNorm/gamma/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_1/attention/output/LayerNorm/gamma/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_1/attention/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_1/attention/output/dense/bias/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_1/attention/output/dense/bias/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_1/attention/output/dense/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_1/attention/output/dense/kernel/adam_m with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_1/attention/output/dense/kernel/adam_v with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_1/attention/self/key/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_1/attention/self/key/bias/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_1/attention/self/key/bias/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_1/attention/self/key/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_1/attention/self/key/kernel/adam_m with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_1/attention/self/key/kernel/adam_v with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_1/attention/self/query/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_1/attention/self/query/bias/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_1/attention/self/query/bias/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_1/attention/self/query/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_1/attention/self/query/kernel/adam_m with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_1/attention/self/query/kernel/adam_v with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_1/attention/self/value/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_1/attention/self/value/bias/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_1/attention/self/value/bias/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_1/attention/self/value/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_1/attention/self/value/kernel/adam_m with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_1/attention/self/value/kernel/adam_v with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_1/intermediate/dense/bias with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_1/intermediate/dense/bias/adam_m with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_1/intermediate/dense/bias/adam_v with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_1/intermediate/dense/kernel with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_1/intermediate/dense/kernel/adam_m with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_1/intermediate/dense/kernel/adam_v with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_1/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_1/output/LayerNorm/beta/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_1/output/LayerNorm/beta/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_1/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_1/output/LayerNorm/gamma/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_1/output/LayerNorm/gamma/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_1/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_1/output/dense/bias/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_1/output/dense/bias/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_1/output/dense/kernel with shape [4096, 1024]\n",
      "Loading TF weight bert/encoder/layer_1/output/dense/kernel/adam_m with shape [4096, 1024]\n",
      "Loading TF weight bert/encoder/layer_1/output/dense/kernel/adam_v with shape [4096, 1024]\n",
      "Loading TF weight bert/encoder/layer_10/attention/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_10/attention/output/LayerNorm/beta/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_10/attention/output/LayerNorm/beta/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_10/attention/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_10/attention/output/LayerNorm/gamma/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_10/attention/output/LayerNorm/gamma/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_10/attention/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_10/attention/output/dense/bias/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_10/attention/output/dense/bias/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_10/attention/output/dense/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_10/attention/output/dense/kernel/adam_m with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_10/attention/output/dense/kernel/adam_v with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_10/attention/self/key/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_10/attention/self/key/bias/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_10/attention/self/key/bias/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_10/attention/self/key/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_10/attention/self/key/kernel/adam_m with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_10/attention/self/key/kernel/adam_v with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_10/attention/self/query/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_10/attention/self/query/bias/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_10/attention/self/query/bias/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_10/attention/self/query/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_10/attention/self/query/kernel/adam_m with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_10/attention/self/query/kernel/adam_v with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_10/attention/self/value/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_10/attention/self/value/bias/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_10/attention/self/value/bias/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_10/attention/self/value/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_10/attention/self/value/kernel/adam_m with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_10/attention/self/value/kernel/adam_v with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_10/intermediate/dense/bias with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_10/intermediate/dense/bias/adam_m with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_10/intermediate/dense/bias/adam_v with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_10/intermediate/dense/kernel with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_10/intermediate/dense/kernel/adam_m with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_10/intermediate/dense/kernel/adam_v with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_10/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_10/output/LayerNorm/beta/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_10/output/LayerNorm/beta/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_10/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_10/output/LayerNorm/gamma/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_10/output/LayerNorm/gamma/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_10/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_10/output/dense/bias/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_10/output/dense/bias/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_10/output/dense/kernel with shape [4096, 1024]\n",
      "Loading TF weight bert/encoder/layer_10/output/dense/kernel/adam_m with shape [4096, 1024]\n",
      "Loading TF weight bert/encoder/layer_10/output/dense/kernel/adam_v with shape [4096, 1024]\n",
      "Loading TF weight bert/encoder/layer_11/attention/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_11/attention/output/LayerNorm/beta/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_11/attention/output/LayerNorm/beta/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_11/attention/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_11/attention/output/LayerNorm/gamma/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_11/attention/output/LayerNorm/gamma/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_11/attention/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_11/attention/output/dense/bias/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_11/attention/output/dense/bias/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_11/attention/output/dense/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_11/attention/output/dense/kernel/adam_m with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_11/attention/output/dense/kernel/adam_v with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_11/attention/self/key/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_11/attention/self/key/bias/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_11/attention/self/key/bias/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_11/attention/self/key/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_11/attention/self/key/kernel/adam_m with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_11/attention/self/key/kernel/adam_v with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_11/attention/self/query/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_11/attention/self/query/bias/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_11/attention/self/query/bias/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_11/attention/self/query/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_11/attention/self/query/kernel/adam_m with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_11/attention/self/query/kernel/adam_v with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_11/attention/self/value/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_11/attention/self/value/bias/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_11/attention/self/value/bias/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_11/attention/self/value/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_11/attention/self/value/kernel/adam_m with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_11/attention/self/value/kernel/adam_v with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_11/intermediate/dense/bias with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_11/intermediate/dense/bias/adam_m with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_11/intermediate/dense/bias/adam_v with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_11/intermediate/dense/kernel with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_11/intermediate/dense/kernel/adam_m with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_11/intermediate/dense/kernel/adam_v with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_11/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_11/output/LayerNorm/beta/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_11/output/LayerNorm/beta/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_11/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_11/output/LayerNorm/gamma/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_11/output/LayerNorm/gamma/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_11/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_11/output/dense/bias/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_11/output/dense/bias/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_11/output/dense/kernel with shape [4096, 1024]\n",
      "Loading TF weight bert/encoder/layer_11/output/dense/kernel/adam_m with shape [4096, 1024]\n",
      "Loading TF weight bert/encoder/layer_11/output/dense/kernel/adam_v with shape [4096, 1024]\n",
      "Loading TF weight bert/encoder/layer_12/attention/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_12/attention/output/LayerNorm/beta/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_12/attention/output/LayerNorm/beta/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_12/attention/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_12/attention/output/LayerNorm/gamma/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_12/attention/output/LayerNorm/gamma/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_12/attention/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_12/attention/output/dense/bias/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_12/attention/output/dense/bias/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_12/attention/output/dense/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_12/attention/output/dense/kernel/adam_m with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_12/attention/output/dense/kernel/adam_v with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_12/attention/self/key/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_12/attention/self/key/bias/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_12/attention/self/key/bias/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_12/attention/self/key/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_12/attention/self/key/kernel/adam_m with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_12/attention/self/key/kernel/adam_v with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_12/attention/self/query/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_12/attention/self/query/bias/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_12/attention/self/query/bias/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_12/attention/self/query/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_12/attention/self/query/kernel/adam_m with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_12/attention/self/query/kernel/adam_v with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_12/attention/self/value/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_12/attention/self/value/bias/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_12/attention/self/value/bias/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_12/attention/self/value/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_12/attention/self/value/kernel/adam_m with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_12/attention/self/value/kernel/adam_v with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_12/intermediate/dense/bias with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_12/intermediate/dense/bias/adam_m with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_12/intermediate/dense/bias/adam_v with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_12/intermediate/dense/kernel with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_12/intermediate/dense/kernel/adam_m with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_12/intermediate/dense/kernel/adam_v with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_12/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_12/output/LayerNorm/beta/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_12/output/LayerNorm/beta/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_12/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_12/output/LayerNorm/gamma/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_12/output/LayerNorm/gamma/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_12/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_12/output/dense/bias/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_12/output/dense/bias/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_12/output/dense/kernel with shape [4096, 1024]\n",
      "Loading TF weight bert/encoder/layer_12/output/dense/kernel/adam_m with shape [4096, 1024]\n",
      "Loading TF weight bert/encoder/layer_12/output/dense/kernel/adam_v with shape [4096, 1024]\n",
      "Loading TF weight bert/encoder/layer_13/attention/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_13/attention/output/LayerNorm/beta/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_13/attention/output/LayerNorm/beta/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_13/attention/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_13/attention/output/LayerNorm/gamma/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_13/attention/output/LayerNorm/gamma/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_13/attention/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_13/attention/output/dense/bias/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_13/attention/output/dense/bias/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_13/attention/output/dense/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_13/attention/output/dense/kernel/adam_m with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_13/attention/output/dense/kernel/adam_v with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_13/attention/self/key/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_13/attention/self/key/bias/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_13/attention/self/key/bias/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_13/attention/self/key/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_13/attention/self/key/kernel/adam_m with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_13/attention/self/key/kernel/adam_v with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_13/attention/self/query/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_13/attention/self/query/bias/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_13/attention/self/query/bias/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_13/attention/self/query/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_13/attention/self/query/kernel/adam_m with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_13/attention/self/query/kernel/adam_v with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_13/attention/self/value/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_13/attention/self/value/bias/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_13/attention/self/value/bias/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_13/attention/self/value/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_13/attention/self/value/kernel/adam_m with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_13/attention/self/value/kernel/adam_v with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_13/intermediate/dense/bias with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_13/intermediate/dense/bias/adam_m with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_13/intermediate/dense/bias/adam_v with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_13/intermediate/dense/kernel with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_13/intermediate/dense/kernel/adam_m with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_13/intermediate/dense/kernel/adam_v with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_13/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_13/output/LayerNorm/beta/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_13/output/LayerNorm/beta/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_13/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_13/output/LayerNorm/gamma/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_13/output/LayerNorm/gamma/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_13/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_13/output/dense/bias/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_13/output/dense/bias/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_13/output/dense/kernel with shape [4096, 1024]\n",
      "Loading TF weight bert/encoder/layer_13/output/dense/kernel/adam_m with shape [4096, 1024]\n",
      "Loading TF weight bert/encoder/layer_13/output/dense/kernel/adam_v with shape [4096, 1024]\n",
      "Loading TF weight bert/encoder/layer_14/attention/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_14/attention/output/LayerNorm/beta/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_14/attention/output/LayerNorm/beta/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_14/attention/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_14/attention/output/LayerNorm/gamma/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_14/attention/output/LayerNorm/gamma/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_14/attention/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_14/attention/output/dense/bias/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_14/attention/output/dense/bias/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_14/attention/output/dense/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_14/attention/output/dense/kernel/adam_m with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_14/attention/output/dense/kernel/adam_v with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_14/attention/self/key/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_14/attention/self/key/bias/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_14/attention/self/key/bias/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_14/attention/self/key/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_14/attention/self/key/kernel/adam_m with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_14/attention/self/key/kernel/adam_v with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_14/attention/self/query/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_14/attention/self/query/bias/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_14/attention/self/query/bias/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_14/attention/self/query/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_14/attention/self/query/kernel/adam_m with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_14/attention/self/query/kernel/adam_v with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_14/attention/self/value/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_14/attention/self/value/bias/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_14/attention/self/value/bias/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_14/attention/self/value/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_14/attention/self/value/kernel/adam_m with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_14/attention/self/value/kernel/adam_v with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_14/intermediate/dense/bias with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_14/intermediate/dense/bias/adam_m with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_14/intermediate/dense/bias/adam_v with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_14/intermediate/dense/kernel with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_14/intermediate/dense/kernel/adam_m with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_14/intermediate/dense/kernel/adam_v with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_14/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_14/output/LayerNorm/beta/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_14/output/LayerNorm/beta/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_14/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_14/output/LayerNorm/gamma/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_14/output/LayerNorm/gamma/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_14/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_14/output/dense/bias/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_14/output/dense/bias/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_14/output/dense/kernel with shape [4096, 1024]\n",
      "Loading TF weight bert/encoder/layer_14/output/dense/kernel/adam_m with shape [4096, 1024]\n",
      "Loading TF weight bert/encoder/layer_14/output/dense/kernel/adam_v with shape [4096, 1024]\n",
      "Loading TF weight bert/encoder/layer_15/attention/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_15/attention/output/LayerNorm/beta/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_15/attention/output/LayerNorm/beta/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_15/attention/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_15/attention/output/LayerNorm/gamma/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_15/attention/output/LayerNorm/gamma/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_15/attention/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_15/attention/output/dense/bias/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_15/attention/output/dense/bias/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_15/attention/output/dense/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_15/attention/output/dense/kernel/adam_m with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_15/attention/output/dense/kernel/adam_v with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_15/attention/self/key/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_15/attention/self/key/bias/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_15/attention/self/key/bias/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_15/attention/self/key/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_15/attention/self/key/kernel/adam_m with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_15/attention/self/key/kernel/adam_v with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_15/attention/self/query/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_15/attention/self/query/bias/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_15/attention/self/query/bias/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_15/attention/self/query/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_15/attention/self/query/kernel/adam_m with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_15/attention/self/query/kernel/adam_v with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_15/attention/self/value/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_15/attention/self/value/bias/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_15/attention/self/value/bias/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_15/attention/self/value/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_15/attention/self/value/kernel/adam_m with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_15/attention/self/value/kernel/adam_v with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_15/intermediate/dense/bias with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_15/intermediate/dense/bias/adam_m with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_15/intermediate/dense/bias/adam_v with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_15/intermediate/dense/kernel with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_15/intermediate/dense/kernel/adam_m with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_15/intermediate/dense/kernel/adam_v with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_15/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_15/output/LayerNorm/beta/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_15/output/LayerNorm/beta/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_15/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_15/output/LayerNorm/gamma/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_15/output/LayerNorm/gamma/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_15/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_15/output/dense/bias/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_15/output/dense/bias/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_15/output/dense/kernel with shape [4096, 1024]\n",
      "Loading TF weight bert/encoder/layer_15/output/dense/kernel/adam_m with shape [4096, 1024]\n",
      "Loading TF weight bert/encoder/layer_15/output/dense/kernel/adam_v with shape [4096, 1024]\n",
      "Loading TF weight bert/encoder/layer_16/attention/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_16/attention/output/LayerNorm/beta/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_16/attention/output/LayerNorm/beta/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_16/attention/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_16/attention/output/LayerNorm/gamma/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_16/attention/output/LayerNorm/gamma/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_16/attention/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_16/attention/output/dense/bias/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_16/attention/output/dense/bias/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_16/attention/output/dense/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_16/attention/output/dense/kernel/adam_m with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_16/attention/output/dense/kernel/adam_v with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_16/attention/self/key/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_16/attention/self/key/bias/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_16/attention/self/key/bias/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_16/attention/self/key/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_16/attention/self/key/kernel/adam_m with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_16/attention/self/key/kernel/adam_v with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_16/attention/self/query/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_16/attention/self/query/bias/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_16/attention/self/query/bias/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_16/attention/self/query/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_16/attention/self/query/kernel/adam_m with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_16/attention/self/query/kernel/adam_v with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_16/attention/self/value/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_16/attention/self/value/bias/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_16/attention/self/value/bias/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_16/attention/self/value/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_16/attention/self/value/kernel/adam_m with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_16/attention/self/value/kernel/adam_v with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_16/intermediate/dense/bias with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_16/intermediate/dense/bias/adam_m with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_16/intermediate/dense/bias/adam_v with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_16/intermediate/dense/kernel with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_16/intermediate/dense/kernel/adam_m with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_16/intermediate/dense/kernel/adam_v with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_16/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_16/output/LayerNorm/beta/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_16/output/LayerNorm/beta/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_16/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_16/output/LayerNorm/gamma/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_16/output/LayerNorm/gamma/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_16/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_16/output/dense/bias/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_16/output/dense/bias/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_16/output/dense/kernel with shape [4096, 1024]\n",
      "Loading TF weight bert/encoder/layer_16/output/dense/kernel/adam_m with shape [4096, 1024]\n",
      "Loading TF weight bert/encoder/layer_16/output/dense/kernel/adam_v with shape [4096, 1024]\n",
      "Loading TF weight bert/encoder/layer_17/attention/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_17/attention/output/LayerNorm/beta/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_17/attention/output/LayerNorm/beta/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_17/attention/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_17/attention/output/LayerNorm/gamma/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_17/attention/output/LayerNorm/gamma/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_17/attention/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_17/attention/output/dense/bias/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_17/attention/output/dense/bias/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_17/attention/output/dense/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_17/attention/output/dense/kernel/adam_m with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_17/attention/output/dense/kernel/adam_v with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_17/attention/self/key/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_17/attention/self/key/bias/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_17/attention/self/key/bias/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_17/attention/self/key/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_17/attention/self/key/kernel/adam_m with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_17/attention/self/key/kernel/adam_v with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_17/attention/self/query/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_17/attention/self/query/bias/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_17/attention/self/query/bias/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_17/attention/self/query/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_17/attention/self/query/kernel/adam_m with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_17/attention/self/query/kernel/adam_v with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_17/attention/self/value/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_17/attention/self/value/bias/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_17/attention/self/value/bias/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_17/attention/self/value/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_17/attention/self/value/kernel/adam_m with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_17/attention/self/value/kernel/adam_v with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_17/intermediate/dense/bias with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_17/intermediate/dense/bias/adam_m with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_17/intermediate/dense/bias/adam_v with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_17/intermediate/dense/kernel with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_17/intermediate/dense/kernel/adam_m with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_17/intermediate/dense/kernel/adam_v with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_17/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_17/output/LayerNorm/beta/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_17/output/LayerNorm/beta/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_17/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_17/output/LayerNorm/gamma/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_17/output/LayerNorm/gamma/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_17/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_17/output/dense/bias/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_17/output/dense/bias/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_17/output/dense/kernel with shape [4096, 1024]\n",
      "Loading TF weight bert/encoder/layer_17/output/dense/kernel/adam_m with shape [4096, 1024]\n",
      "Loading TF weight bert/encoder/layer_17/output/dense/kernel/adam_v with shape [4096, 1024]\n",
      "Loading TF weight bert/encoder/layer_18/attention/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_18/attention/output/LayerNorm/beta/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_18/attention/output/LayerNorm/beta/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_18/attention/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_18/attention/output/LayerNorm/gamma/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_18/attention/output/LayerNorm/gamma/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_18/attention/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_18/attention/output/dense/bias/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_18/attention/output/dense/bias/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_18/attention/output/dense/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_18/attention/output/dense/kernel/adam_m with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_18/attention/output/dense/kernel/adam_v with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_18/attention/self/key/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_18/attention/self/key/bias/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_18/attention/self/key/bias/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_18/attention/self/key/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_18/attention/self/key/kernel/adam_m with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_18/attention/self/key/kernel/adam_v with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_18/attention/self/query/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_18/attention/self/query/bias/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_18/attention/self/query/bias/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_18/attention/self/query/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_18/attention/self/query/kernel/adam_m with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_18/attention/self/query/kernel/adam_v with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_18/attention/self/value/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_18/attention/self/value/bias/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_18/attention/self/value/bias/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_18/attention/self/value/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_18/attention/self/value/kernel/adam_m with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_18/attention/self/value/kernel/adam_v with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_18/intermediate/dense/bias with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_18/intermediate/dense/bias/adam_m with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_18/intermediate/dense/bias/adam_v with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_18/intermediate/dense/kernel with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_18/intermediate/dense/kernel/adam_m with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_18/intermediate/dense/kernel/adam_v with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_18/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_18/output/LayerNorm/beta/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_18/output/LayerNorm/beta/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_18/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_18/output/LayerNorm/gamma/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_18/output/LayerNorm/gamma/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_18/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_18/output/dense/bias/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_18/output/dense/bias/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_18/output/dense/kernel with shape [4096, 1024]\n",
      "Loading TF weight bert/encoder/layer_18/output/dense/kernel/adam_m with shape [4096, 1024]\n",
      "Loading TF weight bert/encoder/layer_18/output/dense/kernel/adam_v with shape [4096, 1024]\n",
      "Loading TF weight bert/encoder/layer_19/attention/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_19/attention/output/LayerNorm/beta/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_19/attention/output/LayerNorm/beta/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_19/attention/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_19/attention/output/LayerNorm/gamma/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_19/attention/output/LayerNorm/gamma/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_19/attention/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_19/attention/output/dense/bias/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_19/attention/output/dense/bias/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_19/attention/output/dense/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_19/attention/output/dense/kernel/adam_m with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_19/attention/output/dense/kernel/adam_v with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_19/attention/self/key/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_19/attention/self/key/bias/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_19/attention/self/key/bias/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_19/attention/self/key/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_19/attention/self/key/kernel/adam_m with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_19/attention/self/key/kernel/adam_v with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_19/attention/self/query/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_19/attention/self/query/bias/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_19/attention/self/query/bias/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_19/attention/self/query/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_19/attention/self/query/kernel/adam_m with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_19/attention/self/query/kernel/adam_v with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_19/attention/self/value/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_19/attention/self/value/bias/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_19/attention/self/value/bias/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_19/attention/self/value/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_19/attention/self/value/kernel/adam_m with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_19/attention/self/value/kernel/adam_v with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_19/intermediate/dense/bias with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_19/intermediate/dense/bias/adam_m with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_19/intermediate/dense/bias/adam_v with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_19/intermediate/dense/kernel with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_19/intermediate/dense/kernel/adam_m with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_19/intermediate/dense/kernel/adam_v with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_19/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_19/output/LayerNorm/beta/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_19/output/LayerNorm/beta/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_19/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_19/output/LayerNorm/gamma/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_19/output/LayerNorm/gamma/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_19/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_19/output/dense/bias/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_19/output/dense/bias/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_19/output/dense/kernel with shape [4096, 1024]\n",
      "Loading TF weight bert/encoder/layer_19/output/dense/kernel/adam_m with shape [4096, 1024]\n",
      "Loading TF weight bert/encoder/layer_19/output/dense/kernel/adam_v with shape [4096, 1024]\n",
      "Loading TF weight bert/encoder/layer_2/attention/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_2/attention/output/LayerNorm/beta/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_2/attention/output/LayerNorm/beta/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_2/attention/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_2/attention/output/LayerNorm/gamma/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_2/attention/output/LayerNorm/gamma/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_2/attention/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_2/attention/output/dense/bias/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_2/attention/output/dense/bias/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_2/attention/output/dense/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_2/attention/output/dense/kernel/adam_m with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_2/attention/output/dense/kernel/adam_v with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_2/attention/self/key/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_2/attention/self/key/bias/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_2/attention/self/key/bias/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_2/attention/self/key/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_2/attention/self/key/kernel/adam_m with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_2/attention/self/key/kernel/adam_v with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_2/attention/self/query/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_2/attention/self/query/bias/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_2/attention/self/query/bias/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_2/attention/self/query/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_2/attention/self/query/kernel/adam_m with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_2/attention/self/query/kernel/adam_v with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_2/attention/self/value/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_2/attention/self/value/bias/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_2/attention/self/value/bias/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_2/attention/self/value/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_2/attention/self/value/kernel/adam_m with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_2/attention/self/value/kernel/adam_v with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_2/intermediate/dense/bias with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_2/intermediate/dense/bias/adam_m with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_2/intermediate/dense/bias/adam_v with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_2/intermediate/dense/kernel with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_2/intermediate/dense/kernel/adam_m with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_2/intermediate/dense/kernel/adam_v with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_2/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_2/output/LayerNorm/beta/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_2/output/LayerNorm/beta/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_2/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_2/output/LayerNorm/gamma/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_2/output/LayerNorm/gamma/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_2/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_2/output/dense/bias/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_2/output/dense/bias/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_2/output/dense/kernel with shape [4096, 1024]\n",
      "Loading TF weight bert/encoder/layer_2/output/dense/kernel/adam_m with shape [4096, 1024]\n",
      "Loading TF weight bert/encoder/layer_2/output/dense/kernel/adam_v with shape [4096, 1024]\n",
      "Loading TF weight bert/encoder/layer_20/attention/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_20/attention/output/LayerNorm/beta/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_20/attention/output/LayerNorm/beta/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_20/attention/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_20/attention/output/LayerNorm/gamma/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_20/attention/output/LayerNorm/gamma/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_20/attention/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_20/attention/output/dense/bias/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_20/attention/output/dense/bias/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_20/attention/output/dense/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_20/attention/output/dense/kernel/adam_m with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_20/attention/output/dense/kernel/adam_v with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_20/attention/self/key/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_20/attention/self/key/bias/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_20/attention/self/key/bias/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_20/attention/self/key/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_20/attention/self/key/kernel/adam_m with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_20/attention/self/key/kernel/adam_v with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_20/attention/self/query/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_20/attention/self/query/bias/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_20/attention/self/query/bias/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_20/attention/self/query/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_20/attention/self/query/kernel/adam_m with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_20/attention/self/query/kernel/adam_v with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_20/attention/self/value/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_20/attention/self/value/bias/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_20/attention/self/value/bias/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_20/attention/self/value/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_20/attention/self/value/kernel/adam_m with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_20/attention/self/value/kernel/adam_v with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_20/intermediate/dense/bias with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_20/intermediate/dense/bias/adam_m with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_20/intermediate/dense/bias/adam_v with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_20/intermediate/dense/kernel with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_20/intermediate/dense/kernel/adam_m with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_20/intermediate/dense/kernel/adam_v with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_20/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_20/output/LayerNorm/beta/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_20/output/LayerNorm/beta/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_20/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_20/output/LayerNorm/gamma/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_20/output/LayerNorm/gamma/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_20/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_20/output/dense/bias/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_20/output/dense/bias/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_20/output/dense/kernel with shape [4096, 1024]\n",
      "Loading TF weight bert/encoder/layer_20/output/dense/kernel/adam_m with shape [4096, 1024]\n",
      "Loading TF weight bert/encoder/layer_20/output/dense/kernel/adam_v with shape [4096, 1024]\n",
      "Loading TF weight bert/encoder/layer_21/attention/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_21/attention/output/LayerNorm/beta/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_21/attention/output/LayerNorm/beta/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_21/attention/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_21/attention/output/LayerNorm/gamma/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_21/attention/output/LayerNorm/gamma/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_21/attention/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_21/attention/output/dense/bias/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_21/attention/output/dense/bias/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_21/attention/output/dense/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_21/attention/output/dense/kernel/adam_m with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_21/attention/output/dense/kernel/adam_v with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_21/attention/self/key/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_21/attention/self/key/bias/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_21/attention/self/key/bias/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_21/attention/self/key/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_21/attention/self/key/kernel/adam_m with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_21/attention/self/key/kernel/adam_v with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_21/attention/self/query/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_21/attention/self/query/bias/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_21/attention/self/query/bias/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_21/attention/self/query/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_21/attention/self/query/kernel/adam_m with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_21/attention/self/query/kernel/adam_v with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_21/attention/self/value/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_21/attention/self/value/bias/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_21/attention/self/value/bias/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_21/attention/self/value/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_21/attention/self/value/kernel/adam_m with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_21/attention/self/value/kernel/adam_v with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_21/intermediate/dense/bias with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_21/intermediate/dense/bias/adam_m with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_21/intermediate/dense/bias/adam_v with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_21/intermediate/dense/kernel with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_21/intermediate/dense/kernel/adam_m with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_21/intermediate/dense/kernel/adam_v with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_21/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_21/output/LayerNorm/beta/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_21/output/LayerNorm/beta/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_21/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_21/output/LayerNorm/gamma/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_21/output/LayerNorm/gamma/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_21/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_21/output/dense/bias/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_21/output/dense/bias/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_21/output/dense/kernel with shape [4096, 1024]\n",
      "Loading TF weight bert/encoder/layer_21/output/dense/kernel/adam_m with shape [4096, 1024]\n",
      "Loading TF weight bert/encoder/layer_21/output/dense/kernel/adam_v with shape [4096, 1024]\n",
      "Loading TF weight bert/encoder/layer_22/attention/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_22/attention/output/LayerNorm/beta/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_22/attention/output/LayerNorm/beta/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_22/attention/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_22/attention/output/LayerNorm/gamma/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_22/attention/output/LayerNorm/gamma/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_22/attention/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_22/attention/output/dense/bias/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_22/attention/output/dense/bias/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_22/attention/output/dense/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_22/attention/output/dense/kernel/adam_m with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_22/attention/output/dense/kernel/adam_v with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_22/attention/self/key/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_22/attention/self/key/bias/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_22/attention/self/key/bias/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_22/attention/self/key/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_22/attention/self/key/kernel/adam_m with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_22/attention/self/key/kernel/adam_v with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_22/attention/self/query/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_22/attention/self/query/bias/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_22/attention/self/query/bias/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_22/attention/self/query/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_22/attention/self/query/kernel/adam_m with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_22/attention/self/query/kernel/adam_v with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_22/attention/self/value/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_22/attention/self/value/bias/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_22/attention/self/value/bias/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_22/attention/self/value/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_22/attention/self/value/kernel/adam_m with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_22/attention/self/value/kernel/adam_v with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_22/intermediate/dense/bias with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_22/intermediate/dense/bias/adam_m with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_22/intermediate/dense/bias/adam_v with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_22/intermediate/dense/kernel with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_22/intermediate/dense/kernel/adam_m with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_22/intermediate/dense/kernel/adam_v with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_22/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_22/output/LayerNorm/beta/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_22/output/LayerNorm/beta/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_22/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_22/output/LayerNorm/gamma/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_22/output/LayerNorm/gamma/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_22/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_22/output/dense/bias/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_22/output/dense/bias/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_22/output/dense/kernel with shape [4096, 1024]\n",
      "Loading TF weight bert/encoder/layer_22/output/dense/kernel/adam_m with shape [4096, 1024]\n",
      "Loading TF weight bert/encoder/layer_22/output/dense/kernel/adam_v with shape [4096, 1024]\n",
      "Loading TF weight bert/encoder/layer_23/attention/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_23/attention/output/LayerNorm/beta/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_23/attention/output/LayerNorm/beta/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_23/attention/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_23/attention/output/LayerNorm/gamma/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_23/attention/output/LayerNorm/gamma/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_23/attention/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_23/attention/output/dense/bias/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_23/attention/output/dense/bias/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_23/attention/output/dense/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_23/attention/output/dense/kernel/adam_m with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_23/attention/output/dense/kernel/adam_v with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_23/attention/self/key/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_23/attention/self/key/bias/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_23/attention/self/key/bias/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_23/attention/self/key/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_23/attention/self/key/kernel/adam_m with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_23/attention/self/key/kernel/adam_v with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_23/attention/self/query/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_23/attention/self/query/bias/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_23/attention/self/query/bias/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_23/attention/self/query/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_23/attention/self/query/kernel/adam_m with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_23/attention/self/query/kernel/adam_v with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_23/attention/self/value/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_23/attention/self/value/bias/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_23/attention/self/value/bias/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_23/attention/self/value/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_23/attention/self/value/kernel/adam_m with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_23/attention/self/value/kernel/adam_v with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_23/intermediate/dense/bias with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_23/intermediate/dense/bias/adam_m with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_23/intermediate/dense/bias/adam_v with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_23/intermediate/dense/kernel with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_23/intermediate/dense/kernel/adam_m with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_23/intermediate/dense/kernel/adam_v with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_23/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_23/output/LayerNorm/beta/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_23/output/LayerNorm/beta/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_23/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_23/output/LayerNorm/gamma/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_23/output/LayerNorm/gamma/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_23/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_23/output/dense/bias/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_23/output/dense/bias/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_23/output/dense/kernel with shape [4096, 1024]\n",
      "Loading TF weight bert/encoder/layer_23/output/dense/kernel/adam_m with shape [4096, 1024]\n",
      "Loading TF weight bert/encoder/layer_23/output/dense/kernel/adam_v with shape [4096, 1024]\n",
      "Loading TF weight bert/encoder/layer_3/attention/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_3/attention/output/LayerNorm/beta/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_3/attention/output/LayerNorm/beta/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_3/attention/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_3/attention/output/LayerNorm/gamma/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_3/attention/output/LayerNorm/gamma/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_3/attention/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_3/attention/output/dense/bias/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_3/attention/output/dense/bias/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_3/attention/output/dense/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_3/attention/output/dense/kernel/adam_m with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_3/attention/output/dense/kernel/adam_v with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_3/attention/self/key/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_3/attention/self/key/bias/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_3/attention/self/key/bias/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_3/attention/self/key/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_3/attention/self/key/kernel/adam_m with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_3/attention/self/key/kernel/adam_v with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_3/attention/self/query/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_3/attention/self/query/bias/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_3/attention/self/query/bias/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_3/attention/self/query/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_3/attention/self/query/kernel/adam_m with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_3/attention/self/query/kernel/adam_v with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_3/attention/self/value/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_3/attention/self/value/bias/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_3/attention/self/value/bias/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_3/attention/self/value/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_3/attention/self/value/kernel/adam_m with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_3/attention/self/value/kernel/adam_v with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_3/intermediate/dense/bias with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_3/intermediate/dense/bias/adam_m with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_3/intermediate/dense/bias/adam_v with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_3/intermediate/dense/kernel with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_3/intermediate/dense/kernel/adam_m with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_3/intermediate/dense/kernel/adam_v with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_3/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_3/output/LayerNorm/beta/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_3/output/LayerNorm/beta/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_3/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_3/output/LayerNorm/gamma/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_3/output/LayerNorm/gamma/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_3/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_3/output/dense/bias/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_3/output/dense/bias/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_3/output/dense/kernel with shape [4096, 1024]\n",
      "Loading TF weight bert/encoder/layer_3/output/dense/kernel/adam_m with shape [4096, 1024]\n",
      "Loading TF weight bert/encoder/layer_3/output/dense/kernel/adam_v with shape [4096, 1024]\n",
      "Loading TF weight bert/encoder/layer_4/attention/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_4/attention/output/LayerNorm/beta/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_4/attention/output/LayerNorm/beta/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_4/attention/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_4/attention/output/LayerNorm/gamma/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_4/attention/output/LayerNorm/gamma/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_4/attention/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_4/attention/output/dense/bias/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_4/attention/output/dense/bias/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_4/attention/output/dense/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_4/attention/output/dense/kernel/adam_m with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_4/attention/output/dense/kernel/adam_v with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_4/attention/self/key/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_4/attention/self/key/bias/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_4/attention/self/key/bias/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_4/attention/self/key/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_4/attention/self/key/kernel/adam_m with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_4/attention/self/key/kernel/adam_v with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_4/attention/self/query/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_4/attention/self/query/bias/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_4/attention/self/query/bias/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_4/attention/self/query/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_4/attention/self/query/kernel/adam_m with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_4/attention/self/query/kernel/adam_v with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_4/attention/self/value/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_4/attention/self/value/bias/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_4/attention/self/value/bias/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_4/attention/self/value/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_4/attention/self/value/kernel/adam_m with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_4/attention/self/value/kernel/adam_v with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_4/intermediate/dense/bias with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_4/intermediate/dense/bias/adam_m with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_4/intermediate/dense/bias/adam_v with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_4/intermediate/dense/kernel with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_4/intermediate/dense/kernel/adam_m with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_4/intermediate/dense/kernel/adam_v with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_4/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_4/output/LayerNorm/beta/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_4/output/LayerNorm/beta/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_4/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_4/output/LayerNorm/gamma/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_4/output/LayerNorm/gamma/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_4/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_4/output/dense/bias/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_4/output/dense/bias/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_4/output/dense/kernel with shape [4096, 1024]\n",
      "Loading TF weight bert/encoder/layer_4/output/dense/kernel/adam_m with shape [4096, 1024]\n",
      "Loading TF weight bert/encoder/layer_4/output/dense/kernel/adam_v with shape [4096, 1024]\n",
      "Loading TF weight bert/encoder/layer_5/attention/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_5/attention/output/LayerNorm/beta/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_5/attention/output/LayerNorm/beta/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_5/attention/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_5/attention/output/LayerNorm/gamma/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_5/attention/output/LayerNorm/gamma/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_5/attention/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_5/attention/output/dense/bias/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_5/attention/output/dense/bias/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_5/attention/output/dense/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_5/attention/output/dense/kernel/adam_m with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_5/attention/output/dense/kernel/adam_v with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_5/attention/self/key/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_5/attention/self/key/bias/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_5/attention/self/key/bias/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_5/attention/self/key/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_5/attention/self/key/kernel/adam_m with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_5/attention/self/key/kernel/adam_v with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_5/attention/self/query/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_5/attention/self/query/bias/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_5/attention/self/query/bias/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_5/attention/self/query/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_5/attention/self/query/kernel/adam_m with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_5/attention/self/query/kernel/adam_v with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_5/attention/self/value/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_5/attention/self/value/bias/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_5/attention/self/value/bias/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_5/attention/self/value/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_5/attention/self/value/kernel/adam_m with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_5/attention/self/value/kernel/adam_v with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_5/intermediate/dense/bias with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_5/intermediate/dense/bias/adam_m with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_5/intermediate/dense/bias/adam_v with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_5/intermediate/dense/kernel with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_5/intermediate/dense/kernel/adam_m with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_5/intermediate/dense/kernel/adam_v with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_5/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_5/output/LayerNorm/beta/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_5/output/LayerNorm/beta/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_5/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_5/output/LayerNorm/gamma/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_5/output/LayerNorm/gamma/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_5/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_5/output/dense/bias/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_5/output/dense/bias/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_5/output/dense/kernel with shape [4096, 1024]\n",
      "Loading TF weight bert/encoder/layer_5/output/dense/kernel/adam_m with shape [4096, 1024]\n",
      "Loading TF weight bert/encoder/layer_5/output/dense/kernel/adam_v with shape [4096, 1024]\n",
      "Loading TF weight bert/encoder/layer_6/attention/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_6/attention/output/LayerNorm/beta/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_6/attention/output/LayerNorm/beta/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_6/attention/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_6/attention/output/LayerNorm/gamma/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_6/attention/output/LayerNorm/gamma/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_6/attention/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_6/attention/output/dense/bias/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_6/attention/output/dense/bias/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_6/attention/output/dense/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_6/attention/output/dense/kernel/adam_m with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_6/attention/output/dense/kernel/adam_v with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_6/attention/self/key/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_6/attention/self/key/bias/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_6/attention/self/key/bias/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_6/attention/self/key/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_6/attention/self/key/kernel/adam_m with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_6/attention/self/key/kernel/adam_v with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_6/attention/self/query/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_6/attention/self/query/bias/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_6/attention/self/query/bias/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_6/attention/self/query/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_6/attention/self/query/kernel/adam_m with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_6/attention/self/query/kernel/adam_v with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_6/attention/self/value/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_6/attention/self/value/bias/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_6/attention/self/value/bias/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_6/attention/self/value/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_6/attention/self/value/kernel/adam_m with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_6/attention/self/value/kernel/adam_v with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_6/intermediate/dense/bias with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_6/intermediate/dense/bias/adam_m with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_6/intermediate/dense/bias/adam_v with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_6/intermediate/dense/kernel with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_6/intermediate/dense/kernel/adam_m with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_6/intermediate/dense/kernel/adam_v with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_6/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_6/output/LayerNorm/beta/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_6/output/LayerNorm/beta/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_6/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_6/output/LayerNorm/gamma/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_6/output/LayerNorm/gamma/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_6/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_6/output/dense/bias/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_6/output/dense/bias/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_6/output/dense/kernel with shape [4096, 1024]\n",
      "Loading TF weight bert/encoder/layer_6/output/dense/kernel/adam_m with shape [4096, 1024]\n",
      "Loading TF weight bert/encoder/layer_6/output/dense/kernel/adam_v with shape [4096, 1024]\n",
      "Loading TF weight bert/encoder/layer_7/attention/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_7/attention/output/LayerNorm/beta/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_7/attention/output/LayerNorm/beta/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_7/attention/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_7/attention/output/LayerNorm/gamma/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_7/attention/output/LayerNorm/gamma/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_7/attention/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_7/attention/output/dense/bias/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_7/attention/output/dense/bias/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_7/attention/output/dense/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_7/attention/output/dense/kernel/adam_m with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_7/attention/output/dense/kernel/adam_v with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_7/attention/self/key/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_7/attention/self/key/bias/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_7/attention/self/key/bias/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_7/attention/self/key/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_7/attention/self/key/kernel/adam_m with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_7/attention/self/key/kernel/adam_v with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_7/attention/self/query/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_7/attention/self/query/bias/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_7/attention/self/query/bias/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_7/attention/self/query/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_7/attention/self/query/kernel/adam_m with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_7/attention/self/query/kernel/adam_v with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_7/attention/self/value/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_7/attention/self/value/bias/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_7/attention/self/value/bias/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_7/attention/self/value/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_7/attention/self/value/kernel/adam_m with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_7/attention/self/value/kernel/adam_v with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_7/intermediate/dense/bias with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_7/intermediate/dense/bias/adam_m with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_7/intermediate/dense/bias/adam_v with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_7/intermediate/dense/kernel with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_7/intermediate/dense/kernel/adam_m with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_7/intermediate/dense/kernel/adam_v with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_7/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_7/output/LayerNorm/beta/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_7/output/LayerNorm/beta/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_7/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_7/output/LayerNorm/gamma/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_7/output/LayerNorm/gamma/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_7/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_7/output/dense/bias/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_7/output/dense/bias/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_7/output/dense/kernel with shape [4096, 1024]\n",
      "Loading TF weight bert/encoder/layer_7/output/dense/kernel/adam_m with shape [4096, 1024]\n",
      "Loading TF weight bert/encoder/layer_7/output/dense/kernel/adam_v with shape [4096, 1024]\n",
      "Loading TF weight bert/encoder/layer_8/attention/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_8/attention/output/LayerNorm/beta/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_8/attention/output/LayerNorm/beta/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_8/attention/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_8/attention/output/LayerNorm/gamma/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_8/attention/output/LayerNorm/gamma/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_8/attention/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_8/attention/output/dense/bias/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_8/attention/output/dense/bias/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_8/attention/output/dense/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_8/attention/output/dense/kernel/adam_m with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_8/attention/output/dense/kernel/adam_v with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_8/attention/self/key/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_8/attention/self/key/bias/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_8/attention/self/key/bias/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_8/attention/self/key/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_8/attention/self/key/kernel/adam_m with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_8/attention/self/key/kernel/adam_v with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_8/attention/self/query/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_8/attention/self/query/bias/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_8/attention/self/query/bias/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_8/attention/self/query/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_8/attention/self/query/kernel/adam_m with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_8/attention/self/query/kernel/adam_v with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_8/attention/self/value/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_8/attention/self/value/bias/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_8/attention/self/value/bias/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_8/attention/self/value/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_8/attention/self/value/kernel/adam_m with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_8/attention/self/value/kernel/adam_v with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_8/intermediate/dense/bias with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_8/intermediate/dense/bias/adam_m with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_8/intermediate/dense/bias/adam_v with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_8/intermediate/dense/kernel with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_8/intermediate/dense/kernel/adam_m with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_8/intermediate/dense/kernel/adam_v with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_8/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_8/output/LayerNorm/beta/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_8/output/LayerNorm/beta/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_8/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_8/output/LayerNorm/gamma/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_8/output/LayerNorm/gamma/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_8/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_8/output/dense/bias/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_8/output/dense/bias/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_8/output/dense/kernel with shape [4096, 1024]\n",
      "Loading TF weight bert/encoder/layer_8/output/dense/kernel/adam_m with shape [4096, 1024]\n",
      "Loading TF weight bert/encoder/layer_8/output/dense/kernel/adam_v with shape [4096, 1024]\n",
      "Loading TF weight bert/encoder/layer_9/attention/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_9/attention/output/LayerNorm/beta/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_9/attention/output/LayerNorm/beta/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_9/attention/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_9/attention/output/LayerNorm/gamma/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_9/attention/output/LayerNorm/gamma/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_9/attention/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_9/attention/output/dense/bias/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_9/attention/output/dense/bias/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_9/attention/output/dense/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_9/attention/output/dense/kernel/adam_m with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_9/attention/output/dense/kernel/adam_v with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_9/attention/self/key/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_9/attention/self/key/bias/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_9/attention/self/key/bias/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_9/attention/self/key/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_9/attention/self/key/kernel/adam_m with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_9/attention/self/key/kernel/adam_v with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_9/attention/self/query/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_9/attention/self/query/bias/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_9/attention/self/query/bias/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_9/attention/self/query/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_9/attention/self/query/kernel/adam_m with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_9/attention/self/query/kernel/adam_v with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_9/attention/self/value/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_9/attention/self/value/bias/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_9/attention/self/value/bias/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_9/attention/self/value/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_9/attention/self/value/kernel/adam_m with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_9/attention/self/value/kernel/adam_v with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_9/intermediate/dense/bias with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_9/intermediate/dense/bias/adam_m with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_9/intermediate/dense/bias/adam_v with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_9/intermediate/dense/kernel with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_9/intermediate/dense/kernel/adam_m with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_9/intermediate/dense/kernel/adam_v with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_9/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_9/output/LayerNorm/beta/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_9/output/LayerNorm/beta/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_9/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_9/output/LayerNorm/gamma/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_9/output/LayerNorm/gamma/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_9/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_9/output/dense/bias/adam_m with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_9/output/dense/bias/adam_v with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_9/output/dense/kernel with shape [4096, 1024]\n",
      "Loading TF weight bert/encoder/layer_9/output/dense/kernel/adam_m with shape [4096, 1024]\n",
      "Loading TF weight bert/encoder/layer_9/output/dense/kernel/adam_v with shape [4096, 1024]\n",
      "Loading TF weight bert/pooler/dense/bias with shape [1024]\n",
      "Loading TF weight bert/pooler/dense/bias/adam_m with shape [1024]\n",
      "Loading TF weight bert/pooler/dense/bias/adam_v with shape [1024]\n",
      "Loading TF weight bert/pooler/dense/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/pooler/dense/kernel/adam_m with shape [1024, 1024]\n",
      "Loading TF weight bert/pooler/dense/kernel/adam_v with shape [1024, 1024]\n",
      "Loading TF weight cls/predictions/output_bias with shape [32000]\n",
      "Loading TF weight cls/predictions/output_bias/adam_m with shape [32000]\n",
      "Loading TF weight cls/predictions/output_bias/adam_v with shape [32000]\n",
      "Loading TF weight cls/predictions/transform/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight cls/predictions/transform/LayerNorm/beta/adam_m with shape [1024]\n",
      "Loading TF weight cls/predictions/transform/LayerNorm/beta/adam_v with shape [1024]\n",
      "Loading TF weight cls/predictions/transform/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight cls/predictions/transform/LayerNorm/gamma/adam_m with shape [1024]\n",
      "Loading TF weight cls/predictions/transform/LayerNorm/gamma/adam_v with shape [1024]\n",
      "Loading TF weight cls/predictions/transform/dense/bias with shape [1024]\n",
      "Loading TF weight cls/predictions/transform/dense/bias/adam_m with shape [1024]\n",
      "Loading TF weight cls/predictions/transform/dense/bias/adam_v with shape [1024]\n",
      "Loading TF weight cls/predictions/transform/dense/kernel with shape [1024, 1024]\n",
      "Loading TF weight cls/predictions/transform/dense/kernel/adam_m with shape [1024, 1024]\n",
      "Loading TF weight cls/predictions/transform/dense/kernel/adam_v with shape [1024, 1024]\n",
      "Loading TF weight cls/seq_relationship/output_bias with shape [2]\n",
      "Loading TF weight cls/seq_relationship/output_bias/adam_m with shape [2]\n",
      "Loading TF weight cls/seq_relationship/output_bias/adam_v with shape [2]\n",
      "Loading TF weight cls/seq_relationship/output_weights with shape [2, 1024]\n",
      "Loading TF weight cls/seq_relationship/output_weights/adam_m with shape [2, 1024]\n",
      "Loading TF weight cls/seq_relationship/output_weights/adam_v with shape [2, 1024]\n",
      "Loading TF weight global_step with shape []\n",
      "Initialize PyTorch weight ['bert', 'embeddings', 'LayerNorm', 'beta']\n",
      "Skipping bert/embeddings/LayerNorm/beta/adam_m\n",
      "Skipping bert/embeddings/LayerNorm/beta/adam_v\n",
      "Initialize PyTorch weight ['bert', 'embeddings', 'LayerNorm', 'gamma']\n",
      "Skipping bert/embeddings/LayerNorm/gamma/adam_m\n",
      "Skipping bert/embeddings/LayerNorm/gamma/adam_v\n",
      "Initialize PyTorch weight ['bert', 'embeddings', 'position_embeddings']\n",
      "Skipping bert/embeddings/position_embeddings/adam_m\n",
      "Skipping bert/embeddings/position_embeddings/adam_v\n",
      "Initialize PyTorch weight ['bert', 'embeddings', 'token_type_embeddings']\n",
      "Skipping bert/embeddings/token_type_embeddings/adam_m\n",
      "Skipping bert/embeddings/token_type_embeddings/adam_v\n",
      "Initialize PyTorch weight ['bert', 'embeddings', 'word_embeddings']\n",
      "Skipping bert/embeddings/word_embeddings/adam_m\n",
      "Skipping bert/embeddings/word_embeddings/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "Skipping bert/encoder/layer_0/attention/output/LayerNorm/beta/adam_m\n",
      "Skipping bert/encoder/layer_0/attention/output/LayerNorm/beta/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "Skipping bert/encoder/layer_0/attention/output/LayerNorm/gamma/adam_m\n",
      "Skipping bert/encoder/layer_0/attention/output/LayerNorm/gamma/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'dense', 'bias']\n",
      "Skipping bert/encoder/layer_0/attention/output/dense/bias/adam_m\n",
      "Skipping bert/encoder/layer_0/attention/output/dense/bias/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'dense', 'kernel']\n",
      "Skipping bert/encoder/layer_0/attention/output/dense/kernel/adam_m\n",
      "Skipping bert/encoder/layer_0/attention/output/dense/kernel/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'key', 'bias']\n",
      "Skipping bert/encoder/layer_0/attention/self/key/bias/adam_m\n",
      "Skipping bert/encoder/layer_0/attention/self/key/bias/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'key', 'kernel']\n",
      "Skipping bert/encoder/layer_0/attention/self/key/kernel/adam_m\n",
      "Skipping bert/encoder/layer_0/attention/self/key/kernel/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'query', 'bias']\n",
      "Skipping bert/encoder/layer_0/attention/self/query/bias/adam_m\n",
      "Skipping bert/encoder/layer_0/attention/self/query/bias/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'query', 'kernel']\n",
      "Skipping bert/encoder/layer_0/attention/self/query/kernel/adam_m\n",
      "Skipping bert/encoder/layer_0/attention/self/query/kernel/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'value', 'bias']\n",
      "Skipping bert/encoder/layer_0/attention/self/value/bias/adam_m\n",
      "Skipping bert/encoder/layer_0/attention/self/value/bias/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'value', 'kernel']\n",
      "Skipping bert/encoder/layer_0/attention/self/value/kernel/adam_m\n",
      "Skipping bert/encoder/layer_0/attention/self/value/kernel/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'intermediate', 'dense', 'bias']\n",
      "Skipping bert/encoder/layer_0/intermediate/dense/bias/adam_m\n",
      "Skipping bert/encoder/layer_0/intermediate/dense/bias/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'intermediate', 'dense', 'kernel']\n",
      "Skipping bert/encoder/layer_0/intermediate/dense/kernel/adam_m\n",
      "Skipping bert/encoder/layer_0/intermediate/dense/kernel/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'LayerNorm', 'beta']\n",
      "Skipping bert/encoder/layer_0/output/LayerNorm/beta/adam_m\n",
      "Skipping bert/encoder/layer_0/output/LayerNorm/beta/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'LayerNorm', 'gamma']\n",
      "Skipping bert/encoder/layer_0/output/LayerNorm/gamma/adam_m\n",
      "Skipping bert/encoder/layer_0/output/LayerNorm/gamma/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'dense', 'bias']\n",
      "Skipping bert/encoder/layer_0/output/dense/bias/adam_m\n",
      "Skipping bert/encoder/layer_0/output/dense/bias/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'dense', 'kernel']\n",
      "Skipping bert/encoder/layer_0/output/dense/kernel/adam_m\n",
      "Skipping bert/encoder/layer_0/output/dense/kernel/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "Skipping bert/encoder/layer_1/attention/output/LayerNorm/beta/adam_m\n",
      "Skipping bert/encoder/layer_1/attention/output/LayerNorm/beta/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "Skipping bert/encoder/layer_1/attention/output/LayerNorm/gamma/adam_m\n",
      "Skipping bert/encoder/layer_1/attention/output/LayerNorm/gamma/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'dense', 'bias']\n",
      "Skipping bert/encoder/layer_1/attention/output/dense/bias/adam_m\n",
      "Skipping bert/encoder/layer_1/attention/output/dense/bias/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'dense', 'kernel']\n",
      "Skipping bert/encoder/layer_1/attention/output/dense/kernel/adam_m\n",
      "Skipping bert/encoder/layer_1/attention/output/dense/kernel/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'key', 'bias']\n",
      "Skipping bert/encoder/layer_1/attention/self/key/bias/adam_m\n",
      "Skipping bert/encoder/layer_1/attention/self/key/bias/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'key', 'kernel']\n",
      "Skipping bert/encoder/layer_1/attention/self/key/kernel/adam_m\n",
      "Skipping bert/encoder/layer_1/attention/self/key/kernel/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'query', 'bias']\n",
      "Skipping bert/encoder/layer_1/attention/self/query/bias/adam_m\n",
      "Skipping bert/encoder/layer_1/attention/self/query/bias/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'query', 'kernel']\n",
      "Skipping bert/encoder/layer_1/attention/self/query/kernel/adam_m\n",
      "Skipping bert/encoder/layer_1/attention/self/query/kernel/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'value', 'bias']\n",
      "Skipping bert/encoder/layer_1/attention/self/value/bias/adam_m\n",
      "Skipping bert/encoder/layer_1/attention/self/value/bias/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'value', 'kernel']\n",
      "Skipping bert/encoder/layer_1/attention/self/value/kernel/adam_m\n",
      "Skipping bert/encoder/layer_1/attention/self/value/kernel/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'intermediate', 'dense', 'bias']\n",
      "Skipping bert/encoder/layer_1/intermediate/dense/bias/adam_m\n",
      "Skipping bert/encoder/layer_1/intermediate/dense/bias/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'intermediate', 'dense', 'kernel']\n",
      "Skipping bert/encoder/layer_1/intermediate/dense/kernel/adam_m\n",
      "Skipping bert/encoder/layer_1/intermediate/dense/kernel/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'LayerNorm', 'beta']\n",
      "Skipping bert/encoder/layer_1/output/LayerNorm/beta/adam_m\n",
      "Skipping bert/encoder/layer_1/output/LayerNorm/beta/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'LayerNorm', 'gamma']\n",
      "Skipping bert/encoder/layer_1/output/LayerNorm/gamma/adam_m\n",
      "Skipping bert/encoder/layer_1/output/LayerNorm/gamma/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'dense', 'bias']\n",
      "Skipping bert/encoder/layer_1/output/dense/bias/adam_m\n",
      "Skipping bert/encoder/layer_1/output/dense/bias/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'dense', 'kernel']\n",
      "Skipping bert/encoder/layer_1/output/dense/kernel/adam_m\n",
      "Skipping bert/encoder/layer_1/output/dense/kernel/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "Skipping bert/encoder/layer_10/attention/output/LayerNorm/beta/adam_m\n",
      "Skipping bert/encoder/layer_10/attention/output/LayerNorm/beta/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "Skipping bert/encoder/layer_10/attention/output/LayerNorm/gamma/adam_m\n",
      "Skipping bert/encoder/layer_10/attention/output/LayerNorm/gamma/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'dense', 'bias']\n",
      "Skipping bert/encoder/layer_10/attention/output/dense/bias/adam_m\n",
      "Skipping bert/encoder/layer_10/attention/output/dense/bias/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'dense', 'kernel']\n",
      "Skipping bert/encoder/layer_10/attention/output/dense/kernel/adam_m\n",
      "Skipping bert/encoder/layer_10/attention/output/dense/kernel/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'key', 'bias']\n",
      "Skipping bert/encoder/layer_10/attention/self/key/bias/adam_m\n",
      "Skipping bert/encoder/layer_10/attention/self/key/bias/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'key', 'kernel']\n",
      "Skipping bert/encoder/layer_10/attention/self/key/kernel/adam_m\n",
      "Skipping bert/encoder/layer_10/attention/self/key/kernel/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'query', 'bias']\n",
      "Skipping bert/encoder/layer_10/attention/self/query/bias/adam_m\n",
      "Skipping bert/encoder/layer_10/attention/self/query/bias/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'query', 'kernel']\n",
      "Skipping bert/encoder/layer_10/attention/self/query/kernel/adam_m\n",
      "Skipping bert/encoder/layer_10/attention/self/query/kernel/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'value', 'bias']\n",
      "Skipping bert/encoder/layer_10/attention/self/value/bias/adam_m\n",
      "Skipping bert/encoder/layer_10/attention/self/value/bias/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'value', 'kernel']\n",
      "Skipping bert/encoder/layer_10/attention/self/value/kernel/adam_m\n",
      "Skipping bert/encoder/layer_10/attention/self/value/kernel/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'intermediate', 'dense', 'bias']\n",
      "Skipping bert/encoder/layer_10/intermediate/dense/bias/adam_m\n",
      "Skipping bert/encoder/layer_10/intermediate/dense/bias/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'intermediate', 'dense', 'kernel']\n",
      "Skipping bert/encoder/layer_10/intermediate/dense/kernel/adam_m\n",
      "Skipping bert/encoder/layer_10/intermediate/dense/kernel/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'LayerNorm', 'beta']\n",
      "Skipping bert/encoder/layer_10/output/LayerNorm/beta/adam_m\n",
      "Skipping bert/encoder/layer_10/output/LayerNorm/beta/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'LayerNorm', 'gamma']\n",
      "Skipping bert/encoder/layer_10/output/LayerNorm/gamma/adam_m\n",
      "Skipping bert/encoder/layer_10/output/LayerNorm/gamma/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'dense', 'bias']\n",
      "Skipping bert/encoder/layer_10/output/dense/bias/adam_m\n",
      "Skipping bert/encoder/layer_10/output/dense/bias/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'dense', 'kernel']\n",
      "Skipping bert/encoder/layer_10/output/dense/kernel/adam_m\n",
      "Skipping bert/encoder/layer_10/output/dense/kernel/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "Skipping bert/encoder/layer_11/attention/output/LayerNorm/beta/adam_m\n",
      "Skipping bert/encoder/layer_11/attention/output/LayerNorm/beta/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "Skipping bert/encoder/layer_11/attention/output/LayerNorm/gamma/adam_m\n",
      "Skipping bert/encoder/layer_11/attention/output/LayerNorm/gamma/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'dense', 'bias']\n",
      "Skipping bert/encoder/layer_11/attention/output/dense/bias/adam_m\n",
      "Skipping bert/encoder/layer_11/attention/output/dense/bias/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'dense', 'kernel']\n",
      "Skipping bert/encoder/layer_11/attention/output/dense/kernel/adam_m\n",
      "Skipping bert/encoder/layer_11/attention/output/dense/kernel/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'key', 'bias']\n",
      "Skipping bert/encoder/layer_11/attention/self/key/bias/adam_m\n",
      "Skipping bert/encoder/layer_11/attention/self/key/bias/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'key', 'kernel']\n",
      "Skipping bert/encoder/layer_11/attention/self/key/kernel/adam_m\n",
      "Skipping bert/encoder/layer_11/attention/self/key/kernel/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'query', 'bias']\n",
      "Skipping bert/encoder/layer_11/attention/self/query/bias/adam_m\n",
      "Skipping bert/encoder/layer_11/attention/self/query/bias/adam_v\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'query', 'kernel']\n",
      "Skipping bert/encoder/layer_11/attention/self/query/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_11/attention/self/query/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'value', 'bias']\r\n",
      "Skipping bert/encoder/layer_11/attention/self/value/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_11/attention/self/value/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'value', 'kernel']\r\n",
      "Skipping bert/encoder/layer_11/attention/self/value/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_11/attention/self/value/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'intermediate', 'dense', 'bias']\r\n",
      "Skipping bert/encoder/layer_11/intermediate/dense/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_11/intermediate/dense/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'intermediate', 'dense', 'kernel']\r\n",
      "Skipping bert/encoder/layer_11/intermediate/dense/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_11/intermediate/dense/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'LayerNorm', 'beta']\r\n",
      "Skipping bert/encoder/layer_11/output/LayerNorm/beta/adam_m\r\n",
      "Skipping bert/encoder/layer_11/output/LayerNorm/beta/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'LayerNorm', 'gamma']\r\n",
      "Skipping bert/encoder/layer_11/output/LayerNorm/gamma/adam_m\r\n",
      "Skipping bert/encoder/layer_11/output/LayerNorm/gamma/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'dense', 'bias']\r\n",
      "Skipping bert/encoder/layer_11/output/dense/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_11/output/dense/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'dense', 'kernel']\r\n",
      "Skipping bert/encoder/layer_11/output/dense/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_11/output/dense/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_12', 'attention', 'output', 'LayerNorm', 'beta']\r\n",
      "Skipping bert/encoder/layer_12/attention/output/LayerNorm/beta/adam_m\r\n",
      "Skipping bert/encoder/layer_12/attention/output/LayerNorm/beta/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_12', 'attention', 'output', 'LayerNorm', 'gamma']\r\n",
      "Skipping bert/encoder/layer_12/attention/output/LayerNorm/gamma/adam_m\r\n",
      "Skipping bert/encoder/layer_12/attention/output/LayerNorm/gamma/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_12', 'attention', 'output', 'dense', 'bias']\r\n",
      "Skipping bert/encoder/layer_12/attention/output/dense/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_12/attention/output/dense/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_12', 'attention', 'output', 'dense', 'kernel']\r\n",
      "Skipping bert/encoder/layer_12/attention/output/dense/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_12/attention/output/dense/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_12', 'attention', 'self', 'key', 'bias']\r\n",
      "Skipping bert/encoder/layer_12/attention/self/key/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_12/attention/self/key/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_12', 'attention', 'self', 'key', 'kernel']\r\n",
      "Skipping bert/encoder/layer_12/attention/self/key/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_12/attention/self/key/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_12', 'attention', 'self', 'query', 'bias']\r\n",
      "Skipping bert/encoder/layer_12/attention/self/query/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_12/attention/self/query/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_12', 'attention', 'self', 'query', 'kernel']\r\n",
      "Skipping bert/encoder/layer_12/attention/self/query/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_12/attention/self/query/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_12', 'attention', 'self', 'value', 'bias']\r\n",
      "Skipping bert/encoder/layer_12/attention/self/value/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_12/attention/self/value/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_12', 'attention', 'self', 'value', 'kernel']\r\n",
      "Skipping bert/encoder/layer_12/attention/self/value/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_12/attention/self/value/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_12', 'intermediate', 'dense', 'bias']\r\n",
      "Skipping bert/encoder/layer_12/intermediate/dense/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_12/intermediate/dense/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_12', 'intermediate', 'dense', 'kernel']\r\n",
      "Skipping bert/encoder/layer_12/intermediate/dense/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_12/intermediate/dense/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_12', 'output', 'LayerNorm', 'beta']\r\n",
      "Skipping bert/encoder/layer_12/output/LayerNorm/beta/adam_m\r\n",
      "Skipping bert/encoder/layer_12/output/LayerNorm/beta/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_12', 'output', 'LayerNorm', 'gamma']\r\n",
      "Skipping bert/encoder/layer_12/output/LayerNorm/gamma/adam_m\r\n",
      "Skipping bert/encoder/layer_12/output/LayerNorm/gamma/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_12', 'output', 'dense', 'bias']\r\n",
      "Skipping bert/encoder/layer_12/output/dense/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_12/output/dense/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_12', 'output', 'dense', 'kernel']\r\n",
      "Skipping bert/encoder/layer_12/output/dense/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_12/output/dense/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_13', 'attention', 'output', 'LayerNorm', 'beta']\r\n",
      "Skipping bert/encoder/layer_13/attention/output/LayerNorm/beta/adam_m\r\n",
      "Skipping bert/encoder/layer_13/attention/output/LayerNorm/beta/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_13', 'attention', 'output', 'LayerNorm', 'gamma']\r\n",
      "Skipping bert/encoder/layer_13/attention/output/LayerNorm/gamma/adam_m\r\n",
      "Skipping bert/encoder/layer_13/attention/output/LayerNorm/gamma/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_13', 'attention', 'output', 'dense', 'bias']\r\n",
      "Skipping bert/encoder/layer_13/attention/output/dense/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_13/attention/output/dense/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_13', 'attention', 'output', 'dense', 'kernel']\r\n",
      "Skipping bert/encoder/layer_13/attention/output/dense/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_13/attention/output/dense/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_13', 'attention', 'self', 'key', 'bias']\r\n",
      "Skipping bert/encoder/layer_13/attention/self/key/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_13/attention/self/key/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_13', 'attention', 'self', 'key', 'kernel']\r\n",
      "Skipping bert/encoder/layer_13/attention/self/key/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_13/attention/self/key/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_13', 'attention', 'self', 'query', 'bias']\r\n",
      "Skipping bert/encoder/layer_13/attention/self/query/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_13/attention/self/query/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_13', 'attention', 'self', 'query', 'kernel']\r\n",
      "Skipping bert/encoder/layer_13/attention/self/query/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_13/attention/self/query/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_13', 'attention', 'self', 'value', 'bias']\r\n",
      "Skipping bert/encoder/layer_13/attention/self/value/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_13/attention/self/value/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_13', 'attention', 'self', 'value', 'kernel']\r\n",
      "Skipping bert/encoder/layer_13/attention/self/value/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_13/attention/self/value/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_13', 'intermediate', 'dense', 'bias']\r\n",
      "Skipping bert/encoder/layer_13/intermediate/dense/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_13/intermediate/dense/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_13', 'intermediate', 'dense', 'kernel']\r\n",
      "Skipping bert/encoder/layer_13/intermediate/dense/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_13/intermediate/dense/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_13', 'output', 'LayerNorm', 'beta']\r\n",
      "Skipping bert/encoder/layer_13/output/LayerNorm/beta/adam_m\r\n",
      "Skipping bert/encoder/layer_13/output/LayerNorm/beta/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_13', 'output', 'LayerNorm', 'gamma']\r\n",
      "Skipping bert/encoder/layer_13/output/LayerNorm/gamma/adam_m\r\n",
      "Skipping bert/encoder/layer_13/output/LayerNorm/gamma/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_13', 'output', 'dense', 'bias']\r\n",
      "Skipping bert/encoder/layer_13/output/dense/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_13/output/dense/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_13', 'output', 'dense', 'kernel']\r\n",
      "Skipping bert/encoder/layer_13/output/dense/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_13/output/dense/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_14', 'attention', 'output', 'LayerNorm', 'beta']\r\n",
      "Skipping bert/encoder/layer_14/attention/output/LayerNorm/beta/adam_m\r\n",
      "Skipping bert/encoder/layer_14/attention/output/LayerNorm/beta/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_14', 'attention', 'output', 'LayerNorm', 'gamma']\r\n",
      "Skipping bert/encoder/layer_14/attention/output/LayerNorm/gamma/adam_m\r\n",
      "Skipping bert/encoder/layer_14/attention/output/LayerNorm/gamma/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_14', 'attention', 'output', 'dense', 'bias']\r\n",
      "Skipping bert/encoder/layer_14/attention/output/dense/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_14/attention/output/dense/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_14', 'attention', 'output', 'dense', 'kernel']\r\n",
      "Skipping bert/encoder/layer_14/attention/output/dense/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_14/attention/output/dense/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_14', 'attention', 'self', 'key', 'bias']\r\n",
      "Skipping bert/encoder/layer_14/attention/self/key/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_14/attention/self/key/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_14', 'attention', 'self', 'key', 'kernel']\r\n",
      "Skipping bert/encoder/layer_14/attention/self/key/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_14/attention/self/key/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_14', 'attention', 'self', 'query', 'bias']\r\n",
      "Skipping bert/encoder/layer_14/attention/self/query/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_14/attention/self/query/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_14', 'attention', 'self', 'query', 'kernel']\r\n",
      "Skipping bert/encoder/layer_14/attention/self/query/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_14/attention/self/query/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_14', 'attention', 'self', 'value', 'bias']\r\n",
      "Skipping bert/encoder/layer_14/attention/self/value/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_14/attention/self/value/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_14', 'attention', 'self', 'value', 'kernel']\r\n",
      "Skipping bert/encoder/layer_14/attention/self/value/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_14/attention/self/value/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_14', 'intermediate', 'dense', 'bias']\r\n",
      "Skipping bert/encoder/layer_14/intermediate/dense/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_14/intermediate/dense/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_14', 'intermediate', 'dense', 'kernel']\r\n",
      "Skipping bert/encoder/layer_14/intermediate/dense/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_14/intermediate/dense/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_14', 'output', 'LayerNorm', 'beta']\r\n",
      "Skipping bert/encoder/layer_14/output/LayerNorm/beta/adam_m\r\n",
      "Skipping bert/encoder/layer_14/output/LayerNorm/beta/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_14', 'output', 'LayerNorm', 'gamma']\r\n",
      "Skipping bert/encoder/layer_14/output/LayerNorm/gamma/adam_m\r\n",
      "Skipping bert/encoder/layer_14/output/LayerNorm/gamma/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_14', 'output', 'dense', 'bias']\r\n",
      "Skipping bert/encoder/layer_14/output/dense/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_14/output/dense/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_14', 'output', 'dense', 'kernel']\r\n",
      "Skipping bert/encoder/layer_14/output/dense/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_14/output/dense/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_15', 'attention', 'output', 'LayerNorm', 'beta']\r\n",
      "Skipping bert/encoder/layer_15/attention/output/LayerNorm/beta/adam_m\r\n",
      "Skipping bert/encoder/layer_15/attention/output/LayerNorm/beta/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_15', 'attention', 'output', 'LayerNorm', 'gamma']\r\n",
      "Skipping bert/encoder/layer_15/attention/output/LayerNorm/gamma/adam_m\r\n",
      "Skipping bert/encoder/layer_15/attention/output/LayerNorm/gamma/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_15', 'attention', 'output', 'dense', 'bias']\r\n",
      "Skipping bert/encoder/layer_15/attention/output/dense/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_15/attention/output/dense/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_15', 'attention', 'output', 'dense', 'kernel']\r\n",
      "Skipping bert/encoder/layer_15/attention/output/dense/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_15/attention/output/dense/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_15', 'attention', 'self', 'key', 'bias']\r\n",
      "Skipping bert/encoder/layer_15/attention/self/key/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_15/attention/self/key/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_15', 'attention', 'self', 'key', 'kernel']\r\n",
      "Skipping bert/encoder/layer_15/attention/self/key/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_15/attention/self/key/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_15', 'attention', 'self', 'query', 'bias']\r\n",
      "Skipping bert/encoder/layer_15/attention/self/query/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_15/attention/self/query/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_15', 'attention', 'self', 'query', 'kernel']\r\n",
      "Skipping bert/encoder/layer_15/attention/self/query/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_15/attention/self/query/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_15', 'attention', 'self', 'value', 'bias']\r\n",
      "Skipping bert/encoder/layer_15/attention/self/value/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_15/attention/self/value/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_15', 'attention', 'self', 'value', 'kernel']\r\n",
      "Skipping bert/encoder/layer_15/attention/self/value/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_15/attention/self/value/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_15', 'intermediate', 'dense', 'bias']\r\n",
      "Skipping bert/encoder/layer_15/intermediate/dense/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_15/intermediate/dense/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_15', 'intermediate', 'dense', 'kernel']\r\n",
      "Skipping bert/encoder/layer_15/intermediate/dense/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_15/intermediate/dense/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_15', 'output', 'LayerNorm', 'beta']\r\n",
      "Skipping bert/encoder/layer_15/output/LayerNorm/beta/adam_m\r\n",
      "Skipping bert/encoder/layer_15/output/LayerNorm/beta/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_15', 'output', 'LayerNorm', 'gamma']\r\n",
      "Skipping bert/encoder/layer_15/output/LayerNorm/gamma/adam_m\r\n",
      "Skipping bert/encoder/layer_15/output/LayerNorm/gamma/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_15', 'output', 'dense', 'bias']\r\n",
      "Skipping bert/encoder/layer_15/output/dense/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_15/output/dense/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_15', 'output', 'dense', 'kernel']\r\n",
      "Skipping bert/encoder/layer_15/output/dense/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_15/output/dense/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_16', 'attention', 'output', 'LayerNorm', 'beta']\r\n",
      "Skipping bert/encoder/layer_16/attention/output/LayerNorm/beta/adam_m\r\n",
      "Skipping bert/encoder/layer_16/attention/output/LayerNorm/beta/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_16', 'attention', 'output', 'LayerNorm', 'gamma']\r\n",
      "Skipping bert/encoder/layer_16/attention/output/LayerNorm/gamma/adam_m\r\n",
      "Skipping bert/encoder/layer_16/attention/output/LayerNorm/gamma/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_16', 'attention', 'output', 'dense', 'bias']\r\n",
      "Skipping bert/encoder/layer_16/attention/output/dense/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_16/attention/output/dense/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_16', 'attention', 'output', 'dense', 'kernel']\r\n",
      "Skipping bert/encoder/layer_16/attention/output/dense/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_16/attention/output/dense/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_16', 'attention', 'self', 'key', 'bias']\r\n",
      "Skipping bert/encoder/layer_16/attention/self/key/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_16/attention/self/key/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_16', 'attention', 'self', 'key', 'kernel']\r\n",
      "Skipping bert/encoder/layer_16/attention/self/key/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_16/attention/self/key/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_16', 'attention', 'self', 'query', 'bias']\r\n",
      "Skipping bert/encoder/layer_16/attention/self/query/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_16/attention/self/query/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_16', 'attention', 'self', 'query', 'kernel']\r\n",
      "Skipping bert/encoder/layer_16/attention/self/query/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_16/attention/self/query/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_16', 'attention', 'self', 'value', 'bias']\r\n",
      "Skipping bert/encoder/layer_16/attention/self/value/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_16/attention/self/value/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_16', 'attention', 'self', 'value', 'kernel']\r\n",
      "Skipping bert/encoder/layer_16/attention/self/value/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_16/attention/self/value/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_16', 'intermediate', 'dense', 'bias']\r\n",
      "Skipping bert/encoder/layer_16/intermediate/dense/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_16/intermediate/dense/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_16', 'intermediate', 'dense', 'kernel']\r\n",
      "Skipping bert/encoder/layer_16/intermediate/dense/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_16/intermediate/dense/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_16', 'output', 'LayerNorm', 'beta']\r\n",
      "Skipping bert/encoder/layer_16/output/LayerNorm/beta/adam_m\r\n",
      "Skipping bert/encoder/layer_16/output/LayerNorm/beta/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_16', 'output', 'LayerNorm', 'gamma']\r\n",
      "Skipping bert/encoder/layer_16/output/LayerNorm/gamma/adam_m\r\n",
      "Skipping bert/encoder/layer_16/output/LayerNorm/gamma/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_16', 'output', 'dense', 'bias']\r\n",
      "Skipping bert/encoder/layer_16/output/dense/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_16/output/dense/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_16', 'output', 'dense', 'kernel']\r\n",
      "Skipping bert/encoder/layer_16/output/dense/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_16/output/dense/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_17', 'attention', 'output', 'LayerNorm', 'beta']\r\n",
      "Skipping bert/encoder/layer_17/attention/output/LayerNorm/beta/adam_m\r\n",
      "Skipping bert/encoder/layer_17/attention/output/LayerNorm/beta/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_17', 'attention', 'output', 'LayerNorm', 'gamma']\r\n",
      "Skipping bert/encoder/layer_17/attention/output/LayerNorm/gamma/adam_m\r\n",
      "Skipping bert/encoder/layer_17/attention/output/LayerNorm/gamma/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_17', 'attention', 'output', 'dense', 'bias']\r\n",
      "Skipping bert/encoder/layer_17/attention/output/dense/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_17/attention/output/dense/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_17', 'attention', 'output', 'dense', 'kernel']\r\n",
      "Skipping bert/encoder/layer_17/attention/output/dense/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_17/attention/output/dense/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_17', 'attention', 'self', 'key', 'bias']\r\n",
      "Skipping bert/encoder/layer_17/attention/self/key/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_17/attention/self/key/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_17', 'attention', 'self', 'key', 'kernel']\r\n",
      "Skipping bert/encoder/layer_17/attention/self/key/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_17/attention/self/key/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_17', 'attention', 'self', 'query', 'bias']\r\n",
      "Skipping bert/encoder/layer_17/attention/self/query/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_17/attention/self/query/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_17', 'attention', 'self', 'query', 'kernel']\r\n",
      "Skipping bert/encoder/layer_17/attention/self/query/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_17/attention/self/query/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_17', 'attention', 'self', 'value', 'bias']\r\n",
      "Skipping bert/encoder/layer_17/attention/self/value/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_17/attention/self/value/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_17', 'attention', 'self', 'value', 'kernel']\r\n",
      "Skipping bert/encoder/layer_17/attention/self/value/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_17/attention/self/value/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_17', 'intermediate', 'dense', 'bias']\r\n",
      "Skipping bert/encoder/layer_17/intermediate/dense/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_17/intermediate/dense/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_17', 'intermediate', 'dense', 'kernel']\r\n",
      "Skipping bert/encoder/layer_17/intermediate/dense/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_17/intermediate/dense/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_17', 'output', 'LayerNorm', 'beta']\r\n",
      "Skipping bert/encoder/layer_17/output/LayerNorm/beta/adam_m\r\n",
      "Skipping bert/encoder/layer_17/output/LayerNorm/beta/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_17', 'output', 'LayerNorm', 'gamma']\r\n",
      "Skipping bert/encoder/layer_17/output/LayerNorm/gamma/adam_m\r\n",
      "Skipping bert/encoder/layer_17/output/LayerNorm/gamma/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_17', 'output', 'dense', 'bias']\r\n",
      "Skipping bert/encoder/layer_17/output/dense/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_17/output/dense/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_17', 'output', 'dense', 'kernel']\r\n",
      "Skipping bert/encoder/layer_17/output/dense/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_17/output/dense/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_18', 'attention', 'output', 'LayerNorm', 'beta']\r\n",
      "Skipping bert/encoder/layer_18/attention/output/LayerNorm/beta/adam_m\r\n",
      "Skipping bert/encoder/layer_18/attention/output/LayerNorm/beta/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_18', 'attention', 'output', 'LayerNorm', 'gamma']\r\n",
      "Skipping bert/encoder/layer_18/attention/output/LayerNorm/gamma/adam_m\r\n",
      "Skipping bert/encoder/layer_18/attention/output/LayerNorm/gamma/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_18', 'attention', 'output', 'dense', 'bias']\r\n",
      "Skipping bert/encoder/layer_18/attention/output/dense/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_18/attention/output/dense/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_18', 'attention', 'output', 'dense', 'kernel']\r\n",
      "Skipping bert/encoder/layer_18/attention/output/dense/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_18/attention/output/dense/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_18', 'attention', 'self', 'key', 'bias']\r\n",
      "Skipping bert/encoder/layer_18/attention/self/key/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_18/attention/self/key/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_18', 'attention', 'self', 'key', 'kernel']\r\n",
      "Skipping bert/encoder/layer_18/attention/self/key/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_18/attention/self/key/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_18', 'attention', 'self', 'query', 'bias']\r\n",
      "Skipping bert/encoder/layer_18/attention/self/query/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_18/attention/self/query/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_18', 'attention', 'self', 'query', 'kernel']\r\n",
      "Skipping bert/encoder/layer_18/attention/self/query/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_18/attention/self/query/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_18', 'attention', 'self', 'value', 'bias']\r\n",
      "Skipping bert/encoder/layer_18/attention/self/value/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_18/attention/self/value/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_18', 'attention', 'self', 'value', 'kernel']\r\n",
      "Skipping bert/encoder/layer_18/attention/self/value/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_18/attention/self/value/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_18', 'intermediate', 'dense', 'bias']\r\n",
      "Skipping bert/encoder/layer_18/intermediate/dense/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_18/intermediate/dense/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_18', 'intermediate', 'dense', 'kernel']\r\n",
      "Skipping bert/encoder/layer_18/intermediate/dense/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_18/intermediate/dense/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_18', 'output', 'LayerNorm', 'beta']\r\n",
      "Skipping bert/encoder/layer_18/output/LayerNorm/beta/adam_m\r\n",
      "Skipping bert/encoder/layer_18/output/LayerNorm/beta/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_18', 'output', 'LayerNorm', 'gamma']\r\n",
      "Skipping bert/encoder/layer_18/output/LayerNorm/gamma/adam_m\r\n",
      "Skipping bert/encoder/layer_18/output/LayerNorm/gamma/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_18', 'output', 'dense', 'bias']\r\n",
      "Skipping bert/encoder/layer_18/output/dense/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_18/output/dense/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_18', 'output', 'dense', 'kernel']\r\n",
      "Skipping bert/encoder/layer_18/output/dense/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_18/output/dense/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_19', 'attention', 'output', 'LayerNorm', 'beta']\r\n",
      "Skipping bert/encoder/layer_19/attention/output/LayerNorm/beta/adam_m\r\n",
      "Skipping bert/encoder/layer_19/attention/output/LayerNorm/beta/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_19', 'attention', 'output', 'LayerNorm', 'gamma']\r\n",
      "Skipping bert/encoder/layer_19/attention/output/LayerNorm/gamma/adam_m\r\n",
      "Skipping bert/encoder/layer_19/attention/output/LayerNorm/gamma/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_19', 'attention', 'output', 'dense', 'bias']\r\n",
      "Skipping bert/encoder/layer_19/attention/output/dense/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_19/attention/output/dense/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_19', 'attention', 'output', 'dense', 'kernel']\r\n",
      "Skipping bert/encoder/layer_19/attention/output/dense/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_19/attention/output/dense/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_19', 'attention', 'self', 'key', 'bias']\r\n",
      "Skipping bert/encoder/layer_19/attention/self/key/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_19/attention/self/key/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_19', 'attention', 'self', 'key', 'kernel']\r\n",
      "Skipping bert/encoder/layer_19/attention/self/key/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_19/attention/self/key/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_19', 'attention', 'self', 'query', 'bias']\r\n",
      "Skipping bert/encoder/layer_19/attention/self/query/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_19/attention/self/query/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_19', 'attention', 'self', 'query', 'kernel']\r\n",
      "Skipping bert/encoder/layer_19/attention/self/query/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_19/attention/self/query/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_19', 'attention', 'self', 'value', 'bias']\r\n",
      "Skipping bert/encoder/layer_19/attention/self/value/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_19/attention/self/value/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_19', 'attention', 'self', 'value', 'kernel']\r\n",
      "Skipping bert/encoder/layer_19/attention/self/value/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_19/attention/self/value/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_19', 'intermediate', 'dense', 'bias']\r\n",
      "Skipping bert/encoder/layer_19/intermediate/dense/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_19/intermediate/dense/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_19', 'intermediate', 'dense', 'kernel']\r\n",
      "Skipping bert/encoder/layer_19/intermediate/dense/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_19/intermediate/dense/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_19', 'output', 'LayerNorm', 'beta']\r\n",
      "Skipping bert/encoder/layer_19/output/LayerNorm/beta/adam_m\r\n",
      "Skipping bert/encoder/layer_19/output/LayerNorm/beta/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_19', 'output', 'LayerNorm', 'gamma']\r\n",
      "Skipping bert/encoder/layer_19/output/LayerNorm/gamma/adam_m\r\n",
      "Skipping bert/encoder/layer_19/output/LayerNorm/gamma/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_19', 'output', 'dense', 'bias']\r\n",
      "Skipping bert/encoder/layer_19/output/dense/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_19/output/dense/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_19', 'output', 'dense', 'kernel']\r\n",
      "Skipping bert/encoder/layer_19/output/dense/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_19/output/dense/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'LayerNorm', 'beta']\r\n",
      "Skipping bert/encoder/layer_2/attention/output/LayerNorm/beta/adam_m\r\n",
      "Skipping bert/encoder/layer_2/attention/output/LayerNorm/beta/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'LayerNorm', 'gamma']\r\n",
      "Skipping bert/encoder/layer_2/attention/output/LayerNorm/gamma/adam_m\r\n",
      "Skipping bert/encoder/layer_2/attention/output/LayerNorm/gamma/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'dense', 'bias']\r\n",
      "Skipping bert/encoder/layer_2/attention/output/dense/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_2/attention/output/dense/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'dense', 'kernel']\r\n",
      "Skipping bert/encoder/layer_2/attention/output/dense/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_2/attention/output/dense/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'key', 'bias']\r\n",
      "Skipping bert/encoder/layer_2/attention/self/key/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_2/attention/self/key/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'key', 'kernel']\r\n",
      "Skipping bert/encoder/layer_2/attention/self/key/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_2/attention/self/key/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'query', 'bias']\r\n",
      "Skipping bert/encoder/layer_2/attention/self/query/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_2/attention/self/query/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'query', 'kernel']\r\n",
      "Skipping bert/encoder/layer_2/attention/self/query/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_2/attention/self/query/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'value', 'bias']\r\n",
      "Skipping bert/encoder/layer_2/attention/self/value/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_2/attention/self/value/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'value', 'kernel']\r\n",
      "Skipping bert/encoder/layer_2/attention/self/value/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_2/attention/self/value/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'intermediate', 'dense', 'bias']\r\n",
      "Skipping bert/encoder/layer_2/intermediate/dense/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_2/intermediate/dense/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'intermediate', 'dense', 'kernel']\r\n",
      "Skipping bert/encoder/layer_2/intermediate/dense/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_2/intermediate/dense/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'LayerNorm', 'beta']\r\n",
      "Skipping bert/encoder/layer_2/output/LayerNorm/beta/adam_m\r\n",
      "Skipping bert/encoder/layer_2/output/LayerNorm/beta/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'LayerNorm', 'gamma']\r\n",
      "Skipping bert/encoder/layer_2/output/LayerNorm/gamma/adam_m\r\n",
      "Skipping bert/encoder/layer_2/output/LayerNorm/gamma/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'dense', 'bias']\r\n",
      "Skipping bert/encoder/layer_2/output/dense/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_2/output/dense/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'dense', 'kernel']\r\n",
      "Skipping bert/encoder/layer_2/output/dense/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_2/output/dense/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_20', 'attention', 'output', 'LayerNorm', 'beta']\r\n",
      "Skipping bert/encoder/layer_20/attention/output/LayerNorm/beta/adam_m\r\n",
      "Skipping bert/encoder/layer_20/attention/output/LayerNorm/beta/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_20', 'attention', 'output', 'LayerNorm', 'gamma']\r\n",
      "Skipping bert/encoder/layer_20/attention/output/LayerNorm/gamma/adam_m\r\n",
      "Skipping bert/encoder/layer_20/attention/output/LayerNorm/gamma/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_20', 'attention', 'output', 'dense', 'bias']\r\n",
      "Skipping bert/encoder/layer_20/attention/output/dense/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_20/attention/output/dense/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_20', 'attention', 'output', 'dense', 'kernel']\r\n",
      "Skipping bert/encoder/layer_20/attention/output/dense/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_20/attention/output/dense/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_20', 'attention', 'self', 'key', 'bias']\r\n",
      "Skipping bert/encoder/layer_20/attention/self/key/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_20/attention/self/key/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_20', 'attention', 'self', 'key', 'kernel']\r\n",
      "Skipping bert/encoder/layer_20/attention/self/key/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_20/attention/self/key/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_20', 'attention', 'self', 'query', 'bias']\r\n",
      "Skipping bert/encoder/layer_20/attention/self/query/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_20/attention/self/query/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_20', 'attention', 'self', 'query', 'kernel']\r\n",
      "Skipping bert/encoder/layer_20/attention/self/query/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_20/attention/self/query/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_20', 'attention', 'self', 'value', 'bias']\r\n",
      "Skipping bert/encoder/layer_20/attention/self/value/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_20/attention/self/value/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_20', 'attention', 'self', 'value', 'kernel']\r\n",
      "Skipping bert/encoder/layer_20/attention/self/value/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_20/attention/self/value/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_20', 'intermediate', 'dense', 'bias']\r\n",
      "Skipping bert/encoder/layer_20/intermediate/dense/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_20/intermediate/dense/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_20', 'intermediate', 'dense', 'kernel']\r\n",
      "Skipping bert/encoder/layer_20/intermediate/dense/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_20/intermediate/dense/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_20', 'output', 'LayerNorm', 'beta']\r\n",
      "Skipping bert/encoder/layer_20/output/LayerNorm/beta/adam_m\r\n",
      "Skipping bert/encoder/layer_20/output/LayerNorm/beta/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_20', 'output', 'LayerNorm', 'gamma']\r\n",
      "Skipping bert/encoder/layer_20/output/LayerNorm/gamma/adam_m\r\n",
      "Skipping bert/encoder/layer_20/output/LayerNorm/gamma/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_20', 'output', 'dense', 'bias']\r\n",
      "Skipping bert/encoder/layer_20/output/dense/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_20/output/dense/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_20', 'output', 'dense', 'kernel']\r\n",
      "Skipping bert/encoder/layer_20/output/dense/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_20/output/dense/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_21', 'attention', 'output', 'LayerNorm', 'beta']\r\n",
      "Skipping bert/encoder/layer_21/attention/output/LayerNorm/beta/adam_m\r\n",
      "Skipping bert/encoder/layer_21/attention/output/LayerNorm/beta/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_21', 'attention', 'output', 'LayerNorm', 'gamma']\r\n",
      "Skipping bert/encoder/layer_21/attention/output/LayerNorm/gamma/adam_m\r\n",
      "Skipping bert/encoder/layer_21/attention/output/LayerNorm/gamma/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_21', 'attention', 'output', 'dense', 'bias']\r\n",
      "Skipping bert/encoder/layer_21/attention/output/dense/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_21/attention/output/dense/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_21', 'attention', 'output', 'dense', 'kernel']\r\n",
      "Skipping bert/encoder/layer_21/attention/output/dense/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_21/attention/output/dense/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_21', 'attention', 'self', 'key', 'bias']\r\n",
      "Skipping bert/encoder/layer_21/attention/self/key/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_21/attention/self/key/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_21', 'attention', 'self', 'key', 'kernel']\r\n",
      "Skipping bert/encoder/layer_21/attention/self/key/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_21/attention/self/key/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_21', 'attention', 'self', 'query', 'bias']\r\n",
      "Skipping bert/encoder/layer_21/attention/self/query/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_21/attention/self/query/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_21', 'attention', 'self', 'query', 'kernel']\r\n",
      "Skipping bert/encoder/layer_21/attention/self/query/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_21/attention/self/query/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_21', 'attention', 'self', 'value', 'bias']\r\n",
      "Skipping bert/encoder/layer_21/attention/self/value/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_21/attention/self/value/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_21', 'attention', 'self', 'value', 'kernel']\r\n",
      "Skipping bert/encoder/layer_21/attention/self/value/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_21/attention/self/value/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_21', 'intermediate', 'dense', 'bias']\r\n",
      "Skipping bert/encoder/layer_21/intermediate/dense/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_21/intermediate/dense/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_21', 'intermediate', 'dense', 'kernel']\r\n",
      "Skipping bert/encoder/layer_21/intermediate/dense/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_21/intermediate/dense/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_21', 'output', 'LayerNorm', 'beta']\r\n",
      "Skipping bert/encoder/layer_21/output/LayerNorm/beta/adam_m\r\n",
      "Skipping bert/encoder/layer_21/output/LayerNorm/beta/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_21', 'output', 'LayerNorm', 'gamma']\r\n",
      "Skipping bert/encoder/layer_21/output/LayerNorm/gamma/adam_m\r\n",
      "Skipping bert/encoder/layer_21/output/LayerNorm/gamma/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_21', 'output', 'dense', 'bias']\r\n",
      "Skipping bert/encoder/layer_21/output/dense/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_21/output/dense/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_21', 'output', 'dense', 'kernel']\r\n",
      "Skipping bert/encoder/layer_21/output/dense/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_21/output/dense/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_22', 'attention', 'output', 'LayerNorm', 'beta']\r\n",
      "Skipping bert/encoder/layer_22/attention/output/LayerNorm/beta/adam_m\r\n",
      "Skipping bert/encoder/layer_22/attention/output/LayerNorm/beta/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_22', 'attention', 'output', 'LayerNorm', 'gamma']\r\n",
      "Skipping bert/encoder/layer_22/attention/output/LayerNorm/gamma/adam_m\r\n",
      "Skipping bert/encoder/layer_22/attention/output/LayerNorm/gamma/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_22', 'attention', 'output', 'dense', 'bias']\r\n",
      "Skipping bert/encoder/layer_22/attention/output/dense/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_22/attention/output/dense/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_22', 'attention', 'output', 'dense', 'kernel']\r\n",
      "Skipping bert/encoder/layer_22/attention/output/dense/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_22/attention/output/dense/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_22', 'attention', 'self', 'key', 'bias']\r\n",
      "Skipping bert/encoder/layer_22/attention/self/key/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_22/attention/self/key/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_22', 'attention', 'self', 'key', 'kernel']\r\n",
      "Skipping bert/encoder/layer_22/attention/self/key/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_22/attention/self/key/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_22', 'attention', 'self', 'query', 'bias']\r\n",
      "Skipping bert/encoder/layer_22/attention/self/query/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_22/attention/self/query/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_22', 'attention', 'self', 'query', 'kernel']\r\n",
      "Skipping bert/encoder/layer_22/attention/self/query/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_22/attention/self/query/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_22', 'attention', 'self', 'value', 'bias']\r\n",
      "Skipping bert/encoder/layer_22/attention/self/value/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_22/attention/self/value/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_22', 'attention', 'self', 'value', 'kernel']\r\n",
      "Skipping bert/encoder/layer_22/attention/self/value/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_22/attention/self/value/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_22', 'intermediate', 'dense', 'bias']\r\n",
      "Skipping bert/encoder/layer_22/intermediate/dense/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_22/intermediate/dense/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_22', 'intermediate', 'dense', 'kernel']\r\n",
      "Skipping bert/encoder/layer_22/intermediate/dense/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_22/intermediate/dense/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_22', 'output', 'LayerNorm', 'beta']\r\n",
      "Skipping bert/encoder/layer_22/output/LayerNorm/beta/adam_m\r\n",
      "Skipping bert/encoder/layer_22/output/LayerNorm/beta/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_22', 'output', 'LayerNorm', 'gamma']\r\n",
      "Skipping bert/encoder/layer_22/output/LayerNorm/gamma/adam_m\r\n",
      "Skipping bert/encoder/layer_22/output/LayerNorm/gamma/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_22', 'output', 'dense', 'bias']\r\n",
      "Skipping bert/encoder/layer_22/output/dense/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_22/output/dense/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_22', 'output', 'dense', 'kernel']\r\n",
      "Skipping bert/encoder/layer_22/output/dense/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_22/output/dense/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_23', 'attention', 'output', 'LayerNorm', 'beta']\r\n",
      "Skipping bert/encoder/layer_23/attention/output/LayerNorm/beta/adam_m\r\n",
      "Skipping bert/encoder/layer_23/attention/output/LayerNorm/beta/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_23', 'attention', 'output', 'LayerNorm', 'gamma']\r\n",
      "Skipping bert/encoder/layer_23/attention/output/LayerNorm/gamma/adam_m\r\n",
      "Skipping bert/encoder/layer_23/attention/output/LayerNorm/gamma/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_23', 'attention', 'output', 'dense', 'bias']\r\n",
      "Skipping bert/encoder/layer_23/attention/output/dense/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_23/attention/output/dense/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_23', 'attention', 'output', 'dense', 'kernel']\r\n",
      "Skipping bert/encoder/layer_23/attention/output/dense/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_23/attention/output/dense/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_23', 'attention', 'self', 'key', 'bias']\r\n",
      "Skipping bert/encoder/layer_23/attention/self/key/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_23/attention/self/key/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_23', 'attention', 'self', 'key', 'kernel']\r\n",
      "Skipping bert/encoder/layer_23/attention/self/key/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_23/attention/self/key/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_23', 'attention', 'self', 'query', 'bias']\r\n",
      "Skipping bert/encoder/layer_23/attention/self/query/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_23/attention/self/query/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_23', 'attention', 'self', 'query', 'kernel']\r\n",
      "Skipping bert/encoder/layer_23/attention/self/query/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_23/attention/self/query/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_23', 'attention', 'self', 'value', 'bias']\r\n",
      "Skipping bert/encoder/layer_23/attention/self/value/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_23/attention/self/value/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_23', 'attention', 'self', 'value', 'kernel']\r\n",
      "Skipping bert/encoder/layer_23/attention/self/value/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_23/attention/self/value/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_23', 'intermediate', 'dense', 'bias']\r\n",
      "Skipping bert/encoder/layer_23/intermediate/dense/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_23/intermediate/dense/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_23', 'intermediate', 'dense', 'kernel']\r\n",
      "Skipping bert/encoder/layer_23/intermediate/dense/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_23/intermediate/dense/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_23', 'output', 'LayerNorm', 'beta']\r\n",
      "Skipping bert/encoder/layer_23/output/LayerNorm/beta/adam_m\r\n",
      "Skipping bert/encoder/layer_23/output/LayerNorm/beta/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_23', 'output', 'LayerNorm', 'gamma']\r\n",
      "Skipping bert/encoder/layer_23/output/LayerNorm/gamma/adam_m\r\n",
      "Skipping bert/encoder/layer_23/output/LayerNorm/gamma/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_23', 'output', 'dense', 'bias']\r\n",
      "Skipping bert/encoder/layer_23/output/dense/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_23/output/dense/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_23', 'output', 'dense', 'kernel']\r\n",
      "Skipping bert/encoder/layer_23/output/dense/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_23/output/dense/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'LayerNorm', 'beta']\r\n",
      "Skipping bert/encoder/layer_3/attention/output/LayerNorm/beta/adam_m\r\n",
      "Skipping bert/encoder/layer_3/attention/output/LayerNorm/beta/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'LayerNorm', 'gamma']\r\n",
      "Skipping bert/encoder/layer_3/attention/output/LayerNorm/gamma/adam_m\r\n",
      "Skipping bert/encoder/layer_3/attention/output/LayerNorm/gamma/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'dense', 'bias']\r\n",
      "Skipping bert/encoder/layer_3/attention/output/dense/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_3/attention/output/dense/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'dense', 'kernel']\r\n",
      "Skipping bert/encoder/layer_3/attention/output/dense/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_3/attention/output/dense/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'key', 'bias']\r\n",
      "Skipping bert/encoder/layer_3/attention/self/key/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_3/attention/self/key/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'key', 'kernel']\r\n",
      "Skipping bert/encoder/layer_3/attention/self/key/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_3/attention/self/key/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'query', 'bias']\r\n",
      "Skipping bert/encoder/layer_3/attention/self/query/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_3/attention/self/query/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'query', 'kernel']\r\n",
      "Skipping bert/encoder/layer_3/attention/self/query/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_3/attention/self/query/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'value', 'bias']\r\n",
      "Skipping bert/encoder/layer_3/attention/self/value/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_3/attention/self/value/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'value', 'kernel']\r\n",
      "Skipping bert/encoder/layer_3/attention/self/value/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_3/attention/self/value/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'intermediate', 'dense', 'bias']\r\n",
      "Skipping bert/encoder/layer_3/intermediate/dense/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_3/intermediate/dense/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'intermediate', 'dense', 'kernel']\r\n",
      "Skipping bert/encoder/layer_3/intermediate/dense/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_3/intermediate/dense/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'LayerNorm', 'beta']\r\n",
      "Skipping bert/encoder/layer_3/output/LayerNorm/beta/adam_m\r\n",
      "Skipping bert/encoder/layer_3/output/LayerNorm/beta/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'LayerNorm', 'gamma']\r\n",
      "Skipping bert/encoder/layer_3/output/LayerNorm/gamma/adam_m\r\n",
      "Skipping bert/encoder/layer_3/output/LayerNorm/gamma/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'dense', 'bias']\r\n",
      "Skipping bert/encoder/layer_3/output/dense/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_3/output/dense/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'dense', 'kernel']\r\n",
      "Skipping bert/encoder/layer_3/output/dense/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_3/output/dense/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'LayerNorm', 'beta']\r\n",
      "Skipping bert/encoder/layer_4/attention/output/LayerNorm/beta/adam_m\r\n",
      "Skipping bert/encoder/layer_4/attention/output/LayerNorm/beta/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'LayerNorm', 'gamma']\r\n",
      "Skipping bert/encoder/layer_4/attention/output/LayerNorm/gamma/adam_m\r\n",
      "Skipping bert/encoder/layer_4/attention/output/LayerNorm/gamma/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'dense', 'bias']\r\n",
      "Skipping bert/encoder/layer_4/attention/output/dense/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_4/attention/output/dense/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'dense', 'kernel']\r\n",
      "Skipping bert/encoder/layer_4/attention/output/dense/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_4/attention/output/dense/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'key', 'bias']\r\n",
      "Skipping bert/encoder/layer_4/attention/self/key/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_4/attention/self/key/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'key', 'kernel']\r\n",
      "Skipping bert/encoder/layer_4/attention/self/key/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_4/attention/self/key/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'query', 'bias']\r\n",
      "Skipping bert/encoder/layer_4/attention/self/query/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_4/attention/self/query/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'query', 'kernel']\r\n",
      "Skipping bert/encoder/layer_4/attention/self/query/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_4/attention/self/query/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'value', 'bias']\r\n",
      "Skipping bert/encoder/layer_4/attention/self/value/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_4/attention/self/value/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'value', 'kernel']\r\n",
      "Skipping bert/encoder/layer_4/attention/self/value/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_4/attention/self/value/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'intermediate', 'dense', 'bias']\r\n",
      "Skipping bert/encoder/layer_4/intermediate/dense/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_4/intermediate/dense/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'intermediate', 'dense', 'kernel']\r\n",
      "Skipping bert/encoder/layer_4/intermediate/dense/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_4/intermediate/dense/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'LayerNorm', 'beta']\r\n",
      "Skipping bert/encoder/layer_4/output/LayerNorm/beta/adam_m\r\n",
      "Skipping bert/encoder/layer_4/output/LayerNorm/beta/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'LayerNorm', 'gamma']\r\n",
      "Skipping bert/encoder/layer_4/output/LayerNorm/gamma/adam_m\r\n",
      "Skipping bert/encoder/layer_4/output/LayerNorm/gamma/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'dense', 'bias']\r\n",
      "Skipping bert/encoder/layer_4/output/dense/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_4/output/dense/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'dense', 'kernel']\r\n",
      "Skipping bert/encoder/layer_4/output/dense/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_4/output/dense/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'LayerNorm', 'beta']\r\n",
      "Skipping bert/encoder/layer_5/attention/output/LayerNorm/beta/adam_m\r\n",
      "Skipping bert/encoder/layer_5/attention/output/LayerNorm/beta/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'LayerNorm', 'gamma']\r\n",
      "Skipping bert/encoder/layer_5/attention/output/LayerNorm/gamma/adam_m\r\n",
      "Skipping bert/encoder/layer_5/attention/output/LayerNorm/gamma/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'dense', 'bias']\r\n",
      "Skipping bert/encoder/layer_5/attention/output/dense/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_5/attention/output/dense/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'dense', 'kernel']\r\n",
      "Skipping bert/encoder/layer_5/attention/output/dense/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_5/attention/output/dense/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'key', 'bias']\r\n",
      "Skipping bert/encoder/layer_5/attention/self/key/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_5/attention/self/key/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'key', 'kernel']\r\n",
      "Skipping bert/encoder/layer_5/attention/self/key/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_5/attention/self/key/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'query', 'bias']\r\n",
      "Skipping bert/encoder/layer_5/attention/self/query/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_5/attention/self/query/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'query', 'kernel']\r\n",
      "Skipping bert/encoder/layer_5/attention/self/query/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_5/attention/self/query/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'value', 'bias']\r\n",
      "Skipping bert/encoder/layer_5/attention/self/value/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_5/attention/self/value/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'value', 'kernel']\r\n",
      "Skipping bert/encoder/layer_5/attention/self/value/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_5/attention/self/value/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'intermediate', 'dense', 'bias']\r\n",
      "Skipping bert/encoder/layer_5/intermediate/dense/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_5/intermediate/dense/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'intermediate', 'dense', 'kernel']\r\n",
      "Skipping bert/encoder/layer_5/intermediate/dense/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_5/intermediate/dense/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'LayerNorm', 'beta']\r\n",
      "Skipping bert/encoder/layer_5/output/LayerNorm/beta/adam_m\r\n",
      "Skipping bert/encoder/layer_5/output/LayerNorm/beta/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'LayerNorm', 'gamma']\r\n",
      "Skipping bert/encoder/layer_5/output/LayerNorm/gamma/adam_m\r\n",
      "Skipping bert/encoder/layer_5/output/LayerNorm/gamma/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'dense', 'bias']\r\n",
      "Skipping bert/encoder/layer_5/output/dense/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_5/output/dense/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'dense', 'kernel']\r\n",
      "Skipping bert/encoder/layer_5/output/dense/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_5/output/dense/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'LayerNorm', 'beta']\r\n",
      "Skipping bert/encoder/layer_6/attention/output/LayerNorm/beta/adam_m\r\n",
      "Skipping bert/encoder/layer_6/attention/output/LayerNorm/beta/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'LayerNorm', 'gamma']\r\n",
      "Skipping bert/encoder/layer_6/attention/output/LayerNorm/gamma/adam_m\r\n",
      "Skipping bert/encoder/layer_6/attention/output/LayerNorm/gamma/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'dense', 'bias']\r\n",
      "Skipping bert/encoder/layer_6/attention/output/dense/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_6/attention/output/dense/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'dense', 'kernel']\r\n",
      "Skipping bert/encoder/layer_6/attention/output/dense/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_6/attention/output/dense/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'key', 'bias']\r\n",
      "Skipping bert/encoder/layer_6/attention/self/key/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_6/attention/self/key/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'key', 'kernel']\r\n",
      "Skipping bert/encoder/layer_6/attention/self/key/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_6/attention/self/key/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'query', 'bias']\r\n",
      "Skipping bert/encoder/layer_6/attention/self/query/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_6/attention/self/query/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'query', 'kernel']\r\n",
      "Skipping bert/encoder/layer_6/attention/self/query/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_6/attention/self/query/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'value', 'bias']\r\n",
      "Skipping bert/encoder/layer_6/attention/self/value/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_6/attention/self/value/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'value', 'kernel']\r\n",
      "Skipping bert/encoder/layer_6/attention/self/value/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_6/attention/self/value/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'intermediate', 'dense', 'bias']\r\n",
      "Skipping bert/encoder/layer_6/intermediate/dense/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_6/intermediate/dense/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'intermediate', 'dense', 'kernel']\r\n",
      "Skipping bert/encoder/layer_6/intermediate/dense/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_6/intermediate/dense/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'LayerNorm', 'beta']\r\n",
      "Skipping bert/encoder/layer_6/output/LayerNorm/beta/adam_m\r\n",
      "Skipping bert/encoder/layer_6/output/LayerNorm/beta/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'LayerNorm', 'gamma']\r\n",
      "Skipping bert/encoder/layer_6/output/LayerNorm/gamma/adam_m\r\n",
      "Skipping bert/encoder/layer_6/output/LayerNorm/gamma/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'dense', 'bias']\r\n",
      "Skipping bert/encoder/layer_6/output/dense/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_6/output/dense/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'dense', 'kernel']\r\n",
      "Skipping bert/encoder/layer_6/output/dense/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_6/output/dense/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'LayerNorm', 'beta']\r\n",
      "Skipping bert/encoder/layer_7/attention/output/LayerNorm/beta/adam_m\r\n",
      "Skipping bert/encoder/layer_7/attention/output/LayerNorm/beta/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'LayerNorm', 'gamma']\r\n",
      "Skipping bert/encoder/layer_7/attention/output/LayerNorm/gamma/adam_m\r\n",
      "Skipping bert/encoder/layer_7/attention/output/LayerNorm/gamma/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'dense', 'bias']\r\n",
      "Skipping bert/encoder/layer_7/attention/output/dense/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_7/attention/output/dense/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'dense', 'kernel']\r\n",
      "Skipping bert/encoder/layer_7/attention/output/dense/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_7/attention/output/dense/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'key', 'bias']\r\n",
      "Skipping bert/encoder/layer_7/attention/self/key/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_7/attention/self/key/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'key', 'kernel']\r\n",
      "Skipping bert/encoder/layer_7/attention/self/key/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_7/attention/self/key/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'query', 'bias']\r\n",
      "Skipping bert/encoder/layer_7/attention/self/query/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_7/attention/self/query/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'query', 'kernel']\r\n",
      "Skipping bert/encoder/layer_7/attention/self/query/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_7/attention/self/query/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'value', 'bias']\r\n",
      "Skipping bert/encoder/layer_7/attention/self/value/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_7/attention/self/value/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'value', 'kernel']\r\n",
      "Skipping bert/encoder/layer_7/attention/self/value/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_7/attention/self/value/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'intermediate', 'dense', 'bias']\r\n",
      "Skipping bert/encoder/layer_7/intermediate/dense/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_7/intermediate/dense/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'intermediate', 'dense', 'kernel']\r\n",
      "Skipping bert/encoder/layer_7/intermediate/dense/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_7/intermediate/dense/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'LayerNorm', 'beta']\r\n",
      "Skipping bert/encoder/layer_7/output/LayerNorm/beta/adam_m\r\n",
      "Skipping bert/encoder/layer_7/output/LayerNorm/beta/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'LayerNorm', 'gamma']\r\n",
      "Skipping bert/encoder/layer_7/output/LayerNorm/gamma/adam_m\r\n",
      "Skipping bert/encoder/layer_7/output/LayerNorm/gamma/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'dense', 'bias']\r\n",
      "Skipping bert/encoder/layer_7/output/dense/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_7/output/dense/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'dense', 'kernel']\r\n",
      "Skipping bert/encoder/layer_7/output/dense/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_7/output/dense/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'LayerNorm', 'beta']\r\n",
      "Skipping bert/encoder/layer_8/attention/output/LayerNorm/beta/adam_m\r\n",
      "Skipping bert/encoder/layer_8/attention/output/LayerNorm/beta/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'LayerNorm', 'gamma']\r\n",
      "Skipping bert/encoder/layer_8/attention/output/LayerNorm/gamma/adam_m\r\n",
      "Skipping bert/encoder/layer_8/attention/output/LayerNorm/gamma/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'dense', 'bias']\r\n",
      "Skipping bert/encoder/layer_8/attention/output/dense/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_8/attention/output/dense/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'dense', 'kernel']\r\n",
      "Skipping bert/encoder/layer_8/attention/output/dense/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_8/attention/output/dense/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'key', 'bias']\r\n",
      "Skipping bert/encoder/layer_8/attention/self/key/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_8/attention/self/key/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'key', 'kernel']\r\n",
      "Skipping bert/encoder/layer_8/attention/self/key/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_8/attention/self/key/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'query', 'bias']\r\n",
      "Skipping bert/encoder/layer_8/attention/self/query/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_8/attention/self/query/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'query', 'kernel']\r\n",
      "Skipping bert/encoder/layer_8/attention/self/query/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_8/attention/self/query/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'value', 'bias']\r\n",
      "Skipping bert/encoder/layer_8/attention/self/value/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_8/attention/self/value/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'value', 'kernel']\r\n",
      "Skipping bert/encoder/layer_8/attention/self/value/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_8/attention/self/value/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'intermediate', 'dense', 'bias']\r\n",
      "Skipping bert/encoder/layer_8/intermediate/dense/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_8/intermediate/dense/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'intermediate', 'dense', 'kernel']\r\n",
      "Skipping bert/encoder/layer_8/intermediate/dense/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_8/intermediate/dense/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'LayerNorm', 'beta']\r\n",
      "Skipping bert/encoder/layer_8/output/LayerNorm/beta/adam_m\r\n",
      "Skipping bert/encoder/layer_8/output/LayerNorm/beta/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'LayerNorm', 'gamma']\r\n",
      "Skipping bert/encoder/layer_8/output/LayerNorm/gamma/adam_m\r\n",
      "Skipping bert/encoder/layer_8/output/LayerNorm/gamma/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'dense', 'bias']\r\n",
      "Skipping bert/encoder/layer_8/output/dense/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_8/output/dense/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'dense', 'kernel']\r\n",
      "Skipping bert/encoder/layer_8/output/dense/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_8/output/dense/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'LayerNorm', 'beta']\r\n",
      "Skipping bert/encoder/layer_9/attention/output/LayerNorm/beta/adam_m\r\n",
      "Skipping bert/encoder/layer_9/attention/output/LayerNorm/beta/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'LayerNorm', 'gamma']\r\n",
      "Skipping bert/encoder/layer_9/attention/output/LayerNorm/gamma/adam_m\r\n",
      "Skipping bert/encoder/layer_9/attention/output/LayerNorm/gamma/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'dense', 'bias']\r\n",
      "Skipping bert/encoder/layer_9/attention/output/dense/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_9/attention/output/dense/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'dense', 'kernel']\r\n",
      "Skipping bert/encoder/layer_9/attention/output/dense/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_9/attention/output/dense/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'key', 'bias']\r\n",
      "Skipping bert/encoder/layer_9/attention/self/key/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_9/attention/self/key/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'key', 'kernel']\r\n",
      "Skipping bert/encoder/layer_9/attention/self/key/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_9/attention/self/key/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'query', 'bias']\r\n",
      "Skipping bert/encoder/layer_9/attention/self/query/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_9/attention/self/query/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'query', 'kernel']\r\n",
      "Skipping bert/encoder/layer_9/attention/self/query/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_9/attention/self/query/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'value', 'bias']\r\n",
      "Skipping bert/encoder/layer_9/attention/self/value/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_9/attention/self/value/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'value', 'kernel']\r\n",
      "Skipping bert/encoder/layer_9/attention/self/value/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_9/attention/self/value/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'intermediate', 'dense', 'bias']\r\n",
      "Skipping bert/encoder/layer_9/intermediate/dense/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_9/intermediate/dense/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'intermediate', 'dense', 'kernel']\r\n",
      "Skipping bert/encoder/layer_9/intermediate/dense/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_9/intermediate/dense/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'LayerNorm', 'beta']\r\n",
      "Skipping bert/encoder/layer_9/output/LayerNorm/beta/adam_m\r\n",
      "Skipping bert/encoder/layer_9/output/LayerNorm/beta/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'LayerNorm', 'gamma']\r\n",
      "Skipping bert/encoder/layer_9/output/LayerNorm/gamma/adam_m\r\n",
      "Skipping bert/encoder/layer_9/output/LayerNorm/gamma/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'dense', 'bias']\r\n",
      "Skipping bert/encoder/layer_9/output/dense/bias/adam_m\r\n",
      "Skipping bert/encoder/layer_9/output/dense/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'dense', 'kernel']\r\n",
      "Skipping bert/encoder/layer_9/output/dense/kernel/adam_m\r\n",
      "Skipping bert/encoder/layer_9/output/dense/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'pooler', 'dense', 'bias']\r\n",
      "Skipping bert/pooler/dense/bias/adam_m\r\n",
      "Skipping bert/pooler/dense/bias/adam_v\r\n",
      "Initialize PyTorch weight ['bert', 'pooler', 'dense', 'kernel']\r\n",
      "Skipping bert/pooler/dense/kernel/adam_m\r\n",
      "Skipping bert/pooler/dense/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['cls', 'predictions', 'output_bias']\r\n",
      "Skipping cls/predictions/output_bias/adam_m\r\n",
      "Skipping cls/predictions/output_bias/adam_v\r\n",
      "Initialize PyTorch weight ['cls', 'predictions', 'transform', 'LayerNorm', 'beta']\r\n",
      "Skipping cls/predictions/transform/LayerNorm/beta/adam_m\r\n",
      "Skipping cls/predictions/transform/LayerNorm/beta/adam_v\r\n",
      "Initialize PyTorch weight ['cls', 'predictions', 'transform', 'LayerNorm', 'gamma']\r\n",
      "Skipping cls/predictions/transform/LayerNorm/gamma/adam_m\r\n",
      "Skipping cls/predictions/transform/LayerNorm/gamma/adam_v\r\n",
      "Initialize PyTorch weight ['cls', 'predictions', 'transform', 'dense', 'bias']\r\n",
      "Skipping cls/predictions/transform/dense/bias/adam_m\r\n",
      "Skipping cls/predictions/transform/dense/bias/adam_v\r\n",
      "Initialize PyTorch weight ['cls', 'predictions', 'transform', 'dense', 'kernel']\r\n",
      "Skipping cls/predictions/transform/dense/kernel/adam_m\r\n",
      "Skipping cls/predictions/transform/dense/kernel/adam_v\r\n",
      "Initialize PyTorch weight ['cls', 'seq_relationship', 'output_bias']\r\n",
      "Skipping cls/seq_relationship/output_bias/adam_m\r\n",
      "Skipping cls/seq_relationship/output_bias/adam_v\r\n",
      "Initialize PyTorch weight ['cls', 'seq_relationship', 'output_weights']\r\n",
      "Skipping cls/seq_relationship/output_weights/adam_m\r\n",
      "Skipping cls/seq_relationship/output_weights/adam_v\r\n",
      "Skipping global_step\r\n",
      "Save PyTorch model to bert-large-bahasa-standard-cased/pytorch_model.bin\r\n"
     ]
    }
   ],
   "source": [
    "!transformers-cli convert --model_type bert \\\n",
    "  --tf_checkpoint bert-large/model.ckpt-700000 \\\n",
    "  --config LARGE_config.json \\\n",
    "  --pytorch_dump_output bert-large-bahasa-standard-cased/pytorch_model.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = BertConfig(f'{directory}/config.json')\n",
    "config.vocab_size = 32000\n",
    "config.hidden_size = 1024\n",
    "config.intermediate_size = 4096\n",
    "config.num_attention_heads = 16\n",
    "config.num_hidden_layers = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/husein/.local/lib/python3.6/site-packages/transformers/modeling_auto.py:821: FutureWarning:\n",
      "\n",
      "The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "\n",
      "Some weights of the model checkpoint at ./bert-large-bahasa-standard-cased/pytorch_model.bin were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelWithLMHead.from_pretrained('./bert-large-bahasa-standard-cased/pytorch_model.bin', config = config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_mask = pipeline('fill-mask', model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sequence': '[CLS] makan ayam dengan garam [SEP]',\n",
       "  'score': 0.1847386658191681,\n",
       "  'token': 12056,\n",
       "  'token_str': 'garam'},\n",
       " {'sequence': '[CLS] makan ayam dengan telur [SEP]',\n",
       "  'score': 0.06259126961231232,\n",
       "  'token': 8900,\n",
       "  'token_str': 'telur'},\n",
       " {'sequence': '[CLS] makan ayam dengan daging [SEP]',\n",
       "  'score': 0.06071213260293007,\n",
       "  'token': 7630,\n",
       "  'token_str': 'daging'},\n",
       " {'sequence': '[CLS] makan ayam dengan kicap [SEP]',\n",
       "  'score': 0.037356991320848465,\n",
       "  'token': 31246,\n",
       "  'token_str': 'kicap'},\n",
       " {'sequence': '[CLS] makan ayam dengan nasi [SEP]',\n",
       "  'score': 0.03496493399143219,\n",
       "  'token': 5354,\n",
       "  'token_str': 'nasi'}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_mask('makan ayam dengan [MASK]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sequence': '[CLS] mahathir sebenarnya sangat mencintai tanah airnya [SEP]',\n",
       "  'score': 0.29897311329841614,\n",
       "  'token': 14727,\n",
       "  'token_str': 'mencintai'},\n",
       " {'sequence': '[CLS] mahathir sebenarnya sangat sayangkan tanah airnya [SEP]',\n",
       "  'score': 0.10250459611415863,\n",
       "  'token': 22562,\n",
       "  'token_str': 'sayangkan'},\n",
       " {'sequence': '[CLS] mahathir sebenarnya sangat menyayangi tanah airnya [SEP]',\n",
       "  'score': 0.08254530280828476,\n",
       "  'token': 27640,\n",
       "  'token_str': 'menyayangi'},\n",
       " {'sequence': '[CLS] mahathir sebenarnya sangat cintakan tanah airnya [SEP]',\n",
       "  'score': 0.05926104262471199,\n",
       "  'token': 24174,\n",
       "  'token_str': 'cintakan'},\n",
       " {'sequence': '[CLS] mahathir sebenarnya sangat cantik tanah airnya [SEP]',\n",
       "  'score': 0.02629738114774227,\n",
       "  'token': 5190,\n",
       "  'token_str': 'cantik'}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_mask('mahathir sebenarnya sangat [MASK] tanah airnya')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "!transformers-cli upload ./bert-large-bahasa-standard-cased"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
