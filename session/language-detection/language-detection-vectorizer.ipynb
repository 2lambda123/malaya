{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def clean_text(string):\n",
    "    string = re.sub(u'[0-9!@#$%^&*()_\\-+{}|\\~`\\'\";:?/.>,<]', ' ', string.lower(), flags=re.UNICODE)\n",
    "    return re.sub(r'[ ]+', ' ', string.lower()).strip()\n",
    "\n",
    "def simple_textcleaning(string):\n",
    "    string = re.sub('[^A-Za-z ]+', ' ', string)\n",
    "    return re.sub(r'[ ]+', ' ', string.lower()).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('language-detection-data-v4.json','r') as fopen:\n",
    "    loaded = json.load(fopen)\n",
    "    sentences = [clean_text(text) for text in loaded['text']]\n",
    "    langs = loaded['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_ids = [i for i in range(len(langs)) if langs[i] == 'ind']\n",
    "zlm_ids = [i for i in range(len(langs)) if langs[i] == 'zlm']\n",
    "other_ids = [i for i in range(len(langs)) if langs[i] == 'OTHER']\n",
    "eng_ids = [i for i in range(len(langs)) if langs[i] == 'eng']\n",
    "\n",
    "other_sentences = [sentences[i] for i in other_ids]\n",
    "eng_sentences = [sentences[i] for i in eng_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('malay-text.txt') as fopen:\n",
    "    malays = filter(None, fopen.read().split('\\n'))\n",
    "\n",
    "with open('indon-text.txt') as fopen:\n",
    "    indons = list(filter(None, fopen.read().split('\\n')))\n",
    "    \n",
    "with open('00-indonesian-wordlist.txt',encoding='ISO-8859-1') as fopen:\n",
    "    another_indons = list(filter(None, fopen.read().split('\\n')))\n",
    "    \n",
    "another_indons = [simple_textcleaning(s) for s in another_indons if len(s) > 2]\n",
    "    \n",
    "with open('/home/husein/Malaya/stop-word-kerulnet') as fopen:\n",
    "    stopwords = set(filter(None, fopen.read().split('\\n')))\n",
    "    \n",
    "ind_set = set(indons)\n",
    "zlm_set = set(malays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# for no, i in enumerate(ind_ids):\n",
    "#     if (no+1) % 10000 == 0:\n",
    "#         print('indon %d'%(no + 1))\n",
    "#     sentences[i] = ' '.join(w for w in sentences[i].split() if w in ind_set and w not in stopwords)\n",
    "    \n",
    "# for no, i in enumerate(zlm_ids):\n",
    "#     if (no+1) % 10000 == 0:\n",
    "#         print('malay %d'%(no + 1))\n",
    "#     sentences[i] = ' '.join(w for w in sentences[i].split() if w in zlm_set and w not in stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zlm_sentences = list(filter(None, set([sentences[i] for i in zlm_ids])))\n",
    "# ind_sentences = list(filter(None, set([sentences[i] for i in ind_ids])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = other_sentences + eng_sentences\n",
    "langs = (['OTHER'] * len(other_sentences)) + (['eng'] * len(eng_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('wiki-id.txt') as fopen:\n",
    "    id_wiki = [simple_textcleaning(s) for s in list(filter(None, fopen.read().split('\\n')[:60000]))]\n",
    "    \n",
    "with open('wiki-ms.txt') as fopen:\n",
    "    ms_wiki = [simple_textcleaning(s) for s in list(filter(None, fopen.read().split('\\n')[:60000]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indon 10000\n",
      "indon 20000\n",
      "indon 30000\n",
      "indon 40000\n",
      "indon 50000\n",
      "indon 60000\n",
      "malay 10000\n",
      "malay 20000\n",
      "malay 30000\n",
      "malay 40000\n",
      "malay 50000\n",
      "malay 60000\n",
      "CPU times: user 6.71 s, sys: 36 ms, total: 6.75 s\n",
      "Wall time: 6.74 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for no, i in enumerate(range(len(id_wiki))):\n",
    "    if (no+1) % 10000 == 0:\n",
    "        print('indon %d'%(no + 1))\n",
    "    id_wiki[i] = ' '.join(w for w in id_wiki[i].split() if w in ind_set and w not in stopwords)\n",
    "    \n",
    "for no, i in enumerate(range(len(ms_wiki))):\n",
    "    if (no+1) % 10000 == 0:\n",
    "        print('malay %d'%(no + 1))\n",
    "    ms_wiki[i] = ' '.join(w for w in ms_wiki[i].split() if w in zlm_set and w not in stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_wiki = list(filter(None, set(id_wiki)))\n",
    "ms_wiki = list(filter(None, set(ms_wiki)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences += id_wiki + ms_wiki\n",
    "langs += (['ind'] * len(id_wiki)) + (['zlm'] * len(ms_wiki))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['OTHER', 'eng', 'ind', 'zlm'], dtype='<U5'),\n",
       " array([46910, 50000, 57327, 53692]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.unique(langs,return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_chars = CountVectorizer(ngram_range=(3, 5), analyzer='char_wb', max_features=700000).fit(sentences)\n",
    "delattr(bow_chars, 'stop_words_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('language-detection-data-v5.json','w') as fopen:\n",
    "    fopen.write(json.dumps({'text':sentences,'label':langs}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('language-detection-vectorizer.pkl','wb') as fopen:\n",
    "    pickle.dump(bow_chars,fopen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
